{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### tensorflow 최신버전이 아닌 1.9버전으로 설치하기\n",
    "##### 다른 모듈들이 이 버전에 못 따라와서 안정적인 게 1.9버전임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공공데이터포털: https://www.data.go.kr/\n",
    "# 세종말뭉치: https://ithub.korean.go.kr/user/guide/corpus/guide1.do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficent est. of word representations in vector space, # 2013 skip gram, Mikolov, Google NLP Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider with pytho\n",
    "# visual studio code: https://code.visualstudio.com/, javascript로 개발되어 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning framework power scores 2018\n",
    "# https://towardsdatascience.com/deep-learning-framework-power-scores-2018-23607ddf297a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense는 출력, 입력 순으로 지정함\n",
    "model.add(Dense(50, input_shape=(784,), activation='sigmoid', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(100, activation='sigmoid', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자동완성\n",
    "from keras.losses import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수를 최적화할 것. SGD(이게 가장 기본적임)\n",
    "model.compile(loss=categorical_crossentropy, optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepy.dataset import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_mnist(flatten=True, normalize=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 2.2701 - acc: 0.1966 - val_loss: 2.2237 - val_acc: 0.4039\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 2.1808 - acc: 0.4160 - val_loss: 2.1302 - val_acc: 0.4597\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 2.0682 - acc: 0.5173 - val_loss: 1.9928 - val_acc: 0.5581\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 1.9081 - acc: 0.5744 - val_loss: 1.8077 - val_acc: 0.6192\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 1.7107 - acc: 0.6230 - val_loss: 1.5980 - val_acc: 0.6483\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 1.5087 - acc: 0.6666 - val_loss: 1.4024 - val_acc: 0.6893\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 1.3305 - acc: 0.7067 - val_loss: 1.2349 - val_acc: 0.7333\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 3s 68us/step - loss: 1.1826 - acc: 0.7388 - val_loss: 1.0991 - val_acc: 0.7737\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 1.0612 - acc: 0.7655 - val_loss: 0.9869 - val_acc: 0.7974\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.9609 - acc: 0.7880 - val_loss: 0.8946 - val_acc: 0.8114\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.8774 - acc: 0.8037 - val_loss: 0.8172 - val_acc: 0.8251\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.8077 - acc: 0.8172 - val_loss: 0.7534 - val_acc: 0.8375\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.7495 - acc: 0.8272 - val_loss: 0.7000 - val_acc: 0.8433\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.7008 - acc: 0.8351 - val_loss: 0.6551 - val_acc: 0.8520\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.6597 - acc: 0.8420 - val_loss: 0.6181 - val_acc: 0.8566\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.6248 - acc: 0.8484 - val_loss: 0.5861 - val_acc: 0.8637\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.5951 - acc: 0.8537 - val_loss: 0.5589 - val_acc: 0.8683\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.5695 - acc: 0.8578 - val_loss: 0.5353 - val_acc: 0.8738\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.5470 - acc: 0.8625 - val_loss: 0.5147 - val_acc: 0.8753\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.5273 - acc: 0.8660 - val_loss: 0.4966 - val_acc: 0.8788\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.5098 - acc: 0.8691 - val_loss: 0.4808 - val_acc: 0.8823\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.4941 - acc: 0.8721 - val_loss: 0.4661 - val_acc: 0.8844\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.4801 - acc: 0.8751 - val_loss: 0.4530 - val_acc: 0.8872\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.4674 - acc: 0.8772 - val_loss: 0.4414 - val_acc: 0.8893\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.4558 - acc: 0.8801 - val_loss: 0.4309 - val_acc: 0.8896\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.4452 - acc: 0.8820 - val_loss: 0.4208 - val_acc: 0.8917\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.4355 - acc: 0.8841 - val_loss: 0.4117 - val_acc: 0.8928\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.4266 - acc: 0.8863 - val_loss: 0.4037 - val_acc: 0.8936\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.4183 - acc: 0.8881 - val_loss: 0.3960 - val_acc: 0.8953\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.4107 - acc: 0.8901 - val_loss: 0.3889 - val_acc: 0.8969\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.4036 - acc: 0.8911 - val_loss: 0.3822 - val_acc: 0.8986\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.3969 - acc: 0.8927 - val_loss: 0.3759 - val_acc: 0.9003\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.3907 - acc: 0.8941 - val_loss: 0.3705 - val_acc: 0.9010\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.3849 - acc: 0.8958 - val_loss: 0.3648 - val_acc: 0.9017\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.3795 - acc: 0.8967 - val_loss: 0.3598 - val_acc: 0.9031\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.3743 - acc: 0.8980 - val_loss: 0.3550 - val_acc: 0.9045\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.3695 - acc: 0.8989 - val_loss: 0.3505 - val_acc: 0.9053\n",
      "Epoch 38/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.3649 - acc: 0.9001 - val_loss: 0.3462 - val_acc: 0.9064\n",
      "Epoch 39/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.3606 - acc: 0.9005 - val_loss: 0.3423 - val_acc: 0.9072\n",
      "Epoch 40/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.3564 - acc: 0.9010 - val_loss: 0.3385 - val_acc: 0.9081\n",
      "Epoch 41/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.3525 - acc: 0.9024 - val_loss: 0.3349 - val_acc: 0.9089\n",
      "Epoch 42/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.3488 - acc: 0.9029 - val_loss: 0.3313 - val_acc: 0.9100\n",
      "Epoch 43/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.3452 - acc: 0.9035 - val_loss: 0.3279 - val_acc: 0.9108\n",
      "Epoch 44/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.3417 - acc: 0.9045 - val_loss: 0.3247 - val_acc: 0.9121\n",
      "Epoch 45/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.3385 - acc: 0.9047 - val_loss: 0.3216 - val_acc: 0.9124\n",
      "Epoch 46/100\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.3353 - acc: 0.9057 - val_loss: 0.3188 - val_acc: 0.9128\n",
      "Epoch 47/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.3323 - acc: 0.9065 - val_loss: 0.3162 - val_acc: 0.9125\n",
      "Epoch 48/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.3293 - acc: 0.9068 - val_loss: 0.3132 - val_acc: 0.9132\n",
      "Epoch 49/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.3265 - acc: 0.9079 - val_loss: 0.3108 - val_acc: 0.9134\n",
      "Epoch 50/100\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.3238 - acc: 0.9085 - val_loss: 0.3082 - val_acc: 0.9142\n",
      "Epoch 51/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.3211 - acc: 0.9086 - val_loss: 0.3059 - val_acc: 0.9147\n",
      "Epoch 52/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.3186 - acc: 0.9098 - val_loss: 0.3037 - val_acc: 0.9148\n",
      "Epoch 53/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.3161 - acc: 0.9102 - val_loss: 0.3014 - val_acc: 0.9154\n",
      "Epoch 54/100\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.3137 - acc: 0.9105 - val_loss: 0.2990 - val_acc: 0.9156\n",
      "Epoch 55/100\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.3114 - acc: 0.9112 - val_loss: 0.2970 - val_acc: 0.9167\n",
      "Epoch 56/100\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.3091 - acc: 0.9117 - val_loss: 0.2948 - val_acc: 0.9172\n",
      "Epoch 57/100\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.3069 - acc: 0.9125 - val_loss: 0.2930 - val_acc: 0.9172\n",
      "Epoch 58/100\n",
      "48000/48000 [==============================] - 3s 68us/step - loss: 0.3047 - acc: 0.9132 - val_loss: 0.2911 - val_acc: 0.9177\n",
      "Epoch 59/100\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.3026 - acc: 0.9136 - val_loss: 0.2893 - val_acc: 0.9180\n",
      "Epoch 60/100\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.3006 - acc: 0.9142 - val_loss: 0.2872 - val_acc: 0.9188\n",
      "Epoch 61/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.2985 - acc: 0.9149 - val_loss: 0.2855 - val_acc: 0.9193\n",
      "Epoch 62/100\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.2966 - acc: 0.9155 - val_loss: 0.2840 - val_acc: 0.9190\n",
      "Epoch 63/100\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.2947 - acc: 0.9160 - val_loss: 0.2821 - val_acc: 0.9198\n",
      "Epoch 64/100\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.2927 - acc: 0.9162 - val_loss: 0.2804 - val_acc: 0.9205\n",
      "Epoch 65/100\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.2909 - acc: 0.9169 - val_loss: 0.2791 - val_acc: 0.9201\n",
      "Epoch 66/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.2891 - acc: 0.9175 - val_loss: 0.2772 - val_acc: 0.9215\n",
      "Epoch 67/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.2874 - acc: 0.9180 - val_loss: 0.2758 - val_acc: 0.9214\n",
      "Epoch 68/100\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.2856 - acc: 0.9186 - val_loss: 0.2743 - val_acc: 0.9227\n",
      "Epoch 69/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.2840 - acc: 0.9187 - val_loss: 0.2726 - val_acc: 0.9230\n",
      "Epoch 70/100\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.2823 - acc: 0.9195 - val_loss: 0.2713 - val_acc: 0.9231\n",
      "Epoch 71/100\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.2806 - acc: 0.9200 - val_loss: 0.2699 - val_acc: 0.9237\n",
      "Epoch 72/100\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.2790 - acc: 0.9201 - val_loss: 0.2684 - val_acc: 0.9238\n",
      "Epoch 73/100\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.2774 - acc: 0.9204 - val_loss: 0.2670 - val_acc: 0.9241\n",
      "Epoch 74/100\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.2758 - acc: 0.9211 - val_loss: 0.2657 - val_acc: 0.9239\n",
      "Epoch 75/100\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.2744 - acc: 0.9216 - val_loss: 0.2643 - val_acc: 0.9248\n",
      "Epoch 76/100\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.2728 - acc: 0.9219 - val_loss: 0.2629 - val_acc: 0.9253\n",
      "Epoch 77/100\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.2713 - acc: 0.9223 - val_loss: 0.2617 - val_acc: 0.9256\n",
      "Epoch 78/100\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.2699 - acc: 0.9231 - val_loss: 0.2604 - val_acc: 0.9258\n",
      "Epoch 79/100\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.2684 - acc: 0.9236 - val_loss: 0.2593 - val_acc: 0.9261\n",
      "Epoch 80/100\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.2670 - acc: 0.9235 - val_loss: 0.2579 - val_acc: 0.9261\n",
      "Epoch 81/100\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.2655 - acc: 0.9244 - val_loss: 0.2569 - val_acc: 0.9264\n",
      "Epoch 82/100\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.2642 - acc: 0.9244 - val_loss: 0.2556 - val_acc: 0.9267\n",
      "Epoch 83/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.2628 - acc: 0.9245 - val_loss: 0.2544 - val_acc: 0.9273\n",
      "Epoch 84/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.2614 - acc: 0.9250 - val_loss: 0.2533 - val_acc: 0.9273\n",
      "Epoch 85/100\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.2601 - acc: 0.9252 - val_loss: 0.2522 - val_acc: 0.9281\n",
      "Epoch 86/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.2588 - acc: 0.9257 - val_loss: 0.2509 - val_acc: 0.9280\n",
      "Epoch 87/100\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.2575 - acc: 0.9263 - val_loss: 0.2499 - val_acc: 0.9278\n",
      "Epoch 88/100\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.2562 - acc: 0.9261 - val_loss: 0.2488 - val_acc: 0.9285\n",
      "Epoch 89/100\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.2549 - acc: 0.9268 - val_loss: 0.2477 - val_acc: 0.9288\n",
      "Epoch 90/100\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.2537 - acc: 0.9270 - val_loss: 0.2465 - val_acc: 0.9291\n",
      "Epoch 91/100\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.2524 - acc: 0.9275 - val_loss: 0.2454 - val_acc: 0.9294\n",
      "Epoch 92/100\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.2512 - acc: 0.9277 - val_loss: 0.2445 - val_acc: 0.9293\n",
      "Epoch 93/100\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.2499 - acc: 0.9280 - val_loss: 0.2434 - val_acc: 0.9297\n",
      "Epoch 94/100\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.2488 - acc: 0.9280 - val_loss: 0.2424 - val_acc: 0.9298\n",
      "Epoch 95/100\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.2475 - acc: 0.9286 - val_loss: 0.2414 - val_acc: 0.9301\n",
      "Epoch 96/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.2463 - acc: 0.9291 - val_loss: 0.2406 - val_acc: 0.9303\n",
      "Epoch 97/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.2452 - acc: 0.9292 - val_loss: 0.2395 - val_acc: 0.9301\n",
      "Epoch 98/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.2441 - acc: 0.9293 - val_loss: 0.2385 - val_acc: 0.9314\n",
      "Epoch 99/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.2429 - acc: 0.9298 - val_loss: 0.2376 - val_acc: 0.9308\n",
      "Epoch 100/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.2418 - acc: 0.9303 - val_loss: 0.2366 - val_acc: 0.9314\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=100, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 45,360\n",
      "Trainable params: 45,360\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.223730</td>\n",
       "      <td>0.403917</td>\n",
       "      <td>2.270078</td>\n",
       "      <td>0.196604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.130212</td>\n",
       "      <td>0.459667</td>\n",
       "      <td>2.180840</td>\n",
       "      <td>0.415958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.992754</td>\n",
       "      <td>0.558083</td>\n",
       "      <td>2.068214</td>\n",
       "      <td>0.517312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.807717</td>\n",
       "      <td>0.619250</td>\n",
       "      <td>1.908086</td>\n",
       "      <td>0.574354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.597969</td>\n",
       "      <td>0.648333</td>\n",
       "      <td>1.710686</td>\n",
       "      <td>0.622979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.402390</td>\n",
       "      <td>0.689333</td>\n",
       "      <td>1.508745</td>\n",
       "      <td>0.666625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.234914</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>1.330504</td>\n",
       "      <td>0.706708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.099074</td>\n",
       "      <td>0.773667</td>\n",
       "      <td>1.182623</td>\n",
       "      <td>0.738833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.986949</td>\n",
       "      <td>0.797417</td>\n",
       "      <td>1.061227</td>\n",
       "      <td>0.765479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.894561</td>\n",
       "      <td>0.811417</td>\n",
       "      <td>0.960948</td>\n",
       "      <td>0.787979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.817187</td>\n",
       "      <td>0.825083</td>\n",
       "      <td>0.877436</td>\n",
       "      <td>0.803750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.753402</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.807749</td>\n",
       "      <td>0.817250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.699955</td>\n",
       "      <td>0.843333</td>\n",
       "      <td>0.749486</td>\n",
       "      <td>0.827187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.655091</td>\n",
       "      <td>0.852000</td>\n",
       "      <td>0.700750</td>\n",
       "      <td>0.835083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.618107</td>\n",
       "      <td>0.856583</td>\n",
       "      <td>0.659663</td>\n",
       "      <td>0.841979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.586083</td>\n",
       "      <td>0.863667</td>\n",
       "      <td>0.624799</td>\n",
       "      <td>0.848354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.558867</td>\n",
       "      <td>0.868333</td>\n",
       "      <td>0.595112</td>\n",
       "      <td>0.853708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.535348</td>\n",
       "      <td>0.873833</td>\n",
       "      <td>0.569459</td>\n",
       "      <td>0.857792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.514736</td>\n",
       "      <td>0.875250</td>\n",
       "      <td>0.547030</td>\n",
       "      <td>0.862521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.496625</td>\n",
       "      <td>0.878833</td>\n",
       "      <td>0.527297</td>\n",
       "      <td>0.866021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.480779</td>\n",
       "      <td>0.882333</td>\n",
       "      <td>0.509795</td>\n",
       "      <td>0.869104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.466149</td>\n",
       "      <td>0.884417</td>\n",
       "      <td>0.494148</td>\n",
       "      <td>0.872125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.452961</td>\n",
       "      <td>0.887167</td>\n",
       "      <td>0.480071</td>\n",
       "      <td>0.875063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.441448</td>\n",
       "      <td>0.889333</td>\n",
       "      <td>0.467352</td>\n",
       "      <td>0.877208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.430859</td>\n",
       "      <td>0.889583</td>\n",
       "      <td>0.455792</td>\n",
       "      <td>0.880062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.420831</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>0.445239</td>\n",
       "      <td>0.882042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.411741</td>\n",
       "      <td>0.892833</td>\n",
       "      <td>0.435487</td>\n",
       "      <td>0.884063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.403726</td>\n",
       "      <td>0.893583</td>\n",
       "      <td>0.426555</td>\n",
       "      <td>0.886271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.396024</td>\n",
       "      <td>0.895250</td>\n",
       "      <td>0.418291</td>\n",
       "      <td>0.888104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.388909</td>\n",
       "      <td>0.896917</td>\n",
       "      <td>0.410698</td>\n",
       "      <td>0.890063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.269897</td>\n",
       "      <td>0.923667</td>\n",
       "      <td>0.280631</td>\n",
       "      <td>0.919979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.268358</td>\n",
       "      <td>0.923833</td>\n",
       "      <td>0.279010</td>\n",
       "      <td>0.920125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.266953</td>\n",
       "      <td>0.924083</td>\n",
       "      <td>0.277440</td>\n",
       "      <td>0.920417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.265665</td>\n",
       "      <td>0.923917</td>\n",
       "      <td>0.275840</td>\n",
       "      <td>0.921063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.264317</td>\n",
       "      <td>0.924833</td>\n",
       "      <td>0.274357</td>\n",
       "      <td>0.921625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.262929</td>\n",
       "      <td>0.925333</td>\n",
       "      <td>0.272806</td>\n",
       "      <td>0.921854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.261707</td>\n",
       "      <td>0.925583</td>\n",
       "      <td>0.271296</td>\n",
       "      <td>0.922271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.260401</td>\n",
       "      <td>0.925750</td>\n",
       "      <td>0.269861</td>\n",
       "      <td>0.923063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.259345</td>\n",
       "      <td>0.926083</td>\n",
       "      <td>0.268409</td>\n",
       "      <td>0.923604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.257872</td>\n",
       "      <td>0.926083</td>\n",
       "      <td>0.266958</td>\n",
       "      <td>0.923458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.256926</td>\n",
       "      <td>0.926417</td>\n",
       "      <td>0.265492</td>\n",
       "      <td>0.924438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.255590</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.264191</td>\n",
       "      <td>0.924375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.254428</td>\n",
       "      <td>0.927333</td>\n",
       "      <td>0.262816</td>\n",
       "      <td>0.924500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.253329</td>\n",
       "      <td>0.927250</td>\n",
       "      <td>0.261427</td>\n",
       "      <td>0.924958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.252246</td>\n",
       "      <td>0.928083</td>\n",
       "      <td>0.260123</td>\n",
       "      <td>0.925229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.250895</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0.258770</td>\n",
       "      <td>0.925708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.249945</td>\n",
       "      <td>0.927833</td>\n",
       "      <td>0.257451</td>\n",
       "      <td>0.926333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.248827</td>\n",
       "      <td>0.928500</td>\n",
       "      <td>0.256199</td>\n",
       "      <td>0.926146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.247745</td>\n",
       "      <td>0.928833</td>\n",
       "      <td>0.254898</td>\n",
       "      <td>0.926813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.246464</td>\n",
       "      <td>0.929083</td>\n",
       "      <td>0.253658</td>\n",
       "      <td>0.926979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.245413</td>\n",
       "      <td>0.929417</td>\n",
       "      <td>0.252420</td>\n",
       "      <td>0.927458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.244544</td>\n",
       "      <td>0.929333</td>\n",
       "      <td>0.251220</td>\n",
       "      <td>0.927729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.243448</td>\n",
       "      <td>0.929667</td>\n",
       "      <td>0.249950</td>\n",
       "      <td>0.928021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.242400</td>\n",
       "      <td>0.929833</td>\n",
       "      <td>0.248770</td>\n",
       "      <td>0.928042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.930083</td>\n",
       "      <td>0.247525</td>\n",
       "      <td>0.928646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.240591</td>\n",
       "      <td>0.930250</td>\n",
       "      <td>0.246334</td>\n",
       "      <td>0.929104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.239500</td>\n",
       "      <td>0.930083</td>\n",
       "      <td>0.245160</td>\n",
       "      <td>0.929167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.238544</td>\n",
       "      <td>0.931417</td>\n",
       "      <td>0.244075</td>\n",
       "      <td>0.929313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.237570</td>\n",
       "      <td>0.930750</td>\n",
       "      <td>0.242894</td>\n",
       "      <td>0.929750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.236553</td>\n",
       "      <td>0.931417</td>\n",
       "      <td>0.241807</td>\n",
       "      <td>0.930333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    val_loss   val_acc      loss       acc\n",
       "0   2.223730  0.403917  2.270078  0.196604\n",
       "1   2.130212  0.459667  2.180840  0.415958\n",
       "2   1.992754  0.558083  2.068214  0.517312\n",
       "3   1.807717  0.619250  1.908086  0.574354\n",
       "4   1.597969  0.648333  1.710686  0.622979\n",
       "5   1.402390  0.689333  1.508745  0.666625\n",
       "6   1.234914  0.733333  1.330504  0.706708\n",
       "7   1.099074  0.773667  1.182623  0.738833\n",
       "8   0.986949  0.797417  1.061227  0.765479\n",
       "9   0.894561  0.811417  0.960948  0.787979\n",
       "10  0.817187  0.825083  0.877436  0.803750\n",
       "11  0.753402  0.837500  0.807749  0.817250\n",
       "12  0.699955  0.843333  0.749486  0.827187\n",
       "13  0.655091  0.852000  0.700750  0.835083\n",
       "14  0.618107  0.856583  0.659663  0.841979\n",
       "15  0.586083  0.863667  0.624799  0.848354\n",
       "16  0.558867  0.868333  0.595112  0.853708\n",
       "17  0.535348  0.873833  0.569459  0.857792\n",
       "18  0.514736  0.875250  0.547030  0.862521\n",
       "19  0.496625  0.878833  0.527297  0.866021\n",
       "20  0.480779  0.882333  0.509795  0.869104\n",
       "21  0.466149  0.884417  0.494148  0.872125\n",
       "22  0.452961  0.887167  0.480071  0.875063\n",
       "23  0.441448  0.889333  0.467352  0.877208\n",
       "24  0.430859  0.889583  0.455792  0.880062\n",
       "25  0.420831  0.891667  0.445239  0.882042\n",
       "26  0.411741  0.892833  0.435487  0.884063\n",
       "27  0.403726  0.893583  0.426555  0.886271\n",
       "28  0.396024  0.895250  0.418291  0.888104\n",
       "29  0.388909  0.896917  0.410698  0.890063\n",
       "..       ...       ...       ...       ...\n",
       "70  0.269897  0.923667  0.280631  0.919979\n",
       "71  0.268358  0.923833  0.279010  0.920125\n",
       "72  0.266953  0.924083  0.277440  0.920417\n",
       "73  0.265665  0.923917  0.275840  0.921063\n",
       "74  0.264317  0.924833  0.274357  0.921625\n",
       "75  0.262929  0.925333  0.272806  0.921854\n",
       "76  0.261707  0.925583  0.271296  0.922271\n",
       "77  0.260401  0.925750  0.269861  0.923063\n",
       "78  0.259345  0.926083  0.268409  0.923604\n",
       "79  0.257872  0.926083  0.266958  0.923458\n",
       "80  0.256926  0.926417  0.265492  0.924438\n",
       "81  0.255590  0.926667  0.264191  0.924375\n",
       "82  0.254428  0.927333  0.262816  0.924500\n",
       "83  0.253329  0.927250  0.261427  0.924958\n",
       "84  0.252246  0.928083  0.260123  0.925229\n",
       "85  0.250895  0.928000  0.258770  0.925708\n",
       "86  0.249945  0.927833  0.257451  0.926333\n",
       "87  0.248827  0.928500  0.256199  0.926146\n",
       "88  0.247745  0.928833  0.254898  0.926813\n",
       "89  0.246464  0.929083  0.253658  0.926979\n",
       "90  0.245413  0.929417  0.252420  0.927458\n",
       "91  0.244544  0.929333  0.251220  0.927729\n",
       "92  0.243448  0.929667  0.249950  0.928021\n",
       "93  0.242400  0.929833  0.248770  0.928042\n",
       "94  0.241379  0.930083  0.247525  0.928646\n",
       "95  0.240591  0.930250  0.246334  0.929104\n",
       "96  0.239500  0.930083  0.245160  0.929167\n",
       "97  0.238544  0.931417  0.244075  0.929313\n",
       "98  0.237570  0.930750  0.242894  0.929750\n",
       "99  0.236553  0.931417  0.241807  0.930333\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result = pd.DataFrame(history.history)\n",
    "train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result[['loss', 'val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result[['loss', 'val_acc']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(50, input_shape=(784,), activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=categorical_crossentropy, optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=100, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = pd.DataFrame(history.history)\n",
    "train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result[['loss', 'val_loss', 'val_acc']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 회귀 데이터 다룸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "columns = boston.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_std_train = scaler.fit_transform(X_train)\n",
    "X_std_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 13)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.38178327, -0.51152897, -0.58282931, ..., -0.20784122,\n",
       "         0.44271845,  2.674192  ],\n",
       "       [-0.40462804, -0.51152897, -1.09582336, ..., -0.25382593,\n",
       "         0.43336534, -0.96457915],\n",
       "       [-0.40663556,  0.89662363, -0.70301649, ..., -1.03556613,\n",
       "         0.4299136 , -0.41390888],\n",
       "       ...,\n",
       "       [-0.34922621,  0.31679609, -1.01374432, ..., -2.46109238,\n",
       "         0.39706637, -0.40086669],\n",
       "       [ 1.29810959, -0.51152897,  1.05728597, ...,  0.84980729,\n",
       "        -0.08528673,  1.81920395],\n",
       "       [-0.37197998, -0.51152897,  1.61278524, ...,  1.30965446,\n",
       "         0.42200799,  0.03532212]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_std_train.shape)\n",
    "\n",
    "X_std_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 회귀에서 손실함수 -> 평균제곱오차\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_shape=(13,), activation='relu'))\n",
    "model.add(Dense(40))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 341 samples, validate on 38 samples\n",
      "Epoch 1/10\n",
      "341/341 [==============================] - 1s 2ms/step - loss: 613.8201 - val_loss: 609.8009\n",
      "Epoch 2/10\n",
      "341/341 [==============================] - 0s 41us/step - loss: 590.6855 - val_loss: 589.9202\n",
      "Epoch 3/10\n",
      "341/341 [==============================] - 0s 38us/step - loss: 566.7438 - val_loss: 569.9667\n",
      "Epoch 4/10\n",
      "341/341 [==============================] - 0s 39us/step - loss: 543.4463 - val_loss: 549.0557\n",
      "Epoch 5/10\n",
      "341/341 [==============================] - 0s 38us/step - loss: 518.6613 - val_loss: 526.5931\n",
      "Epoch 6/10\n",
      "341/341 [==============================] - 0s 44us/step - loss: 491.5486 - val_loss: 501.6921\n",
      "Epoch 7/10\n",
      "341/341 [==============================] - 0s 35us/step - loss: 462.2123 - val_loss: 474.2531\n",
      "Epoch 8/10\n",
      "341/341 [==============================] - 0s 44us/step - loss: 429.3293 - val_loss: 443.7523\n",
      "Epoch 9/10\n",
      "341/341 [==============================] - 0s 38us/step - loss: 394.5058 - val_loss: 409.8260\n",
      "Epoch 10/10\n",
      "341/341 [==============================] - 0s 36us/step - loss: 355.5178 - val_loss: 373.3152\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_std_train, y_train, batch_size=50, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터가 적다보니 더 딥하게 들어갈 수 없고 성능이 잘 안나와서 결정트리로 테스트해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결정트리\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-45cf472e53fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mforest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "forest = RandomForestRegressor()\n",
    "forest.fit(X_train, y_train)\n",
    "forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepy.dataset import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load('data/cifar-10-batches-py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 59,  62,  63],\n",
       "         [ 43,  46,  45],\n",
       "         [ 50,  48,  43],\n",
       "         ...,\n",
       "         [158, 132, 108],\n",
       "         [152, 125, 102],\n",
       "         [148, 124, 103]],\n",
       "\n",
       "        [[ 16,  20,  20],\n",
       "         [  0,   0,   0],\n",
       "         [ 18,   8,   0],\n",
       "         ...,\n",
       "         [123,  88,  55],\n",
       "         [119,  83,  50],\n",
       "         [122,  87,  57]],\n",
       "\n",
       "        [[ 25,  24,  21],\n",
       "         [ 16,   7,   0],\n",
       "         [ 49,  27,   8],\n",
       "         ...,\n",
       "         [118,  84,  50],\n",
       "         [120,  84,  50],\n",
       "         [109,  73,  42]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[208, 170,  96],\n",
       "         [201, 153,  34],\n",
       "         [198, 161,  26],\n",
       "         ...,\n",
       "         [160, 133,  70],\n",
       "         [ 56,  31,   7],\n",
       "         [ 53,  34,  20]],\n",
       "\n",
       "        [[180, 139,  96],\n",
       "         [173, 123,  42],\n",
       "         [186, 144,  30],\n",
       "         ...,\n",
       "         [184, 148,  94],\n",
       "         [ 97,  62,  34],\n",
       "         [ 83,  53,  34]],\n",
       "\n",
       "        [[177, 144, 116],\n",
       "         [168, 129,  94],\n",
       "         [179, 142,  87],\n",
       "         ...,\n",
       "         [216, 184, 140],\n",
       "         [151, 118,  84],\n",
       "         [123,  92,  72]]],\n",
       "\n",
       "\n",
       "       [[[154, 177, 187],\n",
       "         [126, 137, 136],\n",
       "         [105, 104,  95],\n",
       "         ...,\n",
       "         [ 91,  95,  71],\n",
       "         [ 87,  90,  71],\n",
       "         [ 79,  81,  70]],\n",
       "\n",
       "        [[140, 160, 169],\n",
       "         [145, 153, 154],\n",
       "         [125, 125, 118],\n",
       "         ...,\n",
       "         [ 96,  99,  78],\n",
       "         [ 77,  80,  62],\n",
       "         [ 71,  73,  61]],\n",
       "\n",
       "        [[140, 155, 164],\n",
       "         [139, 146, 149],\n",
       "         [115, 115, 112],\n",
       "         ...,\n",
       "         [ 79,  82,  64],\n",
       "         [ 68,  70,  55],\n",
       "         [ 67,  69,  55]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[175, 167, 166],\n",
       "         [156, 154, 160],\n",
       "         [154, 160, 170],\n",
       "         ...,\n",
       "         [ 42,  34,  36],\n",
       "         [ 61,  53,  57],\n",
       "         [ 93,  83,  91]],\n",
       "\n",
       "        [[165, 154, 128],\n",
       "         [156, 152, 130],\n",
       "         [159, 161, 142],\n",
       "         ...,\n",
       "         [103,  93,  96],\n",
       "         [123, 114, 120],\n",
       "         [131, 121, 131]],\n",
       "\n",
       "        [[163, 148, 120],\n",
       "         [158, 148, 122],\n",
       "         [163, 156, 133],\n",
       "         ...,\n",
       "         [143, 133, 139],\n",
       "         [143, 134, 142],\n",
       "         [143, 133, 144]]],\n",
       "\n",
       "\n",
       "       [[[255, 255, 255],\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         ...,\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         ...,\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[113, 120, 112],\n",
       "         [111, 118, 111],\n",
       "         [105, 112, 106],\n",
       "         ...,\n",
       "         [ 72,  81,  80],\n",
       "         [ 72,  80,  79],\n",
       "         [ 72,  80,  79]],\n",
       "\n",
       "        [[111, 118, 110],\n",
       "         [104, 111, 104],\n",
       "         [ 99, 106,  98],\n",
       "         ...,\n",
       "         [ 68,  75,  73],\n",
       "         [ 70,  76,  75],\n",
       "         [ 78,  84,  82]],\n",
       "\n",
       "        [[106, 113, 105],\n",
       "         [ 99, 106,  98],\n",
       "         [ 95, 102,  94],\n",
       "         ...,\n",
       "         [ 78,  85,  83],\n",
       "         [ 79,  85,  83],\n",
       "         [ 80,  86,  84]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 35, 178, 235],\n",
       "         [ 40, 176, 239],\n",
       "         [ 42, 176, 241],\n",
       "         ...,\n",
       "         [ 99, 177, 219],\n",
       "         [ 79, 147, 197],\n",
       "         [ 89, 148, 189]],\n",
       "\n",
       "        [[ 57, 182, 234],\n",
       "         [ 44, 184, 250],\n",
       "         [ 50, 183, 240],\n",
       "         ...,\n",
       "         [156, 182, 200],\n",
       "         [141, 177, 206],\n",
       "         [116, 149, 175]],\n",
       "\n",
       "        [[ 98, 197, 237],\n",
       "         [ 64, 189, 252],\n",
       "         [ 69, 192, 245],\n",
       "         ...,\n",
       "         [188, 195, 206],\n",
       "         [119, 135, 147],\n",
       "         [ 61,  79,  90]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 73,  79,  77],\n",
       "         [ 53,  63,  68],\n",
       "         [ 54,  68,  80],\n",
       "         ...,\n",
       "         [ 17,  40,  64],\n",
       "         [ 21,  36,  51],\n",
       "         [ 33,  48,  49]],\n",
       "\n",
       "        [[ 61,  68,  75],\n",
       "         [ 55,  70,  86],\n",
       "         [ 57,  79, 103],\n",
       "         ...,\n",
       "         [ 24,  48,  72],\n",
       "         [ 17,  35,  53],\n",
       "         [  7,  23,  32]],\n",
       "\n",
       "        [[ 44,  56,  73],\n",
       "         [ 46,  66,  88],\n",
       "         [ 49,  77, 105],\n",
       "         ...,\n",
       "         [ 27,  52,  77],\n",
       "         [ 21,  43,  66],\n",
       "         [ 12,  31,  50]]],\n",
       "\n",
       "\n",
       "       [[[189, 211, 240],\n",
       "         [186, 208, 236],\n",
       "         [185, 207, 235],\n",
       "         ...,\n",
       "         [175, 195, 224],\n",
       "         [172, 194, 222],\n",
       "         [169, 194, 220]],\n",
       "\n",
       "        [[194, 210, 239],\n",
       "         [191, 207, 236],\n",
       "         [190, 206, 235],\n",
       "         ...,\n",
       "         [173, 192, 220],\n",
       "         [171, 191, 218],\n",
       "         [167, 190, 216]],\n",
       "\n",
       "        [[208, 219, 244],\n",
       "         [205, 216, 240],\n",
       "         [204, 215, 239],\n",
       "         ...,\n",
       "         [175, 191, 217],\n",
       "         [172, 190, 216],\n",
       "         [169, 191, 215]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[207, 199, 181],\n",
       "         [203, 195, 175],\n",
       "         [203, 196, 173],\n",
       "         ...,\n",
       "         [135, 132, 127],\n",
       "         [162, 158, 150],\n",
       "         [168, 163, 151]],\n",
       "\n",
       "        [[198, 190, 170],\n",
       "         [189, 181, 159],\n",
       "         [180, 172, 147],\n",
       "         ...,\n",
       "         [178, 171, 160],\n",
       "         [175, 169, 156],\n",
       "         [175, 169, 154]],\n",
       "\n",
       "        [[198, 189, 173],\n",
       "         [189, 181, 162],\n",
       "         [178, 170, 149],\n",
       "         ...,\n",
       "         [195, 184, 169],\n",
       "         [196, 189, 171],\n",
       "         [195, 190, 171]]],\n",
       "\n",
       "\n",
       "       [[[229, 229, 239],\n",
       "         [236, 237, 247],\n",
       "         [234, 236, 247],\n",
       "         ...,\n",
       "         [217, 219, 233],\n",
       "         [221, 223, 234],\n",
       "         [222, 223, 233]],\n",
       "\n",
       "        [[222, 221, 229],\n",
       "         [239, 239, 249],\n",
       "         [233, 234, 246],\n",
       "         ...,\n",
       "         [223, 223, 236],\n",
       "         [227, 228, 238],\n",
       "         [210, 211, 220]],\n",
       "\n",
       "        [[213, 206, 211],\n",
       "         [234, 232, 239],\n",
       "         [231, 233, 244],\n",
       "         ...,\n",
       "         [220, 220, 232],\n",
       "         [220, 219, 232],\n",
       "         [202, 203, 215]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[150, 143, 135],\n",
       "         [140, 135, 127],\n",
       "         [132, 127, 120],\n",
       "         ...,\n",
       "         [224, 222, 218],\n",
       "         [230, 228, 225],\n",
       "         [241, 241, 238]],\n",
       "\n",
       "        [[137, 132, 126],\n",
       "         [130, 127, 120],\n",
       "         [125, 121, 115],\n",
       "         ...,\n",
       "         [181, 180, 178],\n",
       "         [202, 201, 198],\n",
       "         [212, 211, 207]],\n",
       "\n",
       "        [[122, 119, 114],\n",
       "         [118, 116, 110],\n",
       "         [120, 116, 111],\n",
       "         ...,\n",
       "         [179, 177, 173],\n",
       "         [164, 164, 162],\n",
       "         [163, 163, 161]]]], dtype=uint8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = pd.get_dummies(y_train).values\n",
    "Y_test = pd.get_dummies(y_test).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 32*32*3)\n",
    "X_test = X_test.reshape(-1, 32*32*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3072)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(400, input_shape=(32*32*3,), activation='relu'))\n",
    "model.add(Dense(600, activation='relu'))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss=categorical_crossentropy, optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "40000/40000 [==============================] - 14s 349us/step - loss: 1.8737 - acc: 0.3196 - val_loss: 1.7459 - val_acc: 0.3717\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 15s 368us/step - loss: 1.6874 - acc: 0.3925 - val_loss: 1.6584 - val_acc: 0.4100\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 16s 398us/step - loss: 1.5977 - acc: 0.4259 - val_loss: 1.6135 - val_acc: 0.4246\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 15s 372us/step - loss: 1.5445 - acc: 0.4474 - val_loss: 1.6025 - val_acc: 0.4267\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 15s 369us/step - loss: 1.4892 - acc: 0.4664 - val_loss: 1.5209 - val_acc: 0.4596\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 14s 362us/step - loss: 1.4472 - acc: 0.4818 - val_loss: 1.5452 - val_acc: 0.4642\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 14s 357us/step - loss: 1.4210 - acc: 0.4897 - val_loss: 1.5037 - val_acc: 0.4656\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 14s 357us/step - loss: 1.3845 - acc: 0.5031 - val_loss: 1.5084 - val_acc: 0.4677\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 14s 356us/step - loss: 1.3580 - acc: 0.5108 - val_loss: 1.4653 - val_acc: 0.4868\n",
      "Epoch 10/10\n",
      "40000/40000 [==============================] - 15s 365us/step - loss: 1.3297 - acc: 0.5219 - val_loss: 1.4613 - val_acc: 0.4831\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=100, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a362cf198>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_result[['loss', 'val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a36387748>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_result[['loss', 'val_acc']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1997 LeNet\n",
    "\n",
    "...을 시작으로 합성곱 신경망이 탄생함.\n",
    "딥러닝 시대의 시작이라고 불릴 수 있음.\n",
    "#### CNN: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_mnist(flatten=False, normalize=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "축순서 = [0, 2, 3, 1]\n",
    "X_train = X_train.transpose(축순서)\n",
    "X_test = X_test.transpose(축순서)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (10000, 28, 28, 1))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D # 주요계층\n",
    "from keras.layers import MaxPooling2D, Flatten # 보조계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# 1층\n",
    "model.add(Conv2D(20, input_shape=(28, 28, 1), kernel_size=(5,5), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "# --> pooling 거쳐서 28/2 => 14x14가 20개 나옴.\n",
    "\n",
    "# 2층\n",
    "model.add(Conv2D(50, kernel_size=5, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "# --> pooling 거쳐서 14/2 => 7x7이 50개 나옴.\n",
    "\n",
    "# 출력 준비층\n",
    "# 2차원 --> 1차원으로 만들기 위해서\n",
    "model.add(Flatten()) # flatten거쳐서 7x7x50개가 나옴??\n",
    "model.add(Dense(500, activation='relu'))\n",
    "\n",
    "# 출력\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 20)        520       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 14, 14, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 14, 14, 50)        25050     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 50)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2450)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 500)               1225500   \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 1,256,080\n",
      "Trainable params: 1,256,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=categorical_crossentropy, optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 20)        520       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 14, 14, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 14, 14, 50)        25050     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 50)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2450)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 500)               1225500   \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 1,256,080\n",
      "Trainable params: 1,256,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 84s 2ms/step - loss: 0.1755 - acc: 0.9480 - val_loss: 0.0573 - val_acc: 0.9821\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.0479 - acc: 0.9850 - val_loss: 0.0425 - val_acc: 0.9870\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 81s 2ms/step - loss: 0.0326 - acc: 0.9895 - val_loss: 0.0448 - val_acc: 0.9865\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 81s 2ms/step - loss: 0.0257 - acc: 0.9916 - val_loss: 0.0392 - val_acc: 0.9880\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 81s 2ms/step - loss: 0.0181 - acc: 0.9943 - val_loss: 0.0393 - val_acc: 0.9871\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 81s 2ms/step - loss: 0.0127 - acc: 0.9960 - val_loss: 0.0340 - val_acc: 0.9905\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 82s 2ms/step - loss: 0.0113 - acc: 0.9963 - val_loss: 0.0330 - val_acc: 0.9899\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 82s 2ms/step - loss: 0.0094 - acc: 0.9968 - val_loss: 0.0342 - val_acc: 0.9900\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 81s 2ms/step - loss: 0.0082 - acc: 0.9971 - val_loss: 0.0320 - val_acc: 0.9918\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 81s 2ms/step - loss: 0.0068 - acc: 0.9976 - val_loss: 0.0411 - val_acc: 0.9906\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 81s 2ms/step - loss: 0.0059 - acc: 0.9979 - val_loss: 0.0419 - val_acc: 0.9895\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 81s 2ms/step - loss: 0.0059 - acc: 0.9978 - val_loss: 0.0506 - val_acc: 0.9884\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 81s 2ms/step - loss: 0.0057 - acc: 0.9981 - val_loss: 0.0417 - val_acc: 0.9892\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 83s 2ms/step - loss: 0.0034 - acc: 0.9989 - val_loss: 0.0450 - val_acc: 0.9904\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 84s 2ms/step - loss: 0.0060 - acc: 0.9980 - val_loss: 0.0391 - val_acc: 0.9906\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 84s 2ms/step - loss: 0.0033 - acc: 0.9990 - val_loss: 0.0387 - val_acc: 0.9915\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 82s 2ms/step - loss: 0.0065 - acc: 0.9980 - val_loss: 0.0377 - val_acc: 0.9902\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 81s 2ms/step - loss: 0.0016 - acc: 0.9996 - val_loss: 0.0463 - val_acc: 0.9906\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 85s 2ms/step - loss: 0.0038 - acc: 0.9987 - val_loss: 0.0454 - val_acc: 0.9897\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 82s 2ms/step - loss: 0.0041 - acc: 0.9986 - val_loss: 0.0350 - val_acc: 0.9922\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=128, epochs=20, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist results로 google 검색하면\n",
    "# 99.77%\n",
    "# 상위 100위 안에 들려면 무조건 99% 이상은 달성해야 함.\n",
    "# neural network를 거의 다 사용함. 이미지는 거의 이거를 사용함.\n",
    "# 이미지 --> CNN이 무조건 들어가야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaxPooling2D\n",
    "#- 풀을 끄집어 내서 대표값을 끄집어 낸다고 보면 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

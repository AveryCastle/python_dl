{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### tensorflow 최신버전이 아닌 1.9버전으로 설치하기\n",
    "##### 다른 모듈들이 이 버전에 못 따라와서 안정적인 게 1.9버전임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공공데이터포털: https://www.data.go.kr/\n",
    "# 세종말뭉치: https://ithub.korean.go.kr/user/guide/corpus/guide1.do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficent est. of word representations in vector space, # 2013 skip gram, Mikolov, Google NLP Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider with pytho\n",
    "# visual studio code: https://code.visualstudio.com/, javascript로 개발되어 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning framework power scores 2018\n",
    "# https://towardsdatascience.com/deep-learning-framework-power-scores-2018-23607ddf297a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense는 출력, 입력 순으로 지정함\n",
    "model.add(Dense(50, input_shape=(784,), activation='sigmoid', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(100, activation='sigmoid', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자동완성\n",
    "from keras.losses import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수를 최적화할 것. SGD(이게 가장 기본적임)\n",
    "model.compile(loss=categorical_crossentropy, optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepy.dataset import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_mnist(flatten=True, normalize=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 2.2701 - acc: 0.1966 - val_loss: 2.2237 - val_acc: 0.4039\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 2.1808 - acc: 0.4160 - val_loss: 2.1302 - val_acc: 0.4597\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 2.0682 - acc: 0.5173 - val_loss: 1.9928 - val_acc: 0.5581\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 1.9081 - acc: 0.5744 - val_loss: 1.8077 - val_acc: 0.6192\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 1.7107 - acc: 0.6230 - val_loss: 1.5980 - val_acc: 0.6483\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 1.5087 - acc: 0.6666 - val_loss: 1.4024 - val_acc: 0.6893\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 1.3305 - acc: 0.7067 - val_loss: 1.2349 - val_acc: 0.7333\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 3s 68us/step - loss: 1.1826 - acc: 0.7388 - val_loss: 1.0991 - val_acc: 0.7737\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 1.0612 - acc: 0.7655 - val_loss: 0.9869 - val_acc: 0.7974\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.9609 - acc: 0.7880 - val_loss: 0.8946 - val_acc: 0.8114\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.8774 - acc: 0.8037 - val_loss: 0.8172 - val_acc: 0.8251\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.8077 - acc: 0.8172 - val_loss: 0.7534 - val_acc: 0.8375\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.7495 - acc: 0.8272 - val_loss: 0.7000 - val_acc: 0.8433\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.7008 - acc: 0.8351 - val_loss: 0.6551 - val_acc: 0.8520\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.6597 - acc: 0.8420 - val_loss: 0.6181 - val_acc: 0.8566\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.6248 - acc: 0.8484 - val_loss: 0.5861 - val_acc: 0.8637\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.5951 - acc: 0.8537 - val_loss: 0.5589 - val_acc: 0.8683\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.5695 - acc: 0.8578 - val_loss: 0.5353 - val_acc: 0.8738\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.5470 - acc: 0.8625 - val_loss: 0.5147 - val_acc: 0.8753\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.5273 - acc: 0.8660 - val_loss: 0.4966 - val_acc: 0.8788\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.5098 - acc: 0.8691 - val_loss: 0.4808 - val_acc: 0.8823\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.4941 - acc: 0.8721 - val_loss: 0.4661 - val_acc: 0.8844\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.4801 - acc: 0.8751 - val_loss: 0.4530 - val_acc: 0.8872\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.4674 - acc: 0.8772 - val_loss: 0.4414 - val_acc: 0.8893\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.4558 - acc: 0.8801 - val_loss: 0.4309 - val_acc: 0.8896\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.4452 - acc: 0.8820 - val_loss: 0.4208 - val_acc: 0.8917\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.4355 - acc: 0.8841 - val_loss: 0.4117 - val_acc: 0.8928\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.4266 - acc: 0.8863 - val_loss: 0.4037 - val_acc: 0.8936\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.4183 - acc: 0.8881 - val_loss: 0.3960 - val_acc: 0.8953\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.4107 - acc: 0.8901 - val_loss: 0.3889 - val_acc: 0.8969\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.4036 - acc: 0.8911 - val_loss: 0.3822 - val_acc: 0.8986\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.3969 - acc: 0.8927 - val_loss: 0.3759 - val_acc: 0.9003\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.3907 - acc: 0.8941 - val_loss: 0.3705 - val_acc: 0.9010\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.3849 - acc: 0.8958 - val_loss: 0.3648 - val_acc: 0.9017\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.3795 - acc: 0.8967 - val_loss: 0.3598 - val_acc: 0.9031\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.3743 - acc: 0.8980 - val_loss: 0.3550 - val_acc: 0.9045\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.3695 - acc: 0.8989 - val_loss: 0.3505 - val_acc: 0.9053\n",
      "Epoch 38/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.3649 - acc: 0.9001 - val_loss: 0.3462 - val_acc: 0.9064\n",
      "Epoch 39/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.3606 - acc: 0.9005 - val_loss: 0.3423 - val_acc: 0.9072\n",
      "Epoch 40/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.3564 - acc: 0.9010 - val_loss: 0.3385 - val_acc: 0.9081\n",
      "Epoch 41/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.3525 - acc: 0.9024 - val_loss: 0.3349 - val_acc: 0.9089\n",
      "Epoch 42/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.3488 - acc: 0.9029 - val_loss: 0.3313 - val_acc: 0.9100\n",
      "Epoch 43/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.3452 - acc: 0.9035 - val_loss: 0.3279 - val_acc: 0.9108\n",
      "Epoch 44/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.3417 - acc: 0.9045 - val_loss: 0.3247 - val_acc: 0.9121\n",
      "Epoch 45/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.3385 - acc: 0.9047 - val_loss: 0.3216 - val_acc: 0.9124\n",
      "Epoch 46/100\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.3353 - acc: 0.9057 - val_loss: 0.3188 - val_acc: 0.9128\n",
      "Epoch 47/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.3323 - acc: 0.9065 - val_loss: 0.3162 - val_acc: 0.9125\n",
      "Epoch 48/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.3293 - acc: 0.9068 - val_loss: 0.3132 - val_acc: 0.9132\n",
      "Epoch 49/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.3265 - acc: 0.9079 - val_loss: 0.3108 - val_acc: 0.9134\n",
      "Epoch 50/100\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.3238 - acc: 0.9085 - val_loss: 0.3082 - val_acc: 0.9142\n",
      "Epoch 51/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.3211 - acc: 0.9086 - val_loss: 0.3059 - val_acc: 0.9147\n",
      "Epoch 52/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.3186 - acc: 0.9098 - val_loss: 0.3037 - val_acc: 0.9148\n",
      "Epoch 53/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.3161 - acc: 0.9102 - val_loss: 0.3014 - val_acc: 0.9154\n",
      "Epoch 54/100\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.3137 - acc: 0.9105 - val_loss: 0.2990 - val_acc: 0.9156\n",
      "Epoch 55/100\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.3114 - acc: 0.9112 - val_loss: 0.2970 - val_acc: 0.9167\n",
      "Epoch 56/100\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.3091 - acc: 0.9117 - val_loss: 0.2948 - val_acc: 0.9172\n",
      "Epoch 57/100\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.3069 - acc: 0.9125 - val_loss: 0.2930 - val_acc: 0.9172\n",
      "Epoch 58/100\n",
      "48000/48000 [==============================] - 3s 68us/step - loss: 0.3047 - acc: 0.9132 - val_loss: 0.2911 - val_acc: 0.9177\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.3026 - acc: 0.9136 - val_loss: 0.2893 - val_acc: 0.9180\n",
      "Epoch 60/100\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.3006 - acc: 0.9142 - val_loss: 0.2872 - val_acc: 0.9188\n",
      "Epoch 61/100\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 0.2985 - acc: 0.9149 - val_loss: 0.2855 - val_acc: 0.9193\n",
      "Epoch 62/100\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.2966 - acc: 0.9155 - val_loss: 0.2840 - val_acc: 0.9190\n",
      "Epoch 63/100\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.2947 - acc: 0.9160 - val_loss: 0.2821 - val_acc: 0.9198\n",
      "Epoch 64/100\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.2927 - acc: 0.9162 - val_loss: 0.2804 - val_acc: 0.9205\n",
      "Epoch 65/100\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.2909 - acc: 0.9169 - val_loss: 0.2791 - val_acc: 0.9201\n",
      "Epoch 66/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.2891 - acc: 0.9175 - val_loss: 0.2772 - val_acc: 0.9215\n",
      "Epoch 67/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.2874 - acc: 0.9180 - val_loss: 0.2758 - val_acc: 0.9214\n",
      "Epoch 68/100\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.2856 - acc: 0.9186 - val_loss: 0.2743 - val_acc: 0.9227\n",
      "Epoch 69/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.2840 - acc: 0.9187 - val_loss: 0.2726 - val_acc: 0.9230\n",
      "Epoch 70/100\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.2823 - acc: 0.9195 - val_loss: 0.2713 - val_acc: 0.9231\n",
      "Epoch 71/100\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.2806 - acc: 0.9200 - val_loss: 0.2699 - val_acc: 0.9237\n",
      "Epoch 72/100\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.2790 - acc: 0.9201 - val_loss: 0.2684 - val_acc: 0.9238\n",
      "Epoch 73/100\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.2774 - acc: 0.9204 - val_loss: 0.2670 - val_acc: 0.9241\n",
      "Epoch 74/100\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.2758 - acc: 0.9211 - val_loss: 0.2657 - val_acc: 0.9239\n",
      "Epoch 75/100\n",
      "48000/48000 [==============================] - 2s 52us/step - loss: 0.2744 - acc: 0.9216 - val_loss: 0.2643 - val_acc: 0.9248\n",
      "Epoch 76/100\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.2728 - acc: 0.9219 - val_loss: 0.2629 - val_acc: 0.9253\n",
      "Epoch 77/100\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.2713 - acc: 0.9223 - val_loss: 0.2617 - val_acc: 0.9256\n",
      "Epoch 78/100\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.2699 - acc: 0.9231 - val_loss: 0.2604 - val_acc: 0.9258\n",
      "Epoch 79/100\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.2684 - acc: 0.9236 - val_loss: 0.2593 - val_acc: 0.9261\n",
      "Epoch 80/100\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.2670 - acc: 0.9235 - val_loss: 0.2579 - val_acc: 0.9261\n",
      "Epoch 81/100\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.2655 - acc: 0.9244 - val_loss: 0.2569 - val_acc: 0.9264\n",
      "Epoch 82/100\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.2642 - acc: 0.9244 - val_loss: 0.2556 - val_acc: 0.9267\n",
      "Epoch 83/100\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.2628 - acc: 0.9245 - val_loss: 0.2544 - val_acc: 0.9273\n",
      "Epoch 84/100\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.2614 - acc: 0.9250 - val_loss: 0.2533 - val_acc: 0.9273\n",
      "Epoch 85/100\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.2601 - acc: 0.9252 - val_loss: 0.2522 - val_acc: 0.9281\n",
      "Epoch 86/100\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.2588 - acc: 0.9257 - val_loss: 0.2509 - val_acc: 0.9280\n",
      "Epoch 87/100\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.2575 - acc: 0.9263 - val_loss: 0.2499 - val_acc: 0.9278\n",
      "Epoch 88/100\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.2562 - acc: 0.9261 - val_loss: 0.2488 - val_acc: 0.9285\n",
      "Epoch 89/100\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.2549 - acc: 0.9268 - val_loss: 0.2477 - val_acc: 0.9288\n",
      "Epoch 90/100\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.2537 - acc: 0.9270 - val_loss: 0.2465 - val_acc: 0.9291\n",
      "Epoch 91/100\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.2524 - acc: 0.9275 - val_loss: 0.2454 - val_acc: 0.9294\n",
      "Epoch 92/100\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.2512 - acc: 0.9277 - val_loss: 0.2445 - val_acc: 0.9293\n",
      "Epoch 93/100\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.2499 - acc: 0.9280 - val_loss: 0.2434 - val_acc: 0.9297\n",
      "Epoch 94/100\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.2488 - acc: 0.9280 - val_loss: 0.2424 - val_acc: 0.9298\n",
      "Epoch 95/100\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.2475 - acc: 0.9286 - val_loss: 0.2414 - val_acc: 0.9301\n",
      "Epoch 96/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.2463 - acc: 0.9291 - val_loss: 0.2406 - val_acc: 0.9303\n",
      "Epoch 97/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.2452 - acc: 0.9292 - val_loss: 0.2395 - val_acc: 0.9301\n",
      "Epoch 98/100\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.2441 - acc: 0.9293 - val_loss: 0.2385 - val_acc: 0.9314\n",
      "Epoch 99/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.2429 - acc: 0.9298 - val_loss: 0.2376 - val_acc: 0.9308\n",
      "Epoch 100/100\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.2418 - acc: 0.9303 - val_loss: 0.2366 - val_acc: 0.9314\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=100, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 45,360\n",
      "Trainable params: 45,360\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.223730</td>\n",
       "      <td>0.403917</td>\n",
       "      <td>2.270078</td>\n",
       "      <td>0.196604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.130212</td>\n",
       "      <td>0.459667</td>\n",
       "      <td>2.180840</td>\n",
       "      <td>0.415958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.992754</td>\n",
       "      <td>0.558083</td>\n",
       "      <td>2.068214</td>\n",
       "      <td>0.517312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.807717</td>\n",
       "      <td>0.619250</td>\n",
       "      <td>1.908086</td>\n",
       "      <td>0.574354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.597969</td>\n",
       "      <td>0.648333</td>\n",
       "      <td>1.710686</td>\n",
       "      <td>0.622979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.402390</td>\n",
       "      <td>0.689333</td>\n",
       "      <td>1.508745</td>\n",
       "      <td>0.666625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.234914</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>1.330504</td>\n",
       "      <td>0.706708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.099074</td>\n",
       "      <td>0.773667</td>\n",
       "      <td>1.182623</td>\n",
       "      <td>0.738833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.986949</td>\n",
       "      <td>0.797417</td>\n",
       "      <td>1.061227</td>\n",
       "      <td>0.765479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.894561</td>\n",
       "      <td>0.811417</td>\n",
       "      <td>0.960948</td>\n",
       "      <td>0.787979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.817187</td>\n",
       "      <td>0.825083</td>\n",
       "      <td>0.877436</td>\n",
       "      <td>0.803750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.753402</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.807749</td>\n",
       "      <td>0.817250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.699955</td>\n",
       "      <td>0.843333</td>\n",
       "      <td>0.749486</td>\n",
       "      <td>0.827187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.655091</td>\n",
       "      <td>0.852000</td>\n",
       "      <td>0.700750</td>\n",
       "      <td>0.835083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.618107</td>\n",
       "      <td>0.856583</td>\n",
       "      <td>0.659663</td>\n",
       "      <td>0.841979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.586083</td>\n",
       "      <td>0.863667</td>\n",
       "      <td>0.624799</td>\n",
       "      <td>0.848354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.558867</td>\n",
       "      <td>0.868333</td>\n",
       "      <td>0.595112</td>\n",
       "      <td>0.853708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.535348</td>\n",
       "      <td>0.873833</td>\n",
       "      <td>0.569459</td>\n",
       "      <td>0.857792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.514736</td>\n",
       "      <td>0.875250</td>\n",
       "      <td>0.547030</td>\n",
       "      <td>0.862521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.496625</td>\n",
       "      <td>0.878833</td>\n",
       "      <td>0.527297</td>\n",
       "      <td>0.866021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.480779</td>\n",
       "      <td>0.882333</td>\n",
       "      <td>0.509795</td>\n",
       "      <td>0.869104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.466149</td>\n",
       "      <td>0.884417</td>\n",
       "      <td>0.494148</td>\n",
       "      <td>0.872125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.452961</td>\n",
       "      <td>0.887167</td>\n",
       "      <td>0.480071</td>\n",
       "      <td>0.875063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.441448</td>\n",
       "      <td>0.889333</td>\n",
       "      <td>0.467352</td>\n",
       "      <td>0.877208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.430859</td>\n",
       "      <td>0.889583</td>\n",
       "      <td>0.455792</td>\n",
       "      <td>0.880062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.420831</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>0.445239</td>\n",
       "      <td>0.882042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.411741</td>\n",
       "      <td>0.892833</td>\n",
       "      <td>0.435487</td>\n",
       "      <td>0.884063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.403726</td>\n",
       "      <td>0.893583</td>\n",
       "      <td>0.426555</td>\n",
       "      <td>0.886271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.396024</td>\n",
       "      <td>0.895250</td>\n",
       "      <td>0.418291</td>\n",
       "      <td>0.888104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.388909</td>\n",
       "      <td>0.896917</td>\n",
       "      <td>0.410698</td>\n",
       "      <td>0.890063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.269897</td>\n",
       "      <td>0.923667</td>\n",
       "      <td>0.280631</td>\n",
       "      <td>0.919979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.268358</td>\n",
       "      <td>0.923833</td>\n",
       "      <td>0.279010</td>\n",
       "      <td>0.920125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.266953</td>\n",
       "      <td>0.924083</td>\n",
       "      <td>0.277440</td>\n",
       "      <td>0.920417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.265665</td>\n",
       "      <td>0.923917</td>\n",
       "      <td>0.275840</td>\n",
       "      <td>0.921063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.264317</td>\n",
       "      <td>0.924833</td>\n",
       "      <td>0.274357</td>\n",
       "      <td>0.921625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.262929</td>\n",
       "      <td>0.925333</td>\n",
       "      <td>0.272806</td>\n",
       "      <td>0.921854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.261707</td>\n",
       "      <td>0.925583</td>\n",
       "      <td>0.271296</td>\n",
       "      <td>0.922271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.260401</td>\n",
       "      <td>0.925750</td>\n",
       "      <td>0.269861</td>\n",
       "      <td>0.923063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.259345</td>\n",
       "      <td>0.926083</td>\n",
       "      <td>0.268409</td>\n",
       "      <td>0.923604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.257872</td>\n",
       "      <td>0.926083</td>\n",
       "      <td>0.266958</td>\n",
       "      <td>0.923458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.256926</td>\n",
       "      <td>0.926417</td>\n",
       "      <td>0.265492</td>\n",
       "      <td>0.924438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.255590</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.264191</td>\n",
       "      <td>0.924375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.254428</td>\n",
       "      <td>0.927333</td>\n",
       "      <td>0.262816</td>\n",
       "      <td>0.924500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.253329</td>\n",
       "      <td>0.927250</td>\n",
       "      <td>0.261427</td>\n",
       "      <td>0.924958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.252246</td>\n",
       "      <td>0.928083</td>\n",
       "      <td>0.260123</td>\n",
       "      <td>0.925229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.250895</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0.258770</td>\n",
       "      <td>0.925708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.249945</td>\n",
       "      <td>0.927833</td>\n",
       "      <td>0.257451</td>\n",
       "      <td>0.926333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.248827</td>\n",
       "      <td>0.928500</td>\n",
       "      <td>0.256199</td>\n",
       "      <td>0.926146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.247745</td>\n",
       "      <td>0.928833</td>\n",
       "      <td>0.254898</td>\n",
       "      <td>0.926813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.246464</td>\n",
       "      <td>0.929083</td>\n",
       "      <td>0.253658</td>\n",
       "      <td>0.926979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.245413</td>\n",
       "      <td>0.929417</td>\n",
       "      <td>0.252420</td>\n",
       "      <td>0.927458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.244544</td>\n",
       "      <td>0.929333</td>\n",
       "      <td>0.251220</td>\n",
       "      <td>0.927729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.243448</td>\n",
       "      <td>0.929667</td>\n",
       "      <td>0.249950</td>\n",
       "      <td>0.928021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.242400</td>\n",
       "      <td>0.929833</td>\n",
       "      <td>0.248770</td>\n",
       "      <td>0.928042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.930083</td>\n",
       "      <td>0.247525</td>\n",
       "      <td>0.928646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.240591</td>\n",
       "      <td>0.930250</td>\n",
       "      <td>0.246334</td>\n",
       "      <td>0.929104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.239500</td>\n",
       "      <td>0.930083</td>\n",
       "      <td>0.245160</td>\n",
       "      <td>0.929167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.238544</td>\n",
       "      <td>0.931417</td>\n",
       "      <td>0.244075</td>\n",
       "      <td>0.929313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.237570</td>\n",
       "      <td>0.930750</td>\n",
       "      <td>0.242894</td>\n",
       "      <td>0.929750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.236553</td>\n",
       "      <td>0.931417</td>\n",
       "      <td>0.241807</td>\n",
       "      <td>0.930333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    val_loss   val_acc      loss       acc\n",
       "0   2.223730  0.403917  2.270078  0.196604\n",
       "1   2.130212  0.459667  2.180840  0.415958\n",
       "2   1.992754  0.558083  2.068214  0.517312\n",
       "3   1.807717  0.619250  1.908086  0.574354\n",
       "4   1.597969  0.648333  1.710686  0.622979\n",
       "5   1.402390  0.689333  1.508745  0.666625\n",
       "6   1.234914  0.733333  1.330504  0.706708\n",
       "7   1.099074  0.773667  1.182623  0.738833\n",
       "8   0.986949  0.797417  1.061227  0.765479\n",
       "9   0.894561  0.811417  0.960948  0.787979\n",
       "10  0.817187  0.825083  0.877436  0.803750\n",
       "11  0.753402  0.837500  0.807749  0.817250\n",
       "12  0.699955  0.843333  0.749486  0.827187\n",
       "13  0.655091  0.852000  0.700750  0.835083\n",
       "14  0.618107  0.856583  0.659663  0.841979\n",
       "15  0.586083  0.863667  0.624799  0.848354\n",
       "16  0.558867  0.868333  0.595112  0.853708\n",
       "17  0.535348  0.873833  0.569459  0.857792\n",
       "18  0.514736  0.875250  0.547030  0.862521\n",
       "19  0.496625  0.878833  0.527297  0.866021\n",
       "20  0.480779  0.882333  0.509795  0.869104\n",
       "21  0.466149  0.884417  0.494148  0.872125\n",
       "22  0.452961  0.887167  0.480071  0.875063\n",
       "23  0.441448  0.889333  0.467352  0.877208\n",
       "24  0.430859  0.889583  0.455792  0.880062\n",
       "25  0.420831  0.891667  0.445239  0.882042\n",
       "26  0.411741  0.892833  0.435487  0.884063\n",
       "27  0.403726  0.893583  0.426555  0.886271\n",
       "28  0.396024  0.895250  0.418291  0.888104\n",
       "29  0.388909  0.896917  0.410698  0.890063\n",
       "..       ...       ...       ...       ...\n",
       "70  0.269897  0.923667  0.280631  0.919979\n",
       "71  0.268358  0.923833  0.279010  0.920125\n",
       "72  0.266953  0.924083  0.277440  0.920417\n",
       "73  0.265665  0.923917  0.275840  0.921063\n",
       "74  0.264317  0.924833  0.274357  0.921625\n",
       "75  0.262929  0.925333  0.272806  0.921854\n",
       "76  0.261707  0.925583  0.271296  0.922271\n",
       "77  0.260401  0.925750  0.269861  0.923063\n",
       "78  0.259345  0.926083  0.268409  0.923604\n",
       "79  0.257872  0.926083  0.266958  0.923458\n",
       "80  0.256926  0.926417  0.265492  0.924438\n",
       "81  0.255590  0.926667  0.264191  0.924375\n",
       "82  0.254428  0.927333  0.262816  0.924500\n",
       "83  0.253329  0.927250  0.261427  0.924958\n",
       "84  0.252246  0.928083  0.260123  0.925229\n",
       "85  0.250895  0.928000  0.258770  0.925708\n",
       "86  0.249945  0.927833  0.257451  0.926333\n",
       "87  0.248827  0.928500  0.256199  0.926146\n",
       "88  0.247745  0.928833  0.254898  0.926813\n",
       "89  0.246464  0.929083  0.253658  0.926979\n",
       "90  0.245413  0.929417  0.252420  0.927458\n",
       "91  0.244544  0.929333  0.251220  0.927729\n",
       "92  0.243448  0.929667  0.249950  0.928021\n",
       "93  0.242400  0.929833  0.248770  0.928042\n",
       "94  0.241379  0.930083  0.247525  0.928646\n",
       "95  0.240591  0.930250  0.246334  0.929104\n",
       "96  0.239500  0.930083  0.245160  0.929167\n",
       "97  0.238544  0.931417  0.244075  0.929313\n",
       "98  0.237570  0.930750  0.242894  0.929750\n",
       "99  0.236553  0.931417  0.241807  0.930333\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result = pd.DataFrame(history.history)\n",
    "train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result[['loss', 'val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result[['loss', 'val_acc']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(50, input_shape=(784,), activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=categorical_crossentropy, optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=100, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = pd.DataFrame(history.history)\n",
    "train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result[['loss', 'val_loss', 'val_acc']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 회귀 데이터 다룸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "columns = boston.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

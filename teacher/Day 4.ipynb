{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load neuralnet_backprop.py\n",
    "import numpy as np\n",
    "\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c)\n",
    "    y = exp_a / np.sum(exp_a)\n",
    "    return y\n",
    "\n",
    "def softmax_batch(A):\n",
    "    return np.apply_along_axis(arr=A, axis=1, func1d=softmax)\n",
    "\n",
    "def cross_entroy_error(y_pred, y):\n",
    "    \"\"\"분류용 손실함수\"\"\"\n",
    "    delta = 1e-7 # 아주 작은 값.     \n",
    "    return -np.sum(y * np.log(y_pred + delta))\n",
    "\n",
    "def cross_entropy_error_batch(y_pred, y):\n",
    "    batch_size = len(y)\n",
    "    cse = cross_entroy_error(y_pred, y) / batch_size\n",
    "    return cse\n",
    "\n",
    "\n",
    "class ReLu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = x > 0\n",
    "        return np.where(self.mask, x, 0)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return np.where(self.mask, 1, 0)\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.out = 1 / (1 + np.exp(-x))\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, 입력수, 출력수):\n",
    "        self.W = np.random.randn(입력수, 출력수)\n",
    "        self.b = np.random.randn(출력수)\n",
    "        self.X = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        z = np.dot(X, self.W) + self.b\n",
    "        return z\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dX = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.X.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dX\n",
    "\n",
    "\n",
    "class SoftmaxCrossEntropy:\n",
    "    def __init__(self):\n",
    "        self.Y = None\n",
    "        self.Y_pred = None\n",
    "        \n",
    "    def forward(self, X, Y):\n",
    "        self.Y = Y\n",
    "        self.Y_pred = softmax_batch(X)\n",
    "        loss = cross_entropy_error_batch(self.Y_pred, self.Y)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = len(self.Y)\n",
    "        dX = (self.Y_pred - self.Y) / batch_size\n",
    "        return dX\n",
    "\n",
    "\n",
    "class FeedForwadNet:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        layer_output = X\n",
    "        for layer in self.layers[:-1]:\n",
    "            layer_output = layer.forward(layer_output)            \n",
    "        return layer_output\n",
    "    \n",
    "    def compute_loss(self, X, Y):\n",
    "        Y_pred = self.predict(X)\n",
    "        loss = self.layers[-1].forward(Y_pred, Y)\n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X, y, 배치크기, 학습횟수, 학습률):\n",
    "        loss_history = []\n",
    "        for i in range(학습횟수):\n",
    "            # 1. 미니배치\n",
    "            샘플수 = len(X)\n",
    "            배치색인 = np.random.choice(샘플수, 배치크기)\n",
    "            X_batch = X[배치색인]\n",
    "            y_batch = y[배치색인]\n",
    "            # 2. 기울기 산출\n",
    "            #  1) 순전파\n",
    "            self.compute_loss(X_batch, y_batch)\n",
    "            #  2) 역전파\n",
    "            dout = 1\n",
    "            for layer in reversed(self.layers):\n",
    "                dout = layer.backward(dout)\n",
    "            # 3. 갱신\n",
    "            for layer in self.layers:\n",
    "                if isinstance(layer, Affine):\n",
    "                    layer.W -= layer.dW * 학습률\n",
    "                    layer.b -= layer.db * 학습률\n",
    "            \n",
    "            # 손실 확인\n",
    "            loss = self.compute_loss(X_batch, y_batch)\n",
    "            loss_history.append(loss)\n",
    "            print('[학습 {}] Loss: {}'.format(i+1, loss))\n",
    "        \n",
    "        return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deepy.dataset import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = \\\n",
    "    mnist.load_mnist(flatten=True, normalize=True, \n",
    "                     one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = FeedForwadNet()\n",
    "model.add(Affine(784, 50))\n",
    "model.add(Sigmoid())\n",
    "model.add(Affine(50, 100))\n",
    "model.add(Sigmoid())\n",
    "model.add(Affine(100, 10))\n",
    "model.add(SoftmaxCrossEntropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습 1] Loss: 7.3538629866503\n",
      "[학습 2] Loss: 6.478018785314634\n",
      "[학습 3] Loss: 6.2001104801063684\n",
      "[학습 4] Loss: 6.7245169474241075\n",
      "[학습 5] Loss: 6.085240713806126\n",
      "[학습 6] Loss: 6.434449759427757\n",
      "[학습 7] Loss: 6.688489615482231\n",
      "[학습 8] Loss: 7.697694085291639\n",
      "[학습 9] Loss: 5.926272879976865\n",
      "[학습 10] Loss: 5.845396627366892\n",
      "[학습 11] Loss: 6.499744925216391\n",
      "[학습 12] Loss: 5.9523532793543765\n",
      "[학습 13] Loss: 6.123332432468417\n",
      "[학습 14] Loss: 5.196958847940855\n",
      "[학습 15] Loss: 5.246512673443999\n",
      "[학습 16] Loss: 6.217034278463884\n",
      "[학습 17] Loss: 5.706453306679389\n",
      "[학습 18] Loss: 5.609612738568266\n",
      "[학습 19] Loss: 6.026331260990281\n",
      "[학습 20] Loss: 5.259047978547528\n",
      "[학습 21] Loss: 5.194316668666122\n",
      "[학습 22] Loss: 5.622107769938461\n",
      "[학습 23] Loss: 5.202883168471692\n",
      "[학습 24] Loss: 5.311122960914872\n",
      "[학습 25] Loss: 5.201595661585674\n",
      "[학습 26] Loss: 4.6230584779341655\n",
      "[학습 27] Loss: 4.682271865221903\n",
      "[학습 28] Loss: 5.436516049965041\n",
      "[학습 29] Loss: 4.827969310591617\n",
      "[학습 30] Loss: 5.55611003662809\n",
      "[학습 31] Loss: 4.619391928267166\n",
      "[학습 32] Loss: 5.0010053527243805\n",
      "[학습 33] Loss: 5.0420624792959465\n",
      "[학습 34] Loss: 4.871159199523635\n",
      "[학습 35] Loss: 5.147910551985384\n",
      "[학습 36] Loss: 4.789702036036066\n",
      "[학습 37] Loss: 4.689446049725032\n",
      "[학습 38] Loss: 4.376673747955681\n",
      "[학습 39] Loss: 4.916162888571811\n",
      "[학습 40] Loss: 4.703950019039258\n",
      "[학습 41] Loss: 5.11525687659908\n",
      "[학습 42] Loss: 4.51093531918386\n",
      "[학습 43] Loss: 4.825089854397877\n",
      "[학습 44] Loss: 4.516962313086416\n",
      "[학습 45] Loss: 4.621395071783207\n",
      "[학습 46] Loss: 4.395025319550114\n",
      "[학습 47] Loss: 4.505457086872573\n",
      "[학습 48] Loss: 4.227636074804936\n",
      "[학습 49] Loss: 4.586159606107235\n",
      "[학습 50] Loss: 4.473929858026288\n",
      "[학습 51] Loss: 4.231697200394198\n",
      "[학습 52] Loss: 4.5431336482354165\n",
      "[학습 53] Loss: 4.132699074076089\n",
      "[학습 54] Loss: 4.373855014353299\n",
      "[학습 55] Loss: 4.7346567953130325\n",
      "[학습 56] Loss: 3.8859364529797586\n",
      "[학습 57] Loss: 3.937822054130357\n",
      "[학습 58] Loss: 4.322773848673685\n",
      "[학습 59] Loss: 4.217820694451614\n",
      "[학습 60] Loss: 4.116941969683405\n",
      "[학습 61] Loss: 3.9850055993906865\n",
      "[학습 62] Loss: 4.627794558227033\n",
      "[학습 63] Loss: 4.334279028901095\n",
      "[학습 64] Loss: 3.716230199239111\n",
      "[학습 65] Loss: 3.5581617729213115\n",
      "[학습 66] Loss: 4.222558092538507\n",
      "[학습 67] Loss: 4.438296955849007\n",
      "[학습 68] Loss: 3.924616838735194\n",
      "[학습 69] Loss: 3.8464358323137056\n",
      "[학습 70] Loss: 4.169176876790825\n",
      "[학습 71] Loss: 4.214839938563248\n",
      "[학습 72] Loss: 4.2689594549919585\n",
      "[학습 73] Loss: 4.189755302596449\n",
      "[학습 74] Loss: 4.23372853942098\n",
      "[학습 75] Loss: 3.882101604854928\n",
      "[학습 76] Loss: 4.158978545352345\n",
      "[학습 77] Loss: 4.245665058254377\n",
      "[학습 78] Loss: 3.6790283680959077\n",
      "[학습 79] Loss: 4.642145869971327\n",
      "[학습 80] Loss: 4.741803292476786\n",
      "[학습 81] Loss: 4.229771086195942\n",
      "[학습 82] Loss: 3.8313397374830176\n",
      "[학습 83] Loss: 4.253733116787216\n",
      "[학습 84] Loss: 4.071419062340828\n",
      "[학습 85] Loss: 3.957929499257964\n",
      "[학습 86] Loss: 3.9460861220603167\n",
      "[학습 87] Loss: 3.817303803882635\n",
      "[학습 88] Loss: 4.610174382370962\n",
      "[학습 89] Loss: 3.97170810846152\n",
      "[학습 90] Loss: 3.495920193249335\n",
      "[학습 91] Loss: 3.772666122183698\n",
      "[학습 92] Loss: 3.792236102600274\n",
      "[학습 93] Loss: 3.9445509506360903\n",
      "[학습 94] Loss: 3.7898628808691024\n",
      "[학습 95] Loss: 3.984110778684659\n",
      "[학습 96] Loss: 3.475548118342881\n",
      "[학습 97] Loss: 3.8529753258414767\n",
      "[학습 98] Loss: 3.7779270485616405\n",
      "[학습 99] Loss: 3.8839366121984074\n",
      "[학습 100] Loss: 3.8073790488774044\n",
      "[학습 101] Loss: 3.845491486143526\n",
      "[학습 102] Loss: 4.170123167136021\n",
      "[학습 103] Loss: 3.778223144664762\n",
      "[학습 104] Loss: 4.0525346674006935\n",
      "[학습 105] Loss: 3.7725615641365575\n",
      "[학습 106] Loss: 3.9621091697720168\n",
      "[학습 107] Loss: 3.960145823666868\n",
      "[학습 108] Loss: 4.111930827728832\n",
      "[학습 109] Loss: 3.7504610091498476\n",
      "[학습 110] Loss: 3.8249740950569553\n",
      "[학습 111] Loss: 3.8597775579836546\n",
      "[학습 112] Loss: 3.579538558889624\n",
      "[학습 113] Loss: 3.709539437309937\n",
      "[학습 114] Loss: 3.828506490695903\n",
      "[학습 115] Loss: 3.866677402978637\n",
      "[학습 116] Loss: 4.403850247105008\n",
      "[학습 117] Loss: 3.895271462133182\n",
      "[학습 118] Loss: 3.906357092742312\n",
      "[학습 119] Loss: 3.5680761758946717\n",
      "[학습 120] Loss: 3.73122550863103\n",
      "[학습 121] Loss: 3.3868497562819453\n",
      "[학습 122] Loss: 3.6193916218020075\n",
      "[학습 123] Loss: 3.467388353764402\n",
      "[학습 124] Loss: 3.2003936847556758\n",
      "[학습 125] Loss: 3.7425243964100217\n",
      "[학습 126] Loss: 3.5271940023213144\n",
      "[학습 127] Loss: 3.7391525436490554\n",
      "[학습 128] Loss: 3.7739783543766827\n",
      "[학습 129] Loss: 3.2414431010737097\n",
      "[학습 130] Loss: 3.6953759089697966\n",
      "[학습 131] Loss: 3.625238496188839\n",
      "[학습 132] Loss: 4.108877014800896\n",
      "[학습 133] Loss: 3.602648101602872\n",
      "[학습 134] Loss: 3.7043961446000857\n",
      "[학습 135] Loss: 3.7319518537841283\n",
      "[학습 136] Loss: 3.5798826133657475\n",
      "[학습 137] Loss: 3.348510296807889\n",
      "[학습 138] Loss: 3.302331126473614\n",
      "[학습 139] Loss: 3.7726337902555276\n",
      "[학습 140] Loss: 3.4270156764017417\n",
      "[학습 141] Loss: 3.709176522750752\n",
      "[학습 142] Loss: 3.298775114722217\n",
      "[학습 143] Loss: 3.458657461327671\n",
      "[학습 144] Loss: 3.3978775378017945\n",
      "[학습 145] Loss: 3.59852025738155\n",
      "[학습 146] Loss: 3.996645953464807\n",
      "[학습 147] Loss: 3.5134387551451995\n",
      "[학습 148] Loss: 3.715206763803758\n",
      "[학습 149] Loss: 3.451837518991853\n",
      "[학습 150] Loss: 3.3850300905281467\n",
      "[학습 151] Loss: 3.7097490239216713\n",
      "[학습 152] Loss: 3.21514973287592\n",
      "[학습 153] Loss: 4.040766426997991\n",
      "[학습 154] Loss: 3.627519748934394\n",
      "[학습 155] Loss: 3.4795573455072155\n",
      "[학습 156] Loss: 3.5403052492008884\n",
      "[학습 157] Loss: 3.366057683978803\n",
      "[학습 158] Loss: 3.671529642135052\n",
      "[학습 159] Loss: 3.4186600569734695\n",
      "[학습 160] Loss: 3.5314473294958986\n",
      "[학습 161] Loss: 3.6681544585593735\n",
      "[학습 162] Loss: 3.1615569622085093\n",
      "[학습 163] Loss: 3.725795992454168\n",
      "[학습 164] Loss: 3.8475721323939136\n",
      "[학습 165] Loss: 3.646744677674758\n",
      "[학습 166] Loss: 3.665729492520136\n",
      "[학습 167] Loss: 3.2814482819086845\n",
      "[학습 168] Loss: 3.305791458068654\n",
      "[학습 169] Loss: 3.9296464529065283\n",
      "[학습 170] Loss: 3.508294965538864\n",
      "[학습 171] Loss: 3.321196519719072\n",
      "[학습 172] Loss: 3.711282249559507\n",
      "[학습 173] Loss: 3.2906262129455337\n",
      "[학습 174] Loss: 3.8444353789259407\n",
      "[학습 175] Loss: 3.802930149572413\n",
      "[학습 176] Loss: 3.448784101485808\n",
      "[학습 177] Loss: 3.4131115602133675\n",
      "[학습 178] Loss: 3.4567575126077923\n",
      "[학습 179] Loss: 3.5989933236158134\n",
      "[학습 180] Loss: 3.517627338303166\n",
      "[학습 181] Loss: 3.2936365454205783\n",
      "[학습 182] Loss: 3.675883352472415\n",
      "[학습 183] Loss: 3.3864352126349733\n",
      "[학습 184] Loss: 3.3291630072742953\n",
      "[학습 185] Loss: 3.2671218504031456\n",
      "[학습 186] Loss: 3.2923962676945253\n",
      "[학습 187] Loss: 3.3865152753522954\n",
      "[학습 188] Loss: 3.616899849101975\n",
      "[학습 189] Loss: 3.5357053518185904\n",
      "[학습 190] Loss: 3.72344351649592\n",
      "[학습 191] Loss: 3.3609122912637575\n",
      "[학습 192] Loss: 3.060002646841843\n",
      "[학습 193] Loss: 3.334168374892211\n",
      "[학습 194] Loss: 3.596832524257377\n",
      "[학습 195] Loss: 3.295474909232754\n",
      "[학습 196] Loss: 3.642882332676541\n",
      "[학습 197] Loss: 3.2879672888580944\n",
      "[학습 198] Loss: 3.394177660143656\n",
      "[학습 199] Loss: 3.4164037119048736\n",
      "[학습 200] Loss: 3.31318502854141\n",
      "[학습 201] Loss: 3.526563103000466\n",
      "[학습 202] Loss: 3.2081099349222053\n",
      "[학습 203] Loss: 2.9116236114525806\n",
      "[학습 204] Loss: 3.4658439888754486\n",
      "[학습 205] Loss: 3.1706916668190877\n",
      "[학습 206] Loss: 3.2363954983731036\n",
      "[학습 207] Loss: 3.6325348781495523\n",
      "[학습 208] Loss: 3.077103066182541\n",
      "[학습 209] Loss: 3.4324711234965046\n",
      "[학습 210] Loss: 3.154825353863223\n",
      "[학습 211] Loss: 3.4816599677553985\n",
      "[학습 212] Loss: 2.9731126006058433\n",
      "[학습 213] Loss: 3.321150744295859\n",
      "[학습 214] Loss: 3.6269778076958983\n",
      "[학습 215] Loss: 3.292371548305953\n",
      "[학습 216] Loss: 3.1625999511456873\n",
      "[학습 217] Loss: 3.3418565232061646\n",
      "[학습 218] Loss: 2.9844746307692005\n",
      "[학습 219] Loss: 3.2484751780268564\n",
      "[학습 220] Loss: 3.1655469544610573\n",
      "[학습 221] Loss: 3.063505359665554\n",
      "[학습 222] Loss: 3.6618405752736645\n",
      "[학습 223] Loss: 2.9258694310115074\n",
      "[학습 224] Loss: 3.404489813068588\n",
      "[학습 225] Loss: 3.321776968314728\n",
      "[학습 226] Loss: 3.2888146615556866\n",
      "[학습 227] Loss: 3.1571685587838347\n",
      "[학습 228] Loss: 3.2774148742184654\n",
      "[학습 229] Loss: 3.1246346466512254\n",
      "[학습 230] Loss: 3.1986903378037015\n",
      "[학습 231] Loss: 2.9504063761688344\n",
      "[학습 232] Loss: 3.1451042626746095\n",
      "[학습 233] Loss: 3.0539889268155673\n",
      "[학습 234] Loss: 3.235346410283892\n",
      "[학습 235] Loss: 3.3548964839401894\n",
      "[학습 236] Loss: 3.7739764082610576\n",
      "[학습 237] Loss: 3.2378015206629116\n",
      "[학습 238] Loss: 3.254926782894464\n",
      "[학습 239] Loss: 3.1743592712697746\n",
      "[학습 240] Loss: 3.4047055738131133\n",
      "[학습 241] Loss: 3.3645454629383367\n",
      "[학습 242] Loss: 3.227836530891939\n",
      "[학습 243] Loss: 3.4486646147953515\n",
      "[학습 244] Loss: 3.1855392779936347\n",
      "[학습 245] Loss: 3.2542625781576633\n",
      "[학습 246] Loss: 3.498088549616591\n",
      "[학습 247] Loss: 3.0773380658341636\n",
      "[학습 248] Loss: 3.260391035944957\n",
      "[학습 249] Loss: 3.0142545082337118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습 250] Loss: 3.165110599724535\n",
      "[학습 251] Loss: 3.037981325499126\n",
      "[학습 252] Loss: 3.3474251140956484\n",
      "[학습 253] Loss: 3.029829927558538\n",
      "[학습 254] Loss: 2.9709247861604315\n",
      "[학습 255] Loss: 2.8998263154722115\n",
      "[학습 256] Loss: 2.988434208879537\n",
      "[학습 257] Loss: 3.1457560362873886\n",
      "[학습 258] Loss: 3.1441571724548907\n",
      "[학습 259] Loss: 3.2332034466447443\n",
      "[학습 260] Loss: 3.246041710733315\n",
      "[학습 261] Loss: 3.118115888111704\n",
      "[학습 262] Loss: 2.9894708453118346\n",
      "[학습 263] Loss: 2.8480610370078336\n",
      "[학습 264] Loss: 3.478060796781431\n",
      "[학습 265] Loss: 3.0201885512753175\n",
      "[학습 266] Loss: 3.155192273958534\n",
      "[학습 267] Loss: 3.345152667234785\n",
      "[학습 268] Loss: 3.2320333488891886\n",
      "[학습 269] Loss: 2.9903982274809873\n",
      "[학습 270] Loss: 2.9259484002399265\n",
      "[학습 271] Loss: 3.192211150367912\n",
      "[학습 272] Loss: 3.417636251675998\n",
      "[학습 273] Loss: 2.8815375606743108\n",
      "[학습 274] Loss: 3.219172921163891\n",
      "[학습 275] Loss: 2.9514239012299823\n",
      "[학습 276] Loss: 3.1490496989762162\n",
      "[학습 277] Loss: 2.7833923354812216\n",
      "[학습 278] Loss: 3.327142061051072\n",
      "[학습 279] Loss: 3.314824204093168\n",
      "[학습 280] Loss: 2.9888485555717885\n",
      "[학습 281] Loss: 3.1595130681604484\n",
      "[학습 282] Loss: 2.8954130806891865\n",
      "[학습 283] Loss: 3.098410475062116\n",
      "[학습 284] Loss: 3.063122981338145\n",
      "[학습 285] Loss: 2.600458533520922\n",
      "[학습 286] Loss: 2.791627495447427\n",
      "[학습 287] Loss: 3.1325618744140935\n",
      "[학습 288] Loss: 2.9493243008856145\n",
      "[학습 289] Loss: 3.1341342277513284\n",
      "[학습 290] Loss: 2.877805706421382\n",
      "[학습 291] Loss: 3.1635799995792753\n",
      "[학습 292] Loss: 3.2315794401677\n",
      "[학습 293] Loss: 2.9397244299547594\n",
      "[학습 294] Loss: 2.9772140214802323\n",
      "[학습 295] Loss: 2.887805248543543\n",
      "[학습 296] Loss: 2.8429478982590353\n",
      "[학습 297] Loss: 2.98332145193374\n",
      "[학습 298] Loss: 2.958824694655774\n",
      "[학습 299] Loss: 2.8390161397840576\n",
      "[학습 300] Loss: 2.7565842592598653\n",
      "[학습 301] Loss: 3.0764081495173548\n",
      "[학습 302] Loss: 2.924155801549351\n",
      "[학습 303] Loss: 3.1221939263178085\n",
      "[학습 304] Loss: 2.882214598583185\n",
      "[학습 305] Loss: 3.0064140221117452\n",
      "[학습 306] Loss: 2.935700258392982\n",
      "[학습 307] Loss: 3.0120328571074464\n",
      "[학습 308] Loss: 2.949066867995458\n",
      "[학습 309] Loss: 3.2618268791892673\n",
      "[학습 310] Loss: 3.5468578161341977\n",
      "[학습 311] Loss: 2.851185460165608\n",
      "[학습 312] Loss: 3.1693588195762548\n",
      "[학습 313] Loss: 3.232208871811854\n",
      "[학습 314] Loss: 2.9670654552641618\n",
      "[학습 315] Loss: 3.264667836230493\n",
      "[학습 316] Loss: 3.3492714472280944\n",
      "[학습 317] Loss: 2.8663916685879145\n",
      "[학습 318] Loss: 2.687448895230306\n",
      "[학습 319] Loss: 3.0033576961219173\n",
      "[학습 320] Loss: 2.8839683890840484\n",
      "[학습 321] Loss: 3.1365419086164446\n",
      "[학습 322] Loss: 2.9217366805433524\n",
      "[학습 323] Loss: 3.1246514235943916\n",
      "[학습 324] Loss: 2.6779338549626237\n",
      "[학습 325] Loss: 3.163845056226299\n",
      "[학습 326] Loss: 2.860139619435498\n",
      "[학습 327] Loss: 3.0178727479816847\n",
      "[학습 328] Loss: 2.7210228239423704\n",
      "[학습 329] Loss: 2.828361555206098\n",
      "[학습 330] Loss: 2.946831296471211\n",
      "[학습 331] Loss: 2.9278780284039896\n",
      "[학습 332] Loss: 2.631557740972604\n",
      "[학습 333] Loss: 2.8564045617841782\n",
      "[학습 334] Loss: 3.1555501796363843\n",
      "[학습 335] Loss: 2.914807573312439\n",
      "[학습 336] Loss: 2.9891941030100155\n",
      "[학습 337] Loss: 3.342802119635514\n",
      "[학습 338] Loss: 3.032703577517834\n",
      "[학습 339] Loss: 2.7924197836094056\n",
      "[학습 340] Loss: 3.00808745367101\n",
      "[학습 341] Loss: 2.8039728114552736\n",
      "[학습 342] Loss: 2.926526471281775\n",
      "[학습 343] Loss: 2.7031628356962485\n",
      "[학습 344] Loss: 2.7334508895540623\n",
      "[학습 345] Loss: 2.9588736920370398\n",
      "[학습 346] Loss: 2.9299563185030104\n",
      "[학습 347] Loss: 2.8264706419415706\n",
      "[학습 348] Loss: 2.8075490254351068\n",
      "[학습 349] Loss: 2.8875792983775215\n",
      "[학습 350] Loss: 2.6521759674536933\n",
      "[학습 351] Loss: 2.956373985975387\n",
      "[학습 352] Loss: 3.1536166251032465\n",
      "[학습 353] Loss: 3.102297431117359\n",
      "[학습 354] Loss: 2.9747634846405417\n",
      "[학습 355] Loss: 2.5552540423737815\n",
      "[학습 356] Loss: 2.9379108750547\n",
      "[학습 357] Loss: 2.9345183592245068\n",
      "[학습 358] Loss: 2.7405815832079226\n",
      "[학습 359] Loss: 3.165631972506253\n",
      "[학습 360] Loss: 2.5173314678434515\n",
      "[학습 361] Loss: 3.17310345540507\n",
      "[학습 362] Loss: 2.8989666389702267\n",
      "[학습 363] Loss: 2.638940773862405\n",
      "[학습 364] Loss: 3.013331391317601\n",
      "[학습 365] Loss: 2.598771313240078\n",
      "[학습 366] Loss: 3.1334044608181717\n",
      "[학습 367] Loss: 2.6260723514731024\n",
      "[학습 368] Loss: 2.768864159319824\n",
      "[학습 369] Loss: 3.1685251618395016\n",
      "[학습 370] Loss: 2.7022286691965633\n",
      "[학습 371] Loss: 2.8952608501329298\n",
      "[학습 372] Loss: 2.462452466592505\n",
      "[학습 373] Loss: 2.512448117305061\n",
      "[학습 374] Loss: 2.800156645924785\n",
      "[학습 375] Loss: 3.0244598065355235\n",
      "[학습 376] Loss: 2.94731712150577\n",
      "[학습 377] Loss: 2.5221513786126994\n",
      "[학습 378] Loss: 2.842535485823407\n",
      "[학습 379] Loss: 3.137793117892493\n",
      "[학습 380] Loss: 2.8803450715083\n",
      "[학습 381] Loss: 2.7455659210455736\n",
      "[학습 382] Loss: 2.67838368292239\n",
      "[학습 383] Loss: 2.711468733377275\n",
      "[학습 384] Loss: 2.931127552303002\n",
      "[학습 385] Loss: 2.692474754228719\n",
      "[학습 386] Loss: 2.7970363012056487\n",
      "[학습 387] Loss: 3.219408690702268\n",
      "[학습 388] Loss: 2.59020598288948\n",
      "[학습 389] Loss: 2.934358517854032\n",
      "[학습 390] Loss: 2.497255135791947\n",
      "[학습 391] Loss: 2.9253095097316035\n",
      "[학습 392] Loss: 2.7064733905250424\n",
      "[학습 393] Loss: 2.6690321028283024\n",
      "[학습 394] Loss: 2.7772833092100564\n",
      "[학습 395] Loss: 2.9815321077145582\n",
      "[학습 396] Loss: 2.9288127075776607\n",
      "[학습 397] Loss: 2.6758752823349687\n",
      "[학습 398] Loss: 2.88250987538955\n",
      "[학습 399] Loss: 2.6739743065415085\n",
      "[학습 400] Loss: 2.9318642677930096\n",
      "[학습 401] Loss: 2.748907962222072\n",
      "[학습 402] Loss: 2.5129514683047858\n",
      "[학습 403] Loss: 2.792042926896088\n",
      "[학습 404] Loss: 2.7364196469891784\n",
      "[학습 405] Loss: 2.5606949934100034\n",
      "[학습 406] Loss: 2.4693073935776613\n",
      "[학습 407] Loss: 2.6950101336645793\n",
      "[학습 408] Loss: 2.8482585145824957\n",
      "[학습 409] Loss: 2.736221342064184\n",
      "[학습 410] Loss: 2.759721312903365\n",
      "[학습 411] Loss: 2.6997940977812855\n",
      "[학습 412] Loss: 2.7190096207379564\n",
      "[학습 413] Loss: 2.7740595508764905\n",
      "[학습 414] Loss: 2.742713587448977\n",
      "[학습 415] Loss: 2.802864410364348\n",
      "[학습 416] Loss: 2.6235869653065116\n",
      "[학습 417] Loss: 2.944573966237448\n",
      "[학습 418] Loss: 2.588966912642381\n",
      "[학습 419] Loss: 2.756455308791609\n",
      "[학습 420] Loss: 2.8548037205954686\n",
      "[학습 421] Loss: 2.889205586702718\n",
      "[학습 422] Loss: 2.739752990011898\n",
      "[학습 423] Loss: 2.876770943763436\n",
      "[학습 424] Loss: 2.5707516754253517\n",
      "[학습 425] Loss: 2.638560722061484\n",
      "[학습 426] Loss: 2.6017971331153897\n",
      "[학습 427] Loss: 2.7922446847614593\n",
      "[학습 428] Loss: 2.4214030915136773\n",
      "[학습 429] Loss: 2.617311577539218\n",
      "[학습 430] Loss: 2.845811521555295\n",
      "[학습 431] Loss: 2.736489794467073\n",
      "[학습 432] Loss: 2.7383559058949616\n",
      "[학습 433] Loss: 2.6104440742251427\n",
      "[학습 434] Loss: 2.4377728445330633\n",
      "[학습 435] Loss: 2.8562884487332396\n",
      "[학습 436] Loss: 2.6890511361896356\n",
      "[학습 437] Loss: 2.744866820113314\n",
      "[학습 438] Loss: 2.621045139990811\n",
      "[학습 439] Loss: 2.4810093953632224\n",
      "[학습 440] Loss: 2.736508774653448\n",
      "[학습 441] Loss: 2.5445142383074466\n",
      "[학습 442] Loss: 2.723903955807349\n",
      "[학습 443] Loss: 2.642732779454482\n",
      "[학습 444] Loss: 2.8113048604611985\n",
      "[학습 445] Loss: 2.5103294198346555\n",
      "[학습 446] Loss: 2.8351552074300113\n",
      "[학습 447] Loss: 2.767757809188029\n",
      "[학습 448] Loss: 2.5717135006611893\n",
      "[학습 449] Loss: 2.389648294561429\n",
      "[학습 450] Loss: 2.6170915765807963\n",
      "[학습 451] Loss: 2.808906496644014\n",
      "[학습 452] Loss: 2.528302945933645\n",
      "[학습 453] Loss: 2.905489724716866\n",
      "[학습 454] Loss: 2.5106626372789047\n",
      "[학습 455] Loss: 2.588704221465781\n",
      "[학습 456] Loss: 2.497879775024452\n",
      "[학습 457] Loss: 2.377288693609441\n",
      "[학습 458] Loss: 2.486774278340586\n",
      "[학습 459] Loss: 2.499727868693816\n",
      "[학습 460] Loss: 2.682621179040702\n",
      "[학습 461] Loss: 2.438353493346878\n",
      "[학습 462] Loss: 2.62482052291283\n",
      "[학습 463] Loss: 2.7849706772611507\n",
      "[학습 464] Loss: 2.687956581938269\n",
      "[학습 465] Loss: 2.64527082646595\n",
      "[학습 466] Loss: 2.896227508103848\n",
      "[학습 467] Loss: 2.813860305283011\n",
      "[학습 468] Loss: 2.5215401199890484\n",
      "[학습 469] Loss: 2.578104435513973\n",
      "[학습 470] Loss: 2.5290186528622187\n",
      "[학습 471] Loss: 2.490126423889095\n",
      "[학습 472] Loss: 2.692826704745444\n",
      "[학습 473] Loss: 2.516972667630276\n",
      "[학습 474] Loss: 2.522687776348938\n",
      "[학습 475] Loss: 2.528407216666143\n",
      "[학습 476] Loss: 2.4945828856206083\n",
      "[학습 477] Loss: 2.530864905747446\n",
      "[학습 478] Loss: 2.7021107153526738\n",
      "[학습 479] Loss: 2.523244430018891\n",
      "[학습 480] Loss: 2.735594987960526\n",
      "[학습 481] Loss: 2.444181502615481\n",
      "[학습 482] Loss: 2.822364994577116\n",
      "[학습 483] Loss: 2.38680264069461\n",
      "[학습 484] Loss: 2.6997634494401637\n",
      "[학습 485] Loss: 2.648500664744585\n",
      "[학습 486] Loss: 2.7506936779331412\n",
      "[학습 487] Loss: 2.4900880807326957\n",
      "[학습 488] Loss: 2.4549713387923715\n",
      "[학습 489] Loss: 2.3641370332551257\n",
      "[학습 490] Loss: 2.6448061930241904\n",
      "[학습 491] Loss: 2.770085405839233\n",
      "[학습 492] Loss: 2.346385755936439\n",
      "[학습 493] Loss: 2.3168952859611993\n",
      "[학습 494] Loss: 2.149009738024215\n",
      "[학습 495] Loss: 2.630600773993079\n",
      "[학습 496] Loss: 2.3631098549320892\n",
      "[학습 497] Loss: 2.4624641422617755\n",
      "[학습 498] Loss: 2.8765113541205793\n",
      "[학습 499] Loss: 2.3860862653982493\n",
      "[학습 500] Loss: 2.572689531874712\n",
      "[학습 501] Loss: 2.228868881205992\n",
      "[학습 502] Loss: 2.537292016945865\n",
      "[학습 503] Loss: 2.4982469970238985\n",
      "[학습 504] Loss: 2.395833169476715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습 505] Loss: 2.4016637814989266\n",
      "[학습 506] Loss: 2.601148194365901\n",
      "[학습 507] Loss: 2.419651808780573\n",
      "[학습 508] Loss: 2.7748766476206437\n",
      "[학습 509] Loss: 2.709284177639374\n",
      "[학습 510] Loss: 2.5265293299100318\n",
      "[학습 511] Loss: 2.7145201744202176\n",
      "[학습 512] Loss: 2.4415058875302464\n",
      "[학습 513] Loss: 2.593321740476812\n",
      "[학습 514] Loss: 2.3982916978374504\n",
      "[학습 515] Loss: 2.5824727322944443\n",
      "[학습 516] Loss: 2.570391007311298\n",
      "[학습 517] Loss: 2.6331137369453774\n",
      "[학습 518] Loss: 2.127788705931045\n",
      "[학습 519] Loss: 2.2229685934931256\n",
      "[학습 520] Loss: 2.089201479150416\n",
      "[학습 521] Loss: 2.647901948840473\n",
      "[학습 522] Loss: 2.540754790782426\n",
      "[학습 523] Loss: 2.164305915370119\n",
      "[학습 524] Loss: 2.4133734785342718\n",
      "[학습 525] Loss: 2.5231639416995284\n",
      "[학습 526] Loss: 2.6606579728467192\n",
      "[학습 527] Loss: 2.43613956505586\n",
      "[학습 528] Loss: 2.5349821359489115\n",
      "[학습 529] Loss: 2.2651140153219074\n",
      "[학습 530] Loss: 2.440018698953343\n",
      "[학습 531] Loss: 2.663919714194189\n",
      "[학습 532] Loss: 2.272764525251626\n",
      "[학습 533] Loss: 2.449290646928921\n",
      "[학습 534] Loss: 2.6001284252128345\n",
      "[학습 535] Loss: 2.659906230632256\n",
      "[학습 536] Loss: 2.596209023126992\n",
      "[학습 537] Loss: 2.463026941543917\n",
      "[학습 538] Loss: 2.3740980987813405\n",
      "[학습 539] Loss: 2.3367224672982925\n",
      "[학습 540] Loss: 2.3297856367974306\n",
      "[학습 541] Loss: 2.6753376350205644\n",
      "[학습 542] Loss: 2.2951847492183846\n",
      "[학습 543] Loss: 2.6046201062685723\n",
      "[학습 544] Loss: 2.704758705027725\n",
      "[학습 545] Loss: 2.6621161708634133\n",
      "[학습 546] Loss: 2.5629959967781586\n",
      "[학습 547] Loss: 2.3396055005433816\n",
      "[학습 548] Loss: 2.475353539447929\n",
      "[학습 549] Loss: 2.336945917501516\n",
      "[학습 550] Loss: 2.414459092810052\n",
      "[학습 551] Loss: 2.913744994994963\n",
      "[학습 552] Loss: 2.5073004551541156\n",
      "[학습 553] Loss: 2.3242387083330387\n",
      "[학습 554] Loss: 2.2654691807075076\n",
      "[학습 555] Loss: 2.29289459220165\n",
      "[학습 556] Loss: 2.153715282259636\n",
      "[학습 557] Loss: 2.00898369336109\n",
      "[학습 558] Loss: 2.603598834463294\n",
      "[학습 559] Loss: 2.52616717304786\n",
      "[학습 560] Loss: 2.375800868050498\n",
      "[학습 561] Loss: 2.4845028469748898\n",
      "[학습 562] Loss: 2.4948227740129427\n",
      "[학습 563] Loss: 2.3897229326434783\n",
      "[학습 564] Loss: 2.6526261960700617\n",
      "[학습 565] Loss: 2.19471200122513\n",
      "[학습 566] Loss: 2.107957264776386\n",
      "[학습 567] Loss: 2.4249851084771428\n",
      "[학습 568] Loss: 2.818444505799687\n",
      "[학습 569] Loss: 2.5685889066228613\n",
      "[학습 570] Loss: 2.225692014305807\n",
      "[학습 571] Loss: 2.358876421042957\n",
      "[학습 572] Loss: 2.557345312320404\n",
      "[학습 573] Loss: 2.177658819829745\n",
      "[학습 574] Loss: 2.39703002934107\n",
      "[학습 575] Loss: 2.54838939501213\n",
      "[학습 576] Loss: 2.3146058622541426\n",
      "[학습 577] Loss: 2.18926457944057\n",
      "[학습 578] Loss: 2.1460112857025084\n",
      "[학습 579] Loss: 2.477140997458205\n",
      "[학습 580] Loss: 2.566463164599925\n",
      "[학습 581] Loss: 2.4754144622457126\n",
      "[학습 582] Loss: 2.2792834236505346\n",
      "[학습 583] Loss: 2.203319768649303\n",
      "[학습 584] Loss: 2.48923253651477\n",
      "[학습 585] Loss: 2.607688335120696\n",
      "[학습 586] Loss: 2.313930507608568\n",
      "[학습 587] Loss: 2.6528135014669623\n",
      "[학습 588] Loss: 2.6293509484124176\n",
      "[학습 589] Loss: 2.3516355462283363\n",
      "[학습 590] Loss: 2.2493789880893336\n",
      "[학습 591] Loss: 2.603032995940373\n",
      "[학습 592] Loss: 2.39088460113514\n",
      "[학습 593] Loss: 2.2424418055284083\n",
      "[학습 594] Loss: 2.477597226392315\n",
      "[학습 595] Loss: 2.5518984018439337\n",
      "[학습 596] Loss: 2.0709679626993487\n",
      "[학습 597] Loss: 2.436292690041178\n",
      "[학습 598] Loss: 2.545858109404287\n",
      "[학습 599] Loss: 2.234445449151274\n",
      "[학습 600] Loss: 2.4589116346516353\n",
      "[학습 601] Loss: 2.3086417441175344\n",
      "[학습 602] Loss: 2.5881891013613814\n",
      "[학습 603] Loss: 2.2101435210417724\n",
      "[학습 604] Loss: 2.224645782197484\n",
      "[학습 605] Loss: 2.5155777166490294\n",
      "[학습 606] Loss: 2.732152936764073\n",
      "[학습 607] Loss: 2.366522656361897\n",
      "[학습 608] Loss: 2.4628140392137765\n",
      "[학습 609] Loss: 2.4455184706622903\n",
      "[학습 610] Loss: 2.5370802223487607\n",
      "[학습 611] Loss: 2.570941996104565\n",
      "[학습 612] Loss: 2.3390360884531436\n",
      "[학습 613] Loss: 2.568310241087241\n",
      "[학습 614] Loss: 2.530582050556647\n",
      "[학습 615] Loss: 2.5766167652818384\n",
      "[학습 616] Loss: 2.3799278283376357\n",
      "[학습 617] Loss: 2.340945398753714\n",
      "[학습 618] Loss: 2.4687800463555605\n",
      "[학습 619] Loss: 2.5339734624529813\n",
      "[학습 620] Loss: 2.7085031779313224\n",
      "[학습 621] Loss: 2.2972948530225685\n",
      "[학습 622] Loss: 2.153609029118529\n",
      "[학습 623] Loss: 2.3114669824118534\n",
      "[학습 624] Loss: 2.2550342583194998\n",
      "[학습 625] Loss: 2.2738622208680495\n",
      "[학습 626] Loss: 2.0899648329733918\n",
      "[학습 627] Loss: 2.2656258632360875\n",
      "[학습 628] Loss: 2.325323037896723\n",
      "[학습 629] Loss: 2.195466477531447\n",
      "[학습 630] Loss: 2.236358509573179\n",
      "[학습 631] Loss: 2.332520217654066\n",
      "[학습 632] Loss: 2.3940307193195083\n",
      "[학습 633] Loss: 2.387489436248748\n",
      "[학습 634] Loss: 2.5320326529924673\n",
      "[학습 635] Loss: 2.3930494246281393\n",
      "[학습 636] Loss: 2.1064148004325545\n",
      "[학습 637] Loss: 1.9345966972667554\n",
      "[학습 638] Loss: 2.4179181930628104\n",
      "[학습 639] Loss: 2.617505104376211\n",
      "[학습 640] Loss: 2.5884076182224116\n",
      "[학습 641] Loss: 2.4295810521781056\n",
      "[학습 642] Loss: 2.351947514991961\n",
      "[학습 643] Loss: 2.2557578458269747\n",
      "[학습 644] Loss: 2.2463833041521326\n",
      "[학습 645] Loss: 2.1188740388895906\n",
      "[학습 646] Loss: 2.1089543786006257\n",
      "[학습 647] Loss: 2.3956532461656117\n",
      "[학습 648] Loss: 2.2334466229665275\n",
      "[학습 649] Loss: 2.391782300456199\n",
      "[학습 650] Loss: 2.457366988198578\n",
      "[학습 651] Loss: 2.562767028639464\n",
      "[학습 652] Loss: 2.1539482636893332\n",
      "[학습 653] Loss: 2.1967421564882823\n",
      "[학습 654] Loss: 2.602695027583114\n",
      "[학습 655] Loss: 2.4328186548162547\n",
      "[학습 656] Loss: 2.3226768449622903\n",
      "[학습 657] Loss: 2.300511250600257\n",
      "[학습 658] Loss: 2.3283249059821958\n",
      "[학습 659] Loss: 2.227767142722954\n",
      "[학습 660] Loss: 2.398473773911219\n",
      "[학습 661] Loss: 2.2003826072196215\n",
      "[학습 662] Loss: 2.3962816184104185\n",
      "[학습 663] Loss: 2.1298350616372814\n",
      "[학습 664] Loss: 2.243835027218208\n",
      "[학습 665] Loss: 2.368811765865564\n",
      "[학습 666] Loss: 2.1483301999305664\n",
      "[학습 667] Loss: 2.2938170182044906\n",
      "[학습 668] Loss: 2.2228336039577203\n",
      "[학습 669] Loss: 2.512322361801649\n",
      "[학습 670] Loss: 2.3179973128533926\n",
      "[학습 671] Loss: 2.2056056418781407\n",
      "[학습 672] Loss: 2.5198334757865677\n",
      "[학습 673] Loss: 2.6123661326677654\n",
      "[학습 674] Loss: 2.1781970531450914\n",
      "[학습 675] Loss: 2.1403290538098814\n",
      "[학습 676] Loss: 2.379402708489823\n",
      "[학습 677] Loss: 2.3834088201307524\n",
      "[학습 678] Loss: 2.2357933468858437\n",
      "[학습 679] Loss: 2.2816217044202274\n",
      "[학습 680] Loss: 2.172755681919403\n",
      "[학습 681] Loss: 2.108451303888823\n",
      "[학습 682] Loss: 2.180143542139323\n",
      "[학습 683] Loss: 2.696685498424439\n",
      "[학습 684] Loss: 2.2575288295025113\n",
      "[학습 685] Loss: 2.1487818967028556\n",
      "[학습 686] Loss: 2.237035080213171\n",
      "[학습 687] Loss: 2.3520521591653476\n",
      "[학습 688] Loss: 2.3854459380576083\n",
      "[학습 689] Loss: 2.0550983284440445\n",
      "[학습 690] Loss: 2.181531625065209\n",
      "[학습 691] Loss: 2.2129883325587936\n",
      "[학습 692] Loss: 2.131210602214629\n",
      "[학습 693] Loss: 2.293737716507562\n",
      "[학습 694] Loss: 2.4533383329479417\n",
      "[학습 695] Loss: 2.2797843659216706\n",
      "[학습 696] Loss: 2.320089485263873\n",
      "[학습 697] Loss: 2.4082176533054755\n",
      "[학습 698] Loss: 1.9255819123109765\n",
      "[학습 699] Loss: 2.1578180378918086\n",
      "[학습 700] Loss: 2.1775307370516868\n",
      "[학습 701] Loss: 2.1186041370682314\n",
      "[학습 702] Loss: 2.1640401603570023\n",
      "[학습 703] Loss: 2.461838529227716\n",
      "[학습 704] Loss: 2.291212928642859\n",
      "[학습 705] Loss: 2.1689016050517744\n",
      "[학습 706] Loss: 2.3258290488447857\n",
      "[학습 707] Loss: 2.422507315417072\n",
      "[학습 708] Loss: 2.274604429243751\n",
      "[학습 709] Loss: 2.2768084334034246\n",
      "[학습 710] Loss: 2.190464351449369\n",
      "[학습 711] Loss: 2.2390736637173614\n",
      "[학습 712] Loss: 1.942748773803803\n",
      "[학습 713] Loss: 2.283703116095447\n",
      "[학습 714] Loss: 2.2776855306263584\n",
      "[학습 715] Loss: 2.0222039721938323\n",
      "[학습 716] Loss: 2.152703123851813\n",
      "[학습 717] Loss: 2.20602679412789\n",
      "[학습 718] Loss: 2.145292228842118\n",
      "[학습 719] Loss: 1.96858131272761\n",
      "[학습 720] Loss: 2.118124807606404\n",
      "[학습 721] Loss: 2.1455596563228236\n",
      "[학습 722] Loss: 2.1645259301984363\n",
      "[학습 723] Loss: 2.368983089011166\n",
      "[학습 724] Loss: 2.0497241297684963\n",
      "[학습 725] Loss: 2.3819626633835345\n",
      "[학습 726] Loss: 2.179704283558682\n",
      "[학습 727] Loss: 2.274067353229465\n",
      "[학습 728] Loss: 2.375749188683472\n",
      "[학습 729] Loss: 2.1127957422132657\n",
      "[학습 730] Loss: 2.184510717716771\n",
      "[학습 731] Loss: 2.2922925707074193\n",
      "[학습 732] Loss: 1.822305922637269\n",
      "[학습 733] Loss: 2.1363869885658726\n",
      "[학습 734] Loss: 2.5907586237386098\n",
      "[학습 735] Loss: 2.285571457228654\n",
      "[학습 736] Loss: 1.9392533668930505\n",
      "[학습 737] Loss: 2.424665656464381\n",
      "[학습 738] Loss: 2.0618832914926113\n",
      "[학습 739] Loss: 2.2314587755798354\n",
      "[학습 740] Loss: 2.168884328278718\n",
      "[학습 741] Loss: 2.26158953121256\n",
      "[학습 742] Loss: 1.9813016780401915\n",
      "[학습 743] Loss: 2.2002049704453426\n",
      "[학습 744] Loss: 1.9894816904750785\n",
      "[학습 745] Loss: 2.3540008606069507\n",
      "[학습 746] Loss: 2.143801113638539\n",
      "[학습 747] Loss: 2.37240511361407\n",
      "[학습 748] Loss: 2.1091338186839925\n",
      "[학습 749] Loss: 2.5183294193486705\n",
      "[학습 750] Loss: 2.174651388124885\n",
      "[학습 751] Loss: 2.1032817635698935\n",
      "[학습 752] Loss: 1.8782213854031062\n",
      "[학습 753] Loss: 2.2450754301852847\n",
      "[학습 754] Loss: 2.18664515387222\n",
      "[학습 755] Loss: 2.021534601246166\n",
      "[학습 756] Loss: 2.256744046216198\n",
      "[학습 757] Loss: 2.0880194034425346\n",
      "[학습 758] Loss: 2.2532096372473362\n",
      "[학습 759] Loss: 2.2844971200324404\n",
      "[학습 760] Loss: 2.1166000420429407\n",
      "[학습 761] Loss: 2.128976956211469\n",
      "[학습 762] Loss: 2.1240538562774014\n",
      "[학습 763] Loss: 2.516697422616814\n",
      "[학습 764] Loss: 2.06176003959559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습 765] Loss: 2.216649600474337\n",
      "[학습 766] Loss: 2.374967988574169\n",
      "[학습 767] Loss: 2.199720109750437\n",
      "[학습 768] Loss: 1.912679210059553\n",
      "[학습 769] Loss: 2.112032867675597\n",
      "[학습 770] Loss: 2.052239710338158\n",
      "[학습 771] Loss: 2.134688111101914\n",
      "[학습 772] Loss: 1.971744796900004\n",
      "[학습 773] Loss: 1.8183002186474362\n",
      "[학습 774] Loss: 2.0404937281473656\n",
      "[학습 775] Loss: 2.1407025654694305\n",
      "[학습 776] Loss: 2.085163550317178\n",
      "[학습 777] Loss: 2.2017734431708584\n",
      "[학습 778] Loss: 2.248821509101655\n",
      "[학습 779] Loss: 2.497836351636845\n",
      "[학습 780] Loss: 2.0582778504170203\n",
      "[학습 781] Loss: 2.1782565641146228\n",
      "[학습 782] Loss: 2.161183254866952\n",
      "[학습 783] Loss: 2.143260661158603\n",
      "[학습 784] Loss: 2.138191171812258\n",
      "[학습 785] Loss: 2.4218818725402524\n",
      "[학습 786] Loss: 2.2323292923689735\n",
      "[학습 787] Loss: 2.284376893725425\n",
      "[학습 788] Loss: 1.9755902054092354\n",
      "[학습 789] Loss: 2.4363485300035466\n",
      "[학습 790] Loss: 2.101119563795958\n",
      "[학습 791] Loss: 2.1351125066508048\n",
      "[학습 792] Loss: 2.203812391044458\n",
      "[학습 793] Loss: 2.2219845625974917\n",
      "[학습 794] Loss: 2.1821849578197354\n",
      "[학습 795] Loss: 2.1608649684738386\n",
      "[학습 796] Loss: 2.325696301441931\n",
      "[학습 797] Loss: 2.123810339934466\n",
      "[학습 798] Loss: 1.9701996197612963\n",
      "[학습 799] Loss: 2.2227954177537463\n",
      "[학습 800] Loss: 1.8987198547357118\n",
      "[학습 801] Loss: 2.4386721292222218\n",
      "[학습 802] Loss: 1.958049904726998\n",
      "[학습 803] Loss: 2.090605057298424\n",
      "[학습 804] Loss: 1.9912459333628627\n",
      "[학습 805] Loss: 2.2403757037757877\n",
      "[학습 806] Loss: 2.002639894707258\n",
      "[학습 807] Loss: 2.2984257792759557\n",
      "[학습 808] Loss: 1.733108053339476\n",
      "[학습 809] Loss: 2.0280017550266147\n",
      "[학습 810] Loss: 2.0850874831095583\n",
      "[학습 811] Loss: 2.0327911417802995\n",
      "[학습 812] Loss: 2.3506393059232824\n",
      "[학습 813] Loss: 2.1203279180919536\n",
      "[학습 814] Loss: 1.9381198926485999\n",
      "[학습 815] Loss: 2.1623889820333364\n",
      "[학습 816] Loss: 2.1551601750558866\n",
      "[학습 817] Loss: 2.123213461956326\n",
      "[학습 818] Loss: 2.330882549072037\n",
      "[학습 819] Loss: 2.1892310654734346\n",
      "[학습 820] Loss: 1.9401917604438557\n",
      "[학습 821] Loss: 1.9416838270235184\n",
      "[학습 822] Loss: 1.9998578216389382\n",
      "[학습 823] Loss: 1.9841176100707414\n",
      "[학습 824] Loss: 2.119213004506038\n",
      "[학습 825] Loss: 2.3499433637922906\n",
      "[학습 826] Loss: 2.3660143852849504\n",
      "[학습 827] Loss: 2.110313944469677\n",
      "[학습 828] Loss: 2.3117249832088906\n",
      "[학습 829] Loss: 2.0030982799127166\n",
      "[학습 830] Loss: 2.0968560080188654\n",
      "[학습 831] Loss: 2.0479916721365212\n",
      "[학습 832] Loss: 2.16531540130995\n",
      "[학습 833] Loss: 2.2745221784824348\n",
      "[학습 834] Loss: 2.090185912548898\n",
      "[학습 835] Loss: 1.8634642589282258\n",
      "[학습 836] Loss: 2.0826343076659093\n",
      "[학습 837] Loss: 2.2571080806316557\n",
      "[학습 838] Loss: 2.100999634255283\n",
      "[학습 839] Loss: 1.9071105671117516\n",
      "[학습 840] Loss: 1.8124309351860444\n",
      "[학습 841] Loss: 1.7404025832832724\n",
      "[학습 842] Loss: 2.04563400284653\n",
      "[학습 843] Loss: 2.186814358237112\n",
      "[학습 844] Loss: 2.042688899475399\n",
      "[학습 845] Loss: 2.246388170714189\n",
      "[학습 846] Loss: 2.27418432367548\n",
      "[학습 847] Loss: 2.2405376235015604\n",
      "[학습 848] Loss: 2.080769490866375\n",
      "[학습 849] Loss: 2.169451351855523\n",
      "[학습 850] Loss: 1.9198879728911031\n",
      "[학습 851] Loss: 2.213412442642997\n",
      "[학습 852] Loss: 2.2437890584525153\n",
      "[학습 853] Loss: 1.8109041695572514\n",
      "[학습 854] Loss: 2.3431499243054703\n",
      "[학습 855] Loss: 1.9947622660233844\n",
      "[학습 856] Loss: 2.0488336271606418\n",
      "[학습 857] Loss: 2.072176964281721\n",
      "[학습 858] Loss: 2.278426246066159\n",
      "[학습 859] Loss: 1.989178477640076\n",
      "[학습 860] Loss: 2.2770981056068944\n",
      "[학습 861] Loss: 2.167683475101023\n",
      "[학습 862] Loss: 2.3105778043402614\n",
      "[학습 863] Loss: 2.132173724948665\n",
      "[학습 864] Loss: 2.148860061797838\n",
      "[학습 865] Loss: 2.015458128255801\n",
      "[학습 866] Loss: 2.1809953029653824\n",
      "[학습 867] Loss: 1.9699732963512895\n",
      "[학습 868] Loss: 2.154582397707959\n",
      "[학습 869] Loss: 2.2547837471592653\n",
      "[학습 870] Loss: 1.7958658660187872\n",
      "[학습 871] Loss: 2.0540715653313772\n",
      "[학습 872] Loss: 2.209170993333678\n",
      "[학습 873] Loss: 2.1214101063899626\n",
      "[학습 874] Loss: 2.1142371208364765\n",
      "[학습 875] Loss: 1.8732962103323356\n",
      "[학습 876] Loss: 2.04890989261745\n",
      "[학습 877] Loss: 2.073491839613545\n",
      "[학습 878] Loss: 1.565734578655944\n",
      "[학습 879] Loss: 2.4301770593617493\n",
      "[학습 880] Loss: 1.8577657194249304\n",
      "[학습 881] Loss: 1.9936771071642552\n",
      "[학습 882] Loss: 1.969906230314582\n",
      "[학습 883] Loss: 1.9589602442097591\n",
      "[학습 884] Loss: 2.217906245720057\n",
      "[학습 885] Loss: 2.1516493891663604\n",
      "[학습 886] Loss: 2.002836375060844\n",
      "[학습 887] Loss: 2.0242374559026097\n",
      "[학습 888] Loss: 2.357075337877352\n",
      "[학습 889] Loss: 1.9060940432074258\n",
      "[학습 890] Loss: 1.9027044458775206\n",
      "[학습 891] Loss: 1.780644158460318\n",
      "[학습 892] Loss: 2.010988163702028\n",
      "[학습 893] Loss: 2.0402832909950646\n",
      "[학습 894] Loss: 1.879205468618452\n",
      "[학습 895] Loss: 2.2658057684303294\n",
      "[학습 896] Loss: 2.003332610650831\n",
      "[학습 897] Loss: 2.0766778159291746\n",
      "[학습 898] Loss: 2.1991618900649597\n",
      "[학습 899] Loss: 2.245249134172725\n",
      "[학습 900] Loss: 1.7865592017251657\n",
      "[학습 901] Loss: 1.7792992472013858\n",
      "[학습 902] Loss: 2.1024163273587892\n",
      "[학습 903] Loss: 2.0327953643446155\n",
      "[학습 904] Loss: 2.0936052158094314\n",
      "[학습 905] Loss: 2.1142509684266115\n",
      "[학습 906] Loss: 2.1043353810295735\n",
      "[학습 907] Loss: 2.2017017913163497\n",
      "[학습 908] Loss: 1.8478634462061476\n",
      "[학습 909] Loss: 1.8567300181452402\n",
      "[학습 910] Loss: 2.1401104066225374\n",
      "[학습 911] Loss: 2.3140728158678967\n",
      "[학습 912] Loss: 2.2772254633102436\n",
      "[학습 913] Loss: 2.333497700852709\n",
      "[학습 914] Loss: 1.9315438479688867\n",
      "[학습 915] Loss: 2.020262638898998\n",
      "[학습 916] Loss: 2.309022535216754\n",
      "[학습 917] Loss: 2.135775832118266\n",
      "[학습 918] Loss: 1.8940652748873419\n",
      "[학습 919] Loss: 2.256602841937381\n",
      "[학습 920] Loss: 1.989997806588849\n",
      "[학습 921] Loss: 2.253973326209121\n",
      "[학습 922] Loss: 1.9073471444994539\n",
      "[학습 923] Loss: 2.0992011786967693\n",
      "[학습 924] Loss: 1.8987046748145429\n",
      "[학습 925] Loss: 2.1476961982044473\n",
      "[학습 926] Loss: 2.103037189912503\n",
      "[학습 927] Loss: 1.8383621048632905\n",
      "[학습 928] Loss: 2.011324954981089\n",
      "[학습 929] Loss: 1.7805984901092342\n",
      "[학습 930] Loss: 1.6983995429004297\n",
      "[학습 931] Loss: 2.232239552348092\n",
      "[학습 932] Loss: 1.866123566539452\n",
      "[학습 933] Loss: 2.0168411853185417\n",
      "[학습 934] Loss: 1.7584705925216673\n",
      "[학습 935] Loss: 2.2112787627831993\n",
      "[학습 936] Loss: 2.0501504257665566\n",
      "[학습 937] Loss: 2.2309331786410764\n",
      "[학습 938] Loss: 1.9631509651027954\n",
      "[학습 939] Loss: 1.921899953233036\n",
      "[학습 940] Loss: 1.9544965720200327\n",
      "[학습 941] Loss: 1.9723545689170998\n",
      "[학습 942] Loss: 2.016718567759238\n",
      "[학습 943] Loss: 2.2376523162483957\n",
      "[학습 944] Loss: 2.0917134014442813\n",
      "[학습 945] Loss: 1.8880525312926477\n",
      "[학습 946] Loss: 2.0387140488861295\n",
      "[학습 947] Loss: 1.7683388378807643\n",
      "[학습 948] Loss: 2.100237283145784\n",
      "[학습 949] Loss: 1.9113854869482356\n",
      "[학습 950] Loss: 1.931175766388585\n",
      "[학습 951] Loss: 2.0440794639150504\n",
      "[학습 952] Loss: 1.7640170913217137\n",
      "[학습 953] Loss: 2.0653462771456397\n",
      "[학습 954] Loss: 1.9562359297141756\n",
      "[학습 955] Loss: 2.0834868028031237\n",
      "[학습 956] Loss: 2.0280119319546337\n",
      "[학습 957] Loss: 1.8407033030731876\n",
      "[학습 958] Loss: 1.9665272545435755\n",
      "[학습 959] Loss: 2.1723188485906983\n",
      "[학습 960] Loss: 1.7467778786003278\n",
      "[학습 961] Loss: 1.8716478796050606\n",
      "[학습 962] Loss: 1.8992202140563583\n",
      "[학습 963] Loss: 2.094878329341056\n",
      "[학습 964] Loss: 2.0502741582327477\n",
      "[학습 965] Loss: 1.5835145464831464\n",
      "[학습 966] Loss: 2.2277523163615824\n",
      "[학습 967] Loss: 2.02956944024299\n",
      "[학습 968] Loss: 1.9257942388753657\n",
      "[학습 969] Loss: 1.8886531036784089\n",
      "[학습 970] Loss: 1.9486148484105092\n",
      "[학습 971] Loss: 1.9369615349118714\n",
      "[학습 972] Loss: 1.9009474887783642\n",
      "[학습 973] Loss: 1.9839927079880542\n",
      "[학습 974] Loss: 2.21502885990681\n",
      "[학습 975] Loss: 1.8685947462921448\n",
      "[학습 976] Loss: 1.8679066526417614\n",
      "[학습 977] Loss: 1.848573547785668\n",
      "[학습 978] Loss: 1.9592774742883154\n",
      "[학습 979] Loss: 2.1034788438673555\n",
      "[학습 980] Loss: 2.031812012869592\n",
      "[학습 981] Loss: 2.1273024922518275\n",
      "[학습 982] Loss: 2.2063800522340964\n",
      "[학습 983] Loss: 1.9562820390243616\n",
      "[학습 984] Loss: 2.202492197704675\n",
      "[학습 985] Loss: 2.021058719053964\n",
      "[학습 986] Loss: 1.968594768790765\n",
      "[학습 987] Loss: 1.8544908261480684\n",
      "[학습 988] Loss: 1.790330299332448\n",
      "[학습 989] Loss: 1.896941004072711\n",
      "[학습 990] Loss: 2.0745344610376515\n",
      "[학습 991] Loss: 1.7024938894826982\n",
      "[학습 992] Loss: 1.928357331520134\n",
      "[학습 993] Loss: 2.027953711470713\n",
      "[학습 994] Loss: 1.9993903328966518\n",
      "[학습 995] Loss: 1.9685124455332692\n",
      "[학습 996] Loss: 1.9096230585520697\n",
      "[학습 997] Loss: 2.075521356314049\n",
      "[학습 998] Loss: 2.186450523017918\n",
      "[학습 999] Loss: 1.9717002787411042\n",
      "[학습 1000] Loss: 1.9634280697110433\n",
      "[학습 1001] Loss: 1.940358220789311\n",
      "[학습 1002] Loss: 2.1004114246999213\n",
      "[학습 1003] Loss: 2.274903503265283\n",
      "[학습 1004] Loss: 2.166189214126032\n",
      "[학습 1005] Loss: 2.0014937160012547\n",
      "[학습 1006] Loss: 1.9351484025940966\n",
      "[학습 1007] Loss: 2.097060108276396\n",
      "[학습 1008] Loss: 1.8540298245917382\n",
      "[학습 1009] Loss: 1.896172861752558\n",
      "[학습 1010] Loss: 1.9993068892225552\n",
      "[학습 1011] Loss: 1.8212453970796054\n",
      "[학습 1012] Loss: 1.6443459467050516\n",
      "[학습 1013] Loss: 2.0032484553011147\n",
      "[학습 1014] Loss: 1.843518491584461\n",
      "[학습 1015] Loss: 1.9949772443890603\n",
      "[학습 1016] Loss: 1.957001906938644\n",
      "[학습 1017] Loss: 1.882922515865032\n",
      "[학습 1018] Loss: 1.790231067880822\n",
      "[학습 1019] Loss: 1.956365313263975\n",
      "[학습 1020] Loss: 1.852451472539696\n",
      "[학습 1021] Loss: 2.1172917749128923\n",
      "[학습 1022] Loss: 1.9972013988463302\n",
      "[학습 1023] Loss: 1.910559684249518\n",
      "[학습 1024] Loss: 1.9471293032769228\n",
      "[학습 1025] Loss: 1.8226559195977812\n",
      "[학습 1026] Loss: 1.6941502670169644\n",
      "[학습 1027] Loss: 2.032616868115059\n",
      "[학습 1028] Loss: 2.1139273885585523\n",
      "[학습 1029] Loss: 1.9770617944115298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습 1030] Loss: 2.2363692986476598\n",
      "[학습 1031] Loss: 1.6210393586432383\n",
      "[학습 1032] Loss: 2.0516042257773486\n",
      "[학습 1033] Loss: 2.060358640970409\n",
      "[학습 1034] Loss: 1.7808657835460366\n",
      "[학습 1035] Loss: 1.9797796998009547\n",
      "[학습 1036] Loss: 1.6723816939753136\n",
      "[학습 1037] Loss: 1.8644416505882335\n",
      "[학습 1038] Loss: 1.840281765086815\n",
      "[학습 1039] Loss: 2.07605432444554\n",
      "[학습 1040] Loss: 1.6592486186859279\n",
      "[학습 1041] Loss: 1.99322388486961\n",
      "[학습 1042] Loss: 1.9519998418730484\n",
      "[학습 1043] Loss: 1.8691754431100813\n",
      "[학습 1044] Loss: 1.9070091317952562\n",
      "[학습 1045] Loss: 2.1329151559741866\n",
      "[학습 1046] Loss: 2.174986149295053\n",
      "[학습 1047] Loss: 1.674463584323924\n",
      "[학습 1048] Loss: 1.914366113797675\n",
      "[학습 1049] Loss: 2.126995294410279\n",
      "[학습 1050] Loss: 2.0664211962295203\n",
      "[학습 1051] Loss: 1.9152347543267052\n",
      "[학습 1052] Loss: 2.0350283511209244\n",
      "[학습 1053] Loss: 1.9282074748608795\n",
      "[학습 1054] Loss: 1.7984023921463943\n",
      "[학습 1055] Loss: 1.925393157273851\n",
      "[학습 1056] Loss: 2.052108472075072\n",
      "[학습 1057] Loss: 1.9943838339871531\n",
      "[학습 1058] Loss: 1.7871148559411636\n",
      "[학습 1059] Loss: 2.095152233436399\n",
      "[학습 1060] Loss: 1.839205765680265\n",
      "[학습 1061] Loss: 1.7195399632232131\n",
      "[학습 1062] Loss: 1.8824667528505252\n",
      "[학습 1063] Loss: 1.8389965767833578\n",
      "[학습 1064] Loss: 1.8253232934066017\n",
      "[학습 1065] Loss: 2.0189200214126046\n",
      "[학습 1066] Loss: 1.789781001268625\n",
      "[학습 1067] Loss: 1.7030372942095613\n",
      "[학습 1068] Loss: 1.8164017628978746\n",
      "[학습 1069] Loss: 1.8717868757342258\n",
      "[학습 1070] Loss: 1.9567637543419374\n",
      "[학습 1071] Loss: 1.762803138185299\n",
      "[학습 1072] Loss: 2.110500492622268\n",
      "[학습 1073] Loss: 2.064090177360009\n",
      "[학습 1074] Loss: 1.7610981574701723\n",
      "[학습 1075] Loss: 2.0392870787125363\n",
      "[학습 1076] Loss: 2.1550510254621233\n",
      "[학습 1077] Loss: 2.0270041633870575\n",
      "[학습 1078] Loss: 2.000578977751344\n",
      "[학습 1079] Loss: 1.9402850506930287\n",
      "[학습 1080] Loss: 2.0060249849168694\n",
      "[학습 1081] Loss: 1.9516887326588424\n",
      "[학습 1082] Loss: 1.8522077992994115\n",
      "[학습 1083] Loss: 1.9053309611641691\n",
      "[학습 1084] Loss: 1.9537250978701102\n",
      "[학습 1085] Loss: 1.9751688302377925\n",
      "[학습 1086] Loss: 1.8018877870650016\n",
      "[학습 1087] Loss: 2.0445822203127966\n",
      "[학습 1088] Loss: 1.81559330813856\n",
      "[학습 1089] Loss: 1.834082673159378\n",
      "[학습 1090] Loss: 1.9704119793162977\n",
      "[학습 1091] Loss: 1.8626786579508972\n",
      "[학습 1092] Loss: 1.8749526923070294\n",
      "[학습 1093] Loss: 2.029065459943849\n",
      "[학습 1094] Loss: 1.7813735176865158\n",
      "[학습 1095] Loss: 1.752483044113652\n",
      "[학습 1096] Loss: 1.785146235267635\n",
      "[학습 1097] Loss: 2.3303380191558385\n",
      "[학습 1098] Loss: 1.7971318330581973\n",
      "[학습 1099] Loss: 1.8055056823194278\n",
      "[학습 1100] Loss: 2.037317346588706\n",
      "[학습 1101] Loss: 1.8095458058840803\n",
      "[학습 1102] Loss: 1.9284104538188547\n",
      "[학습 1103] Loss: 1.8085632735512371\n",
      "[학습 1104] Loss: 1.8309742509438667\n",
      "[학습 1105] Loss: 1.965464842108326\n",
      "[학습 1106] Loss: 2.0531438756011826\n",
      "[학습 1107] Loss: 1.9329198730464299\n",
      "[학습 1108] Loss: 1.8947793015861163\n",
      "[학습 1109] Loss: 1.9745252999871474\n",
      "[학습 1110] Loss: 1.823619035087079\n",
      "[학습 1111] Loss: 1.7036577690159214\n",
      "[학습 1112] Loss: 1.95252664333403\n",
      "[학습 1113] Loss: 1.8789110286471091\n",
      "[학습 1114] Loss: 1.9819280824185148\n",
      "[학습 1115] Loss: 1.9506899158477862\n",
      "[학습 1116] Loss: 1.854857886296838\n",
      "[학습 1117] Loss: 1.7220459171284102\n",
      "[학습 1118] Loss: 1.666778928953297\n",
      "[학습 1119] Loss: 2.006185349499716\n",
      "[학습 1120] Loss: 1.8056956390736527\n",
      "[학습 1121] Loss: 1.7583010246497457\n",
      "[학습 1122] Loss: 1.682269783566046\n",
      "[학습 1123] Loss: 1.7541068498754158\n",
      "[학습 1124] Loss: 1.5534989703611706\n",
      "[학습 1125] Loss: 2.0951972284212053\n",
      "[학습 1126] Loss: 1.949332078228872\n",
      "[학습 1127] Loss: 1.966450487298905\n",
      "[학습 1128] Loss: 1.7976206128417638\n",
      "[학습 1129] Loss: 1.7244600432320283\n",
      "[학습 1130] Loss: 1.6859307371331949\n",
      "[학습 1131] Loss: 1.9379724735835455\n",
      "[학습 1132] Loss: 1.993282356826986\n",
      "[학습 1133] Loss: 1.8521804953663985\n",
      "[학습 1134] Loss: 1.7327421389719513\n",
      "[학습 1135] Loss: 1.896130323998389\n",
      "[학습 1136] Loss: 1.9355510726491791\n",
      "[학습 1137] Loss: 1.9830890640336036\n",
      "[학습 1138] Loss: 1.8820341295967564\n",
      "[학습 1139] Loss: 2.0700268587063926\n",
      "[학습 1140] Loss: 2.04257325115633\n",
      "[학습 1141] Loss: 1.9213809232000745\n",
      "[학습 1142] Loss: 2.153635904333802\n",
      "[학습 1143] Loss: 1.8934697452451743\n",
      "[학습 1144] Loss: 1.4753007998880567\n",
      "[학습 1145] Loss: 1.9127741294696647\n",
      "[학습 1146] Loss: 2.0141049088077954\n",
      "[학습 1147] Loss: 1.9241858946125432\n",
      "[학습 1148] Loss: 1.9169602970073942\n",
      "[학습 1149] Loss: 1.799143207211521\n",
      "[학습 1150] Loss: 1.9373787722913085\n",
      "[학습 1151] Loss: 1.7369408467257492\n",
      "[학습 1152] Loss: 2.023516834927781\n",
      "[학습 1153] Loss: 1.892750952106873\n",
      "[학습 1154] Loss: 1.742579431418564\n",
      "[학습 1155] Loss: 1.8155848849303717\n",
      "[학습 1156] Loss: 2.020332014513819\n",
      "[학습 1157] Loss: 1.9855782579797507\n",
      "[학습 1158] Loss: 1.9583515538167193\n",
      "[학습 1159] Loss: 1.7880220794293007\n",
      "[학습 1160] Loss: 1.7811156967689976\n",
      "[학습 1161] Loss: 1.6719289693647783\n",
      "[학습 1162] Loss: 1.9768150086744618\n",
      "[학습 1163] Loss: 1.8386715823997717\n",
      "[학습 1164] Loss: 1.9234644103754528\n",
      "[학습 1165] Loss: 1.7996757578523883\n",
      "[학습 1166] Loss: 1.9639079228897558\n",
      "[학습 1167] Loss: 1.5762646391818038\n",
      "[학습 1168] Loss: 1.9083823630530785\n",
      "[학습 1169] Loss: 2.14369601194917\n",
      "[학습 1170] Loss: 1.8004616674251717\n",
      "[학습 1171] Loss: 1.8663712716988254\n",
      "[학습 1172] Loss: 1.9795241006431785\n",
      "[학습 1173] Loss: 1.6632192852321814\n",
      "[학습 1174] Loss: 1.957893914305236\n",
      "[학습 1175] Loss: 1.9878794538882045\n",
      "[학습 1176] Loss: 1.9858440031477647\n",
      "[학습 1177] Loss: 1.6972335919481416\n",
      "[학습 1178] Loss: 1.5676561903789024\n",
      "[학습 1179] Loss: 1.793526721835988\n",
      "[학습 1180] Loss: 2.040454664938989\n",
      "[학습 1181] Loss: 1.9033411686458088\n",
      "[학습 1182] Loss: 1.6006436153591261\n",
      "[학습 1183] Loss: 2.02923434957271\n",
      "[학습 1184] Loss: 1.6694966069936528\n",
      "[학습 1185] Loss: 1.695384675341248\n",
      "[학습 1186] Loss: 1.8873675803755772\n",
      "[학습 1187] Loss: 2.0778309437356497\n",
      "[학습 1188] Loss: 1.8972370699796703\n",
      "[학습 1189] Loss: 1.7592172054845627\n",
      "[학습 1190] Loss: 1.8293123321895741\n",
      "[학습 1191] Loss: 1.802722205079955\n",
      "[학습 1192] Loss: 1.6785938367306619\n",
      "[학습 1193] Loss: 1.8316923226892106\n",
      "[학습 1194] Loss: 1.9244603470931627\n",
      "[학습 1195] Loss: 1.9186462930207722\n",
      "[학습 1196] Loss: 1.8465204870847043\n",
      "[학습 1197] Loss: 1.9136383811282942\n",
      "[학습 1198] Loss: 1.4851229610519574\n",
      "[학습 1199] Loss: 1.8920135843874237\n",
      "[학습 1200] Loss: 1.7369025598420487\n",
      "[학습 1201] Loss: 1.958135714611276\n",
      "[학습 1202] Loss: 1.9293880428568855\n",
      "[학습 1203] Loss: 1.8704747736705676\n",
      "[학습 1204] Loss: 1.7033664998617297\n",
      "[학습 1205] Loss: 1.8199209846639604\n",
      "[학습 1206] Loss: 1.6892400470474944\n",
      "[학습 1207] Loss: 1.7913712949279097\n",
      "[학습 1208] Loss: 1.6327071807590354\n",
      "[학습 1209] Loss: 1.8334158209155942\n",
      "[학습 1210] Loss: 1.5268347122657213\n",
      "[학습 1211] Loss: 1.6343092637978367\n",
      "[학습 1212] Loss: 1.7378527607088008\n",
      "[학습 1213] Loss: 1.7296335128821902\n",
      "[학습 1214] Loss: 1.7338518620624415\n",
      "[학습 1215] Loss: 1.8518394331171022\n",
      "[학습 1216] Loss: 1.6611069945502153\n",
      "[학습 1217] Loss: 1.9620961155066943\n",
      "[학습 1218] Loss: 2.0211284395805\n",
      "[학습 1219] Loss: 1.8089209908477815\n",
      "[학습 1220] Loss: 1.8070255047543173\n",
      "[학습 1221] Loss: 1.9591453061153237\n",
      "[학습 1222] Loss: 1.7018864746356615\n",
      "[학습 1223] Loss: 1.6475834773871378\n",
      "[학습 1224] Loss: 1.6571823852429146\n",
      "[학습 1225] Loss: 1.8635601552580243\n",
      "[학습 1226] Loss: 1.854836375560535\n",
      "[학습 1227] Loss: 1.8909558542490845\n",
      "[학습 1228] Loss: 1.9157061410908793\n",
      "[학습 1229] Loss: 1.803823672265671\n",
      "[학습 1230] Loss: 2.0784807654342887\n",
      "[학습 1231] Loss: 1.8594119361134664\n",
      "[학습 1232] Loss: 1.7216726836763478\n",
      "[학습 1233] Loss: 1.803159230008128\n",
      "[학습 1234] Loss: 2.082860317677232\n",
      "[학습 1235] Loss: 2.007040768996062\n",
      "[학습 1236] Loss: 1.661051068952646\n",
      "[학습 1237] Loss: 1.6983632715323496\n",
      "[학습 1238] Loss: 1.567670633073206\n",
      "[학습 1239] Loss: 2.013763342622375\n",
      "[학습 1240] Loss: 1.6970708284904874\n",
      "[학습 1241] Loss: 1.9515735668365004\n",
      "[학습 1242] Loss: 1.611743047544012\n",
      "[학습 1243] Loss: 1.6360327864934547\n",
      "[학습 1244] Loss: 1.6199597849922447\n",
      "[학습 1245] Loss: 1.723178249926857\n",
      "[학습 1246] Loss: 1.6362680714431799\n",
      "[학습 1247] Loss: 1.8971185438599851\n",
      "[학습 1248] Loss: 1.8335711915041621\n",
      "[학습 1249] Loss: 1.9159422383745661\n",
      "[학습 1250] Loss: 1.7428181174237607\n",
      "[학습 1251] Loss: 1.744149586735184\n",
      "[학습 1252] Loss: 1.6450077319807506\n",
      "[학습 1253] Loss: 1.6476254048476584\n",
      "[학습 1254] Loss: 1.9800394240735264\n",
      "[학습 1255] Loss: 1.7019273052023476\n",
      "[학습 1256] Loss: 1.7849671582537991\n",
      "[학습 1257] Loss: 1.9685065971015994\n",
      "[학습 1258] Loss: 1.8214671043538664\n",
      "[학습 1259] Loss: 1.8288456935897568\n",
      "[학습 1260] Loss: 1.6204003805455296\n",
      "[학습 1261] Loss: 1.7177331664533484\n",
      "[학습 1262] Loss: 1.749043084353525\n",
      "[학습 1263] Loss: 1.9049580930374033\n",
      "[학습 1264] Loss: 1.9462433163370674\n",
      "[학습 1265] Loss: 1.8843913143271545\n",
      "[학습 1266] Loss: 1.7753855141146704\n",
      "[학습 1267] Loss: 1.8319332238907804\n",
      "[학습 1268] Loss: 1.6466429649345564\n",
      "[학습 1269] Loss: 1.8352095738596435\n",
      "[학습 1270] Loss: 1.4774629614361878\n",
      "[학습 1271] Loss: 1.8198445430485648\n",
      "[학습 1272] Loss: 1.7187234806699132\n",
      "[학습 1273] Loss: 1.7547633346425016\n",
      "[학습 1274] Loss: 1.9001165349480664\n",
      "[학습 1275] Loss: 1.7555096971894997\n",
      "[학습 1276] Loss: 1.654798570040976\n",
      "[학습 1277] Loss: 1.633266393620541\n",
      "[학습 1278] Loss: 1.5892300232681906\n",
      "[학습 1279] Loss: 1.6650853598325228\n",
      "[학습 1280] Loss: 1.593696595409424\n",
      "[학습 1281] Loss: 1.8825263239230434\n",
      "[학습 1282] Loss: 1.7080901969365498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습 1283] Loss: 1.9640792514997145\n",
      "[학습 1284] Loss: 1.966350780150416\n",
      "[학습 1285] Loss: 1.718898823274326\n",
      "[학습 1286] Loss: 1.6946279027104945\n",
      "[학습 1287] Loss: 1.762448210395605\n",
      "[학습 1288] Loss: 1.5630036912964338\n",
      "[학습 1289] Loss: 1.8286672063952194\n",
      "[학습 1290] Loss: 1.9405574700443577\n",
      "[학습 1291] Loss: 1.902884097048605\n",
      "[학습 1292] Loss: 2.039481080258623\n",
      "[학습 1293] Loss: 1.864973401255325\n",
      "[학습 1294] Loss: 1.929016054433862\n",
      "[학습 1295] Loss: 1.7527358487603215\n",
      "[학습 1296] Loss: 1.8638818476432744\n",
      "[학습 1297] Loss: 1.820546198711417\n",
      "[학습 1298] Loss: 1.7345338922601121\n",
      "[학습 1299] Loss: 1.5658730702583343\n",
      "[학습 1300] Loss: 1.7946382014030728\n",
      "[학습 1301] Loss: 1.6360090553387723\n",
      "[학습 1302] Loss: 1.6221112357341487\n",
      "[학습 1303] Loss: 1.777667892202719\n",
      "[학습 1304] Loss: 1.7704660861294657\n",
      "[학습 1305] Loss: 1.7745543617901118\n",
      "[학습 1306] Loss: 1.6910800390073246\n",
      "[학습 1307] Loss: 1.7875043554122738\n",
      "[학습 1308] Loss: 1.5249984661328981\n",
      "[학습 1309] Loss: 1.6282979103586854\n",
      "[학습 1310] Loss: 1.7769259703376494\n",
      "[학습 1311] Loss: 1.7884172536610823\n",
      "[학습 1312] Loss: 1.8908780475764255\n",
      "[학습 1313] Loss: 1.943982445348032\n",
      "[학습 1314] Loss: 1.6268198410635109\n",
      "[학습 1315] Loss: 1.7285319419498564\n",
      "[학습 1316] Loss: 1.6395614894674402\n",
      "[학습 1317] Loss: 1.5539113795254003\n",
      "[학습 1318] Loss: 1.6537440933346506\n",
      "[학습 1319] Loss: 1.6116232830020971\n",
      "[학습 1320] Loss: 1.6416053206625372\n",
      "[학습 1321] Loss: 1.9170240573717032\n",
      "[학습 1322] Loss: 1.696400067271656\n",
      "[학습 1323] Loss: 1.7442599290001015\n",
      "[학습 1324] Loss: 1.7568457169067346\n",
      "[학습 1325] Loss: 1.8313713180948417\n",
      "[학습 1326] Loss: 1.756771656225081\n",
      "[학습 1327] Loss: 1.6232875496990178\n",
      "[학습 1328] Loss: 1.8044780569439751\n",
      "[학습 1329] Loss: 1.6260953161024236\n",
      "[학습 1330] Loss: 1.75945993226263\n",
      "[학습 1331] Loss: 2.0831268055739316\n",
      "[학습 1332] Loss: 1.7477347223228263\n",
      "[학습 1333] Loss: 1.5518650480529963\n",
      "[학습 1334] Loss: 1.8216672380934984\n",
      "[학습 1335] Loss: 1.8611790187525872\n",
      "[학습 1336] Loss: 1.886837541531342\n",
      "[학습 1337] Loss: 1.5940364493566799\n",
      "[학습 1338] Loss: 1.7867695800761687\n",
      "[학습 1339] Loss: 1.8888208331348386\n",
      "[학습 1340] Loss: 1.6991918332557083\n",
      "[학습 1341] Loss: 1.7404680891760504\n",
      "[학습 1342] Loss: 1.64575591115003\n",
      "[학습 1343] Loss: 1.918809811738824\n",
      "[학습 1344] Loss: 1.5722418674686205\n",
      "[학습 1345] Loss: 1.8717026437945368\n",
      "[학습 1346] Loss: 1.9796616847329787\n",
      "[학습 1347] Loss: 1.7313295126423245\n",
      "[학습 1348] Loss: 1.6725405785814962\n",
      "[학습 1349] Loss: 1.7412937219206452\n",
      "[학습 1350] Loss: 1.798663764116548\n",
      "[학습 1351] Loss: 1.6573262544082195\n",
      "[학습 1352] Loss: 1.5378439413332918\n",
      "[학습 1353] Loss: 1.6165691159382982\n",
      "[학습 1354] Loss: 1.5929455956452494\n",
      "[학습 1355] Loss: 1.7115460850635515\n",
      "[학습 1356] Loss: 1.720480278870366\n",
      "[학습 1357] Loss: 1.6004511557598697\n",
      "[학습 1358] Loss: 1.8167979141487216\n",
      "[학습 1359] Loss: 1.7897118012503188\n",
      "[학습 1360] Loss: 2.0039908422852126\n",
      "[학습 1361] Loss: 1.9527635601103186\n",
      "[학습 1362] Loss: 1.7216101596177653\n",
      "[학습 1363] Loss: 1.6374494887319724\n",
      "[학습 1364] Loss: 1.7948035134731435\n",
      "[학습 1365] Loss: 2.2044760278487727\n",
      "[학습 1366] Loss: 1.7138876913257144\n",
      "[학습 1367] Loss: 1.6163374669923343\n",
      "[학습 1368] Loss: 1.5127549984906579\n",
      "[학습 1369] Loss: 1.4510099263015994\n",
      "[학습 1370] Loss: 1.875018865324087\n",
      "[학습 1371] Loss: 1.6540053524478002\n",
      "[학습 1372] Loss: 1.924479207637797\n",
      "[학습 1373] Loss: 1.9748113354708272\n",
      "[학습 1374] Loss: 1.7281708187341565\n",
      "[학습 1375] Loss: 1.7914158659247557\n",
      "[학습 1376] Loss: 1.65109901756848\n",
      "[학습 1377] Loss: 1.7417661742735828\n",
      "[학습 1378] Loss: 1.6560031314299808\n",
      "[학습 1379] Loss: 1.8240849841556035\n",
      "[학습 1380] Loss: 1.5950052158789219\n",
      "[학습 1381] Loss: 1.6065673987844769\n",
      "[학습 1382] Loss: 1.5055912214583709\n",
      "[학습 1383] Loss: 1.595355858442524\n",
      "[학습 1384] Loss: 1.4664115570398986\n",
      "[학습 1385] Loss: 1.5313110106215717\n",
      "[학습 1386] Loss: 1.7425787876452399\n",
      "[학습 1387] Loss: 1.8289888889686312\n",
      "[학습 1388] Loss: 1.884523265626315\n",
      "[학습 1389] Loss: 1.4343693592439706\n",
      "[학습 1390] Loss: 1.7225736562318008\n",
      "[학습 1391] Loss: 1.889102694861067\n",
      "[학습 1392] Loss: 1.8664765711130105\n",
      "[학습 1393] Loss: 2.041382519943777\n",
      "[학습 1394] Loss: 1.6950607255748198\n",
      "[학습 1395] Loss: 1.5829242753109016\n",
      "[학습 1396] Loss: 1.4941224518258929\n",
      "[학습 1397] Loss: 1.6117588136776544\n",
      "[학습 1398] Loss: 1.7957250754847331\n",
      "[학습 1399] Loss: 1.7666891806035592\n",
      "[학습 1400] Loss: 1.9488199552824255\n",
      "[학습 1401] Loss: 1.8228101860701134\n",
      "[학습 1402] Loss: 1.8465586276062402\n",
      "[학습 1403] Loss: 1.5320761258817106\n",
      "[학습 1404] Loss: 1.7100096005307308\n",
      "[학습 1405] Loss: 1.7140327747480257\n",
      "[학습 1406] Loss: 1.677509678339099\n",
      "[학습 1407] Loss: 1.843331497464408\n",
      "[학습 1408] Loss: 1.539062214356793\n",
      "[학습 1409] Loss: 1.7171888037086034\n",
      "[학습 1410] Loss: 1.6252325504230776\n",
      "[학습 1411] Loss: 1.611939302051599\n",
      "[학습 1412] Loss: 1.6099183415906566\n",
      "[학습 1413] Loss: 1.7405126812288552\n",
      "[학습 1414] Loss: 1.6592102247643161\n",
      "[학습 1415] Loss: 1.7220621037905324\n",
      "[학습 1416] Loss: 1.676077249728474\n",
      "[학습 1417] Loss: 1.6780358826861272\n",
      "[학습 1418] Loss: 1.5188229488608496\n",
      "[학습 1419] Loss: 1.8705994681036913\n",
      "[학습 1420] Loss: 1.5391793608144777\n",
      "[학습 1421] Loss: 1.9034761384269716\n",
      "[학습 1422] Loss: 1.8078390195715617\n",
      "[학습 1423] Loss: 1.6501629597382554\n",
      "[학습 1424] Loss: 1.8981985020851357\n",
      "[학습 1425] Loss: 1.733784918236384\n",
      "[학습 1426] Loss: 1.546603115203718\n",
      "[학습 1427] Loss: 2.009454185882703\n",
      "[학습 1428] Loss: 1.579790416753841\n",
      "[학습 1429] Loss: 1.54039936263406\n",
      "[학습 1430] Loss: 1.5790207503471916\n",
      "[학습 1431] Loss: 1.4777266209561122\n",
      "[학습 1432] Loss: 1.4741119478202374\n",
      "[학습 1433] Loss: 1.8042881465100078\n",
      "[학습 1434] Loss: 1.7393210062732611\n",
      "[학습 1435] Loss: 1.470750726038995\n",
      "[학습 1436] Loss: 1.668790998577292\n",
      "[학습 1437] Loss: 1.8578584598187973\n",
      "[학습 1438] Loss: 1.50371997748615\n",
      "[학습 1439] Loss: 1.8522282464674027\n",
      "[학습 1440] Loss: 1.7163203449084887\n",
      "[학습 1441] Loss: 1.7437579345420822\n",
      "[학습 1442] Loss: 1.6630487687104027\n",
      "[학습 1443] Loss: 1.8425038988546287\n",
      "[학습 1444] Loss: 1.6760982112293557\n",
      "[학습 1445] Loss: 1.8569000781674265\n",
      "[학습 1446] Loss: 1.938492844382605\n",
      "[학습 1447] Loss: 1.7576545339336138\n",
      "[학습 1448] Loss: 1.670086999069577\n",
      "[학습 1449] Loss: 1.9207069789148725\n",
      "[학습 1450] Loss: 1.8553011919130729\n",
      "[학습 1451] Loss: 1.6838274948921146\n",
      "[학습 1452] Loss: 1.9321668369690184\n",
      "[학습 1453] Loss: 1.607341162652036\n",
      "[학습 1454] Loss: 1.7544410650039797\n",
      "[학습 1455] Loss: 1.689058145610759\n",
      "[학습 1456] Loss: 1.7093061870730173\n",
      "[학습 1457] Loss: 1.7152039746559922\n",
      "[학습 1458] Loss: 1.8681589027373218\n",
      "[학습 1459] Loss: 1.6324763559318458\n",
      "[학습 1460] Loss: 1.621937254959052\n",
      "[학습 1461] Loss: 1.7572654762481266\n",
      "[학습 1462] Loss: 1.7527282913090583\n",
      "[학습 1463] Loss: 1.3923706642138143\n",
      "[학습 1464] Loss: 1.6984091615856927\n",
      "[학습 1465] Loss: 1.5918741060791228\n",
      "[학습 1466] Loss: 1.5372855932538902\n",
      "[학습 1467] Loss: 1.7396219529565615\n",
      "[학습 1468] Loss: 1.6175641771796188\n",
      "[학습 1469] Loss: 1.6157867882479133\n",
      "[학습 1470] Loss: 1.5488235836836595\n",
      "[학습 1471] Loss: 1.5860795641551027\n",
      "[학습 1472] Loss: 1.4614023619939553\n",
      "[학습 1473] Loss: 1.6606224690349944\n",
      "[학습 1474] Loss: 1.9104292231399937\n",
      "[학습 1475] Loss: 1.8201216076375324\n",
      "[학습 1476] Loss: 1.584456838728803\n",
      "[학습 1477] Loss: 1.7356987065151521\n",
      "[학습 1478] Loss: 1.8484769588237713\n",
      "[학습 1479] Loss: 1.7123663264977995\n",
      "[학습 1480] Loss: 1.8973266272827911\n",
      "[학습 1481] Loss: 1.729850698999311\n",
      "[학습 1482] Loss: 1.7919592318961275\n",
      "[학습 1483] Loss: 1.9667111284845153\n",
      "[학습 1484] Loss: 1.7915148309922768\n",
      "[학습 1485] Loss: 1.6500095700924098\n",
      "[학습 1486] Loss: 1.5510451837566503\n",
      "[학습 1487] Loss: 1.7828390340346312\n",
      "[학습 1488] Loss: 1.59083591667309\n",
      "[학습 1489] Loss: 1.710665660252163\n",
      "[학습 1490] Loss: 1.6231442594720589\n",
      "[학습 1491] Loss: 1.5595111568186955\n",
      "[학습 1492] Loss: 1.687190141028771\n",
      "[학습 1493] Loss: 1.550112964287208\n",
      "[학습 1494] Loss: 1.66324683769738\n",
      "[학습 1495] Loss: 1.5800930457385436\n",
      "[학습 1496] Loss: 1.8376984253807103\n",
      "[학습 1497] Loss: 1.4427898210493317\n",
      "[학습 1498] Loss: 1.6379254436257384\n",
      "[학습 1499] Loss: 1.8394062093093635\n",
      "[학습 1500] Loss: 1.759821978219103\n",
      "[학습 1501] Loss: 1.7153660057807962\n",
      "[학습 1502] Loss: 1.8510647004621197\n",
      "[학습 1503] Loss: 1.8589217554281448\n",
      "[학습 1504] Loss: 1.7291244986888104\n",
      "[학습 1505] Loss: 1.7558530717882035\n",
      "[학습 1506] Loss: 1.7343600426118355\n",
      "[학습 1507] Loss: 1.6593116852686238\n",
      "[학습 1508] Loss: 1.3572543278387996\n",
      "[학습 1509] Loss: 1.737158357645889\n",
      "[학습 1510] Loss: 1.6507036893213576\n",
      "[학습 1511] Loss: 1.2734672817908204\n",
      "[학습 1512] Loss: 1.6114165786683394\n",
      "[학습 1513] Loss: 1.64456550557105\n",
      "[학습 1514] Loss: 1.7277833651628844\n",
      "[학습 1515] Loss: 1.4750084266054182\n",
      "[학습 1516] Loss: 1.7514603339114136\n",
      "[학습 1517] Loss: 1.6459089811781218\n",
      "[학습 1518] Loss: 1.6170741071320793\n",
      "[학습 1519] Loss: 1.847867926737823\n",
      "[학습 1520] Loss: 1.7551788747449075\n",
      "[학습 1521] Loss: 1.7030424502009636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습 1522] Loss: 1.7784546985433423\n",
      "[학습 1523] Loss: 1.7276736923353375\n",
      "[학습 1524] Loss: 1.5405863268549138\n",
      "[학습 1525] Loss: 1.8899745265861938\n",
      "[학습 1526] Loss: 1.647657646355742\n",
      "[학습 1527] Loss: 1.5195186680718762\n",
      "[학습 1528] Loss: 1.8139487880965772\n",
      "[학습 1529] Loss: 1.7005892022158742\n",
      "[학습 1530] Loss: 1.4119045679547986\n",
      "[학습 1531] Loss: 1.474380629544481\n",
      "[학습 1532] Loss: 1.508134277041693\n",
      "[학습 1533] Loss: 1.589160356060196\n",
      "[학습 1534] Loss: 1.5452995892977683\n",
      "[학습 1535] Loss: 1.7556895808827315\n",
      "[학습 1536] Loss: 1.4942801828140881\n",
      "[학습 1537] Loss: 1.7105757569298103\n",
      "[학습 1538] Loss: 1.6828127775434154\n",
      "[학습 1539] Loss: 1.3400774686989516\n",
      "[학습 1540] Loss: 1.362469370557668\n",
      "[학습 1541] Loss: 1.7304771777163848\n",
      "[학습 1542] Loss: 1.619180971817942\n",
      "[학습 1543] Loss: 1.7901078882899861\n",
      "[학습 1544] Loss: 1.6393016060410937\n",
      "[학습 1545] Loss: 1.5591578784738906\n",
      "[학습 1546] Loss: 1.4528825743636042\n",
      "[학습 1547] Loss: 1.6159836553039622\n",
      "[학습 1548] Loss: 1.8285591335698899\n",
      "[학습 1549] Loss: 1.7181411036925311\n",
      "[학습 1550] Loss: 1.8780373801680583\n",
      "[학습 1551] Loss: 1.7245690920927357\n",
      "[학습 1552] Loss: 1.5065555355672486\n",
      "[학습 1553] Loss: 1.5048874921746467\n",
      "[학습 1554] Loss: 1.513143441656934\n",
      "[학습 1555] Loss: 1.6995387020867476\n",
      "[학습 1556] Loss: 1.5499466921270488\n",
      "[학습 1557] Loss: 1.5421924024357008\n",
      "[학습 1558] Loss: 1.6813499879897347\n",
      "[학습 1559] Loss: 1.5084500339345492\n",
      "[학습 1560] Loss: 1.7011878554673994\n",
      "[학습 1561] Loss: 1.8244205892981882\n",
      "[학습 1562] Loss: 1.5649780371825506\n",
      "[학습 1563] Loss: 1.5357165328828726\n",
      "[학습 1564] Loss: 1.6831685629342525\n",
      "[학습 1565] Loss: 1.3141877256412746\n",
      "[학습 1566] Loss: 1.5925632983912548\n",
      "[학습 1567] Loss: 1.8127346608711248\n",
      "[학습 1568] Loss: 1.7195092848826727\n",
      "[학습 1569] Loss: 1.7565828072268903\n",
      "[학습 1570] Loss: 1.5841699722555846\n",
      "[학습 1571] Loss: 1.642669214935885\n",
      "[학습 1572] Loss: 1.505087547859607\n",
      "[학습 1573] Loss: 1.639334852745377\n",
      "[학습 1574] Loss: 1.7622765344582922\n",
      "[학습 1575] Loss: 1.4555728124296903\n",
      "[학습 1576] Loss: 1.9671576463731568\n",
      "[학습 1577] Loss: 1.3876955114333338\n",
      "[학습 1578] Loss: 1.4411019962607086\n",
      "[학습 1579] Loss: 1.4729000877762082\n",
      "[학습 1580] Loss: 1.4846543379920514\n",
      "[학습 1581] Loss: 1.74610787596614\n",
      "[학습 1582] Loss: 1.6227119971840824\n",
      "[학습 1583] Loss: 1.591330091500256\n",
      "[학습 1584] Loss: 1.802443825335442\n",
      "[학습 1585] Loss: 1.574741311092793\n",
      "[학습 1586] Loss: 1.8465142871829665\n",
      "[학습 1587] Loss: 1.5267098324429202\n",
      "[학습 1588] Loss: 1.582629363105856\n",
      "[학습 1589] Loss: 1.7988212782206592\n",
      "[학습 1590] Loss: 1.7563414839681473\n",
      "[학습 1591] Loss: 1.5543983381918602\n",
      "[학습 1592] Loss: 1.775998836439478\n",
      "[학습 1593] Loss: 1.4016498507026234\n",
      "[학습 1594] Loss: 1.4832688324996781\n",
      "[학습 1595] Loss: 1.9910190044698535\n",
      "[학습 1596] Loss: 1.7762895719135037\n",
      "[학습 1597] Loss: 1.8043460130799445\n",
      "[학습 1598] Loss: 1.7297303477615509\n",
      "[학습 1599] Loss: 1.5590047677713534\n",
      "[학습 1600] Loss: 1.5869222056123482\n",
      "[학습 1601] Loss: 1.7025483361953448\n",
      "[학습 1602] Loss: 1.4927492415845935\n",
      "[학습 1603] Loss: 1.740787833803795\n",
      "[학습 1604] Loss: 1.7259255822890902\n",
      "[학습 1605] Loss: 1.757874254200149\n",
      "[학습 1606] Loss: 1.620175633690577\n",
      "[학습 1607] Loss: 1.4839881129338972\n",
      "[학습 1608] Loss: 1.640655567993875\n",
      "[학습 1609] Loss: 1.5788458345098246\n",
      "[학습 1610] Loss: 1.5495579679523253\n",
      "[학습 1611] Loss: 1.6232139824864051\n",
      "[학습 1612] Loss: 1.7461083270415017\n",
      "[학습 1613] Loss: 1.623124811495365\n",
      "[학습 1614] Loss: 1.6559219499705773\n",
      "[학습 1615] Loss: 1.65085077144754\n",
      "[학습 1616] Loss: 1.7750383851785796\n",
      "[학습 1617] Loss: 1.5098935383822414\n",
      "[학습 1618] Loss: 1.451361657334172\n",
      "[학습 1619] Loss: 1.3207275942849968\n",
      "[학습 1620] Loss: 1.6026346937349996\n",
      "[학습 1621] Loss: 1.664054289240924\n",
      "[학습 1622] Loss: 1.6796658385123584\n",
      "[학습 1623] Loss: 1.6498680803422747\n",
      "[학습 1624] Loss: 1.6285541684863778\n",
      "[학습 1625] Loss: 1.7093961885035651\n",
      "[학습 1626] Loss: 1.638521305507403\n",
      "[학습 1627] Loss: 1.5515683956641624\n",
      "[학습 1628] Loss: 1.523182051223519\n",
      "[학습 1629] Loss: 1.7854578302633444\n",
      "[학습 1630] Loss: 1.5140419458244332\n",
      "[학습 1631] Loss: 1.7997366668066201\n",
      "[학습 1632] Loss: 1.7120489612912702\n",
      "[학습 1633] Loss: 1.7468169705468708\n",
      "[학습 1634] Loss: 1.563592252951147\n",
      "[학습 1635] Loss: 1.593623994501653\n",
      "[학습 1636] Loss: 1.8198381193841846\n",
      "[학습 1637] Loss: 1.631821678694545\n",
      "[학습 1638] Loss: 1.5456857713951933\n",
      "[학습 1639] Loss: 1.8743948215071304\n",
      "[학습 1640] Loss: 1.673630717273789\n",
      "[학습 1641] Loss: 1.6273517484005608\n",
      "[학습 1642] Loss: 1.4600793838306905\n",
      "[학습 1643] Loss: 1.5876662724537178\n",
      "[학습 1644] Loss: 1.7732233522588334\n",
      "[학습 1645] Loss: 1.4166283981227827\n",
      "[학습 1646] Loss: 1.3926185375109614\n",
      "[학습 1647] Loss: 1.5972529222543497\n",
      "[학습 1648] Loss: 1.7454036481002464\n",
      "[학습 1649] Loss: 1.7799658109724374\n",
      "[학습 1650] Loss: 1.6949472315172287\n",
      "[학습 1651] Loss: 1.6116609200034104\n",
      "[학습 1652] Loss: 1.575244559867001\n",
      "[학습 1653] Loss: 1.564582754091793\n",
      "[학습 1654] Loss: 1.5774777653093812\n",
      "[학습 1655] Loss: 1.6520200162743452\n",
      "[학습 1656] Loss: 1.6601577648239367\n",
      "[학습 1657] Loss: 1.5399830933181498\n",
      "[학습 1658] Loss: 1.66580507470099\n",
      "[학습 1659] Loss: 1.5271580772704771\n",
      "[학습 1660] Loss: 1.4670131740328802\n",
      "[학습 1661] Loss: 1.4577769085071728\n",
      "[학습 1662] Loss: 1.71860920720802\n",
      "[학습 1663] Loss: 1.6876064410893787\n",
      "[학습 1664] Loss: 1.6248228332070687\n",
      "[학습 1665] Loss: 1.5620674399016337\n",
      "[학습 1666] Loss: 1.6365481749595994\n",
      "[학습 1667] Loss: 1.2478171206677304\n",
      "[학습 1668] Loss: 1.7402589416818115\n",
      "[학습 1669] Loss: 1.8354491949147331\n",
      "[학습 1670] Loss: 1.6102818264345513\n",
      "[학습 1671] Loss: 1.485748370740547\n",
      "[학습 1672] Loss: 1.642957548043907\n",
      "[학습 1673] Loss: 1.6500378514840168\n",
      "[학습 1674] Loss: 1.8663145662430616\n",
      "[학습 1675] Loss: 1.5441182564453455\n",
      "[학습 1676] Loss: 1.728334584808533\n",
      "[학습 1677] Loss: 1.5807357680523486\n",
      "[학습 1678] Loss: 1.8113754991999977\n",
      "[학습 1679] Loss: 1.818240181779361\n",
      "[학습 1680] Loss: 1.4988964201651804\n",
      "[학습 1681] Loss: 1.5582496416784941\n",
      "[학습 1682] Loss: 1.5333602089874359\n",
      "[학습 1683] Loss: 1.7120523907462648\n",
      "[학습 1684] Loss: 1.504636716675304\n",
      "[학습 1685] Loss: 1.640147639757277\n",
      "[학습 1686] Loss: 1.7396994359263707\n",
      "[학습 1687] Loss: 1.5493479927318885\n",
      "[학습 1688] Loss: 1.5206357155775756\n",
      "[학습 1689] Loss: 1.600284068135286\n",
      "[학습 1690] Loss: 1.5291678604601777\n",
      "[학습 1691] Loss: 1.7118356788566174\n",
      "[학습 1692] Loss: 1.6858694037692181\n",
      "[학습 1693] Loss: 1.6795189720447063\n",
      "[학습 1694] Loss: 1.764225174675551\n",
      "[학습 1695] Loss: 1.606804846907495\n",
      "[학습 1696] Loss: 1.5885501618592257\n",
      "[학습 1697] Loss: 1.4638109368550725\n",
      "[학습 1698] Loss: 1.5777965846714368\n",
      "[학습 1699] Loss: 1.9469894156609677\n",
      "[학습 1700] Loss: 1.5126370939897293\n",
      "[학습 1701] Loss: 1.4349925683395042\n",
      "[학습 1702] Loss: 1.7936049642730563\n",
      "[학습 1703] Loss: 1.598922686664548\n",
      "[학습 1704] Loss: 1.5745189298794822\n",
      "[학습 1705] Loss: 1.7523352536763084\n",
      "[학습 1706] Loss: 1.6073756849061371\n",
      "[학습 1707] Loss: 1.5989792424790499\n",
      "[학습 1708] Loss: 1.4024650716173113\n",
      "[학습 1709] Loss: 1.7369526450739203\n",
      "[학습 1710] Loss: 1.625797813403155\n",
      "[학습 1711] Loss: 1.6939794131400623\n",
      "[학습 1712] Loss: 1.3846793006626776\n",
      "[학습 1713] Loss: 1.719394318645865\n",
      "[학습 1714] Loss: 1.52965055869301\n",
      "[학습 1715] Loss: 1.3670330458954192\n",
      "[학습 1716] Loss: 1.288634736110165\n",
      "[학습 1717] Loss: 1.813406805956559\n",
      "[학습 1718] Loss: 1.7063797752375052\n",
      "[학습 1719] Loss: 1.836548814687792\n",
      "[학습 1720] Loss: 1.55284408912347\n",
      "[학습 1721] Loss: 1.7464200572878399\n",
      "[학습 1722] Loss: 1.6075503173200048\n",
      "[학습 1723] Loss: 1.7361579299678238\n",
      "[학습 1724] Loss: 1.630941898648552\n",
      "[학습 1725] Loss: 1.6505154615841158\n",
      "[학습 1726] Loss: 1.4897903173166065\n",
      "[학습 1727] Loss: 1.5091156381185709\n",
      "[학습 1728] Loss: 1.3094810920711826\n",
      "[학습 1729] Loss: 1.5108728658761366\n",
      "[학습 1730] Loss: 1.7614908179056368\n",
      "[학습 1731] Loss: 1.7193068353629506\n",
      "[학습 1732] Loss: 1.6211931112198141\n",
      "[학습 1733] Loss: 1.3989483358357437\n",
      "[학습 1734] Loss: 1.6394525151393498\n",
      "[학습 1735] Loss: 1.6333567149244255\n",
      "[학습 1736] Loss: 1.4763743853993865\n",
      "[학습 1737] Loss: 1.6481651916776503\n",
      "[학습 1738] Loss: 1.6372658628599863\n",
      "[학습 1739] Loss: 1.4046488597016389\n",
      "[학습 1740] Loss: 1.5054161913196844\n",
      "[학습 1741] Loss: 1.5033299244084959\n",
      "[학습 1742] Loss: 1.6357638787815234\n",
      "[학습 1743] Loss: 1.8146700745997568\n",
      "[학습 1744] Loss: 1.783859714941198\n",
      "[학습 1745] Loss: 1.790865536697334\n",
      "[학습 1746] Loss: 1.539805061773523\n",
      "[학습 1747] Loss: 1.7947056935044667\n",
      "[학습 1748] Loss: 1.7589263819551189\n",
      "[학습 1749] Loss: 1.5364437839405576\n",
      "[학습 1750] Loss: 1.4183677424588694\n",
      "[학습 1751] Loss: 1.5946740387567908\n",
      "[학습 1752] Loss: 1.5891537884035603\n",
      "[학습 1753] Loss: 1.4575711212116227\n",
      "[학습 1754] Loss: 1.642578033447667\n",
      "[학습 1755] Loss: 1.5538395697725518\n",
      "[학습 1756] Loss: 1.69851595103854\n",
      "[학습 1757] Loss: 1.629445465683716\n",
      "[학습 1758] Loss: 1.4609202767270795\n",
      "[학습 1759] Loss: 1.4490764808529024\n",
      "[학습 1760] Loss: 1.5945416158680918\n",
      "[학습 1761] Loss: 1.5098605834208838\n",
      "[학습 1762] Loss: 1.6197919796200706\n",
      "[학습 1763] Loss: 1.6096437928263208\n",
      "[학습 1764] Loss: 1.6764624272671045\n",
      "[학습 1765] Loss: 1.6846634878838955\n",
      "[학습 1766] Loss: 1.4902531481774828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습 1767] Loss: 1.5421575001888297\n",
      "[학습 1768] Loss: 1.6120869501052055\n",
      "[학습 1769] Loss: 1.5830167926275884\n",
      "[학습 1770] Loss: 1.598886941609925\n",
      "[학습 1771] Loss: 1.3122090195978182\n",
      "[학습 1772] Loss: 1.7367889480139886\n",
      "[학습 1773] Loss: 1.5341818853353748\n",
      "[학습 1774] Loss: 1.4684056037832938\n",
      "[학습 1775] Loss: 1.5439808119677831\n",
      "[학습 1776] Loss: 1.8519773113680333\n",
      "[학습 1777] Loss: 1.572249769501881\n",
      "[학습 1778] Loss: 1.678758367034194\n",
      "[학습 1779] Loss: 1.6961309117657395\n",
      "[학습 1780] Loss: 1.6507633859533257\n",
      "[학습 1781] Loss: 1.5911216031906597\n",
      "[학습 1782] Loss: 1.4036030306778429\n",
      "[학습 1783] Loss: 1.5362411519159658\n",
      "[학습 1784] Loss: 1.4269113374602314\n",
      "[학습 1785] Loss: 1.6643372319215575\n",
      "[학습 1786] Loss: 1.5970498996246096\n",
      "[학습 1787] Loss: 1.422104306080327\n",
      "[학습 1788] Loss: 1.4830722948571917\n",
      "[학습 1789] Loss: 1.377130453945292\n",
      "[학습 1790] Loss: 1.3446518068508917\n",
      "[학습 1791] Loss: 1.4333636997333403\n",
      "[학습 1792] Loss: 1.7846988509968267\n",
      "[학습 1793] Loss: 1.4750691766615733\n",
      "[학습 1794] Loss: 1.7739914874318348\n",
      "[학습 1795] Loss: 1.3974613000146485\n",
      "[학습 1796] Loss: 1.5117883637406049\n",
      "[학습 1797] Loss: 1.628650365519924\n",
      "[학습 1798] Loss: 1.7893792517741176\n",
      "[학습 1799] Loss: 1.6016490834346424\n",
      "[학습 1800] Loss: 1.6295895796204918\n",
      "[학습 1801] Loss: 1.4956449548401185\n",
      "[학습 1802] Loss: 1.6263154742735408\n",
      "[학습 1803] Loss: 1.4947510538503836\n",
      "[학습 1804] Loss: 1.6844885693839584\n",
      "[학습 1805] Loss: 1.571164617984138\n",
      "[학습 1806] Loss: 1.8171630089516813\n",
      "[학습 1807] Loss: 1.6952677572136192\n",
      "[학습 1808] Loss: 1.569595599292101\n",
      "[학습 1809] Loss: 1.5186978111380487\n",
      "[학습 1810] Loss: 1.3239391522308483\n",
      "[학습 1811] Loss: 1.6006116437841122\n",
      "[학습 1812] Loss: 1.4401088056239228\n",
      "[학습 1813] Loss: 1.448519960135063\n",
      "[학습 1814] Loss: 1.803750922113479\n",
      "[학습 1815] Loss: 1.6513173055988926\n",
      "[학습 1816] Loss: 1.4567080335749578\n",
      "[학습 1817] Loss: 1.5070425331770985\n",
      "[학습 1818] Loss: 1.6114254361201767\n",
      "[학습 1819] Loss: 1.7303631845933256\n",
      "[학습 1820] Loss: 1.4820261849860337\n",
      "[학습 1821] Loss: 1.3307438130146467\n",
      "[학습 1822] Loss: 1.8891307533396695\n",
      "[학습 1823] Loss: 1.8020924367964266\n",
      "[학습 1824] Loss: 1.3824009528208796\n",
      "[학습 1825] Loss: 1.871219360129095\n",
      "[학습 1826] Loss: 1.5108343188670654\n",
      "[학습 1827] Loss: 1.429878225760412\n",
      "[학습 1828] Loss: 1.4147007550512274\n",
      "[학습 1829] Loss: 1.5665357727495428\n",
      "[학습 1830] Loss: 1.6937368446818504\n",
      "[학습 1831] Loss: 1.5340862252980094\n",
      "[학습 1832] Loss: 1.6775658209574615\n",
      "[학습 1833] Loss: 1.7091029475672588\n",
      "[학습 1834] Loss: 1.4210189599310876\n",
      "[학습 1835] Loss: 1.3440547197302535\n",
      "[학습 1836] Loss: 1.7377980848221342\n",
      "[학습 1837] Loss: 1.5561356097044818\n",
      "[학습 1838] Loss: 1.451790490225794\n",
      "[학습 1839] Loss: 1.4367086392079913\n",
      "[학습 1840] Loss: 1.598857722584735\n",
      "[학습 1841] Loss: 1.8477306076482862\n",
      "[학습 1842] Loss: 1.4412614272142859\n",
      "[학습 1843] Loss: 1.720212664691097\n",
      "[학습 1844] Loss: 1.6422568848117283\n",
      "[학습 1845] Loss: 1.7051153952730658\n",
      "[학습 1846] Loss: 1.7612676838982435\n",
      "[학습 1847] Loss: 1.5636166373173213\n",
      "[학습 1848] Loss: 1.607122107274442\n",
      "[학습 1849] Loss: 1.596030321262942\n",
      "[학습 1850] Loss: 1.5996990859563267\n",
      "[학습 1851] Loss: 1.5931738040562573\n",
      "[학습 1852] Loss: 1.4199204805720007\n",
      "[학습 1853] Loss: 1.4756607766766978\n",
      "[학습 1854] Loss: 1.4133251130815396\n",
      "[학습 1855] Loss: 1.6296354511849316\n",
      "[학습 1856] Loss: 1.4519984841828597\n",
      "[학습 1857] Loss: 1.5003587809356735\n",
      "[학습 1858] Loss: 1.6397914681387769\n",
      "[학습 1859] Loss: 1.292393614935515\n",
      "[학습 1860] Loss: 1.6837987677881374\n",
      "[학습 1861] Loss: 1.7306527701275842\n",
      "[학습 1862] Loss: 1.3224652834698418\n",
      "[학습 1863] Loss: 1.1698557171846828\n",
      "[학습 1864] Loss: 1.6082543003678251\n",
      "[학습 1865] Loss: 1.5760552071002802\n",
      "[학습 1866] Loss: 1.5534235007868658\n",
      "[학습 1867] Loss: 1.6925673010385918\n",
      "[학습 1868] Loss: 1.6283273228867814\n",
      "[학습 1869] Loss: 1.5632171451706944\n",
      "[학습 1870] Loss: 1.4164983459146065\n",
      "[학습 1871] Loss: 1.3748104354295185\n",
      "[학습 1872] Loss: 1.4566785020856912\n",
      "[학습 1873] Loss: 1.5718954997695869\n",
      "[학습 1874] Loss: 1.4540157698390919\n",
      "[학습 1875] Loss: 1.5159348823923002\n",
      "[학습 1876] Loss: 1.5358126600255781\n",
      "[학습 1877] Loss: 1.558324028347798\n",
      "[학습 1878] Loss: 1.40314851418525\n",
      "[학습 1879] Loss: 1.463957000347913\n",
      "[학습 1880] Loss: 1.4872453962105343\n",
      "[학습 1881] Loss: 1.6026347374016416\n",
      "[학습 1882] Loss: 1.6704112058725769\n",
      "[학습 1883] Loss: 1.4641988877922067\n",
      "[학습 1884] Loss: 1.630209909185453\n",
      "[학습 1885] Loss: 1.385616367048961\n",
      "[학습 1886] Loss: 1.5249719679964175\n",
      "[학습 1887] Loss: 1.5148831267508114\n",
      "[학습 1888] Loss: 1.7302101310203801\n",
      "[학습 1889] Loss: 1.50839456200402\n",
      "[학습 1890] Loss: 1.7125768420121081\n",
      "[학습 1891] Loss: 1.7377244331603288\n",
      "[학습 1892] Loss: 1.3413571484353355\n",
      "[학습 1893] Loss: 1.6000980453272038\n",
      "[학습 1894] Loss: 1.3420595238612547\n",
      "[학습 1895] Loss: 1.594568087962171\n",
      "[학습 1896] Loss: 1.6481500000599687\n",
      "[학습 1897] Loss: 1.7616908375143177\n",
      "[학습 1898] Loss: 1.6161350354993496\n",
      "[학습 1899] Loss: 1.3715229409693108\n",
      "[학습 1900] Loss: 1.583070183708852\n",
      "[학습 1901] Loss: 1.6742771423177327\n",
      "[학습 1902] Loss: 1.5530755439316666\n",
      "[학습 1903] Loss: 1.3601469862302655\n",
      "[학습 1904] Loss: 1.7724878338400476\n",
      "[학습 1905] Loss: 1.8127826813143735\n",
      "[학습 1906] Loss: 1.2224300271792599\n",
      "[학습 1907] Loss: 1.6015359879093471\n",
      "[학습 1908] Loss: 1.594045392505651\n",
      "[학습 1909] Loss: 1.2976962438248727\n",
      "[학습 1910] Loss: 1.567106248670926\n",
      "[학습 1911] Loss: 1.646596225277853\n",
      "[학습 1912] Loss: 1.4811427359134222\n",
      "[학습 1913] Loss: 1.4328745370392186\n",
      "[학습 1914] Loss: 1.5407850764761184\n",
      "[학습 1915] Loss: 1.5615503532922421\n",
      "[학습 1916] Loss: 1.5074578895078736\n",
      "[학습 1917] Loss: 1.7153529445880968\n",
      "[학습 1918] Loss: 1.4719197546589975\n",
      "[학습 1919] Loss: 1.7105990958246275\n",
      "[학습 1920] Loss: 1.4309464213180325\n",
      "[학습 1921] Loss: 1.5140798808715363\n",
      "[학습 1922] Loss: 1.4081911567267569\n",
      "[학습 1923] Loss: 1.1218248199743053\n",
      "[학습 1924] Loss: 1.4363192510163776\n",
      "[학습 1925] Loss: 1.4375595035090827\n",
      "[학습 1926] Loss: 1.4707189243599663\n",
      "[학습 1927] Loss: 1.3928964456957125\n",
      "[학습 1928] Loss: 1.5920865658768037\n",
      "[학습 1929] Loss: 1.4988714110653059\n",
      "[학습 1930] Loss: 1.305001490776212\n",
      "[학습 1931] Loss: 1.4014225144436672\n",
      "[학습 1932] Loss: 1.3754509324049047\n",
      "[학습 1933] Loss: 1.5189176437748313\n",
      "[학습 1934] Loss: 1.399126927598799\n",
      "[학습 1935] Loss: 1.2989821375409076\n",
      "[학습 1936] Loss: 1.6455345210352703\n",
      "[학습 1937] Loss: 1.326699369150944\n",
      "[학습 1938] Loss: 1.5520260279228484\n",
      "[학습 1939] Loss: 1.702885403152912\n",
      "[학습 1940] Loss: 1.461047259415505\n",
      "[학습 1941] Loss: 1.6500441086603232\n",
      "[학습 1942] Loss: 1.529341949960343\n",
      "[학습 1943] Loss: 1.321928994647489\n",
      "[학습 1944] Loss: 1.6042825042759956\n",
      "[학습 1945] Loss: 1.335566864538277\n",
      "[학습 1946] Loss: 1.4213611759724376\n",
      "[학습 1947] Loss: 1.3113494679242552\n",
      "[학습 1948] Loss: 1.444582343911161\n",
      "[학습 1949] Loss: 1.557001546994208\n",
      "[학습 1950] Loss: 1.290398011635623\n",
      "[학습 1951] Loss: 1.509532575136305\n",
      "[학습 1952] Loss: 1.6666840909286558\n",
      "[학습 1953] Loss: 1.5105666894091172\n",
      "[학습 1954] Loss: 1.468970951297295\n",
      "[학습 1955] Loss: 1.328240145266669\n",
      "[학습 1956] Loss: 1.6099337743881852\n",
      "[학습 1957] Loss: 1.3449694202698674\n",
      "[학습 1958] Loss: 1.6216164760303193\n",
      "[학습 1959] Loss: 1.440214719856382\n",
      "[학습 1960] Loss: 1.4773909869314776\n",
      "[학습 1961] Loss: 1.652442100198946\n",
      "[학습 1962] Loss: 1.7252317469915521\n",
      "[학습 1963] Loss: 1.5670495474280006\n",
      "[학습 1964] Loss: 1.6959600143787643\n",
      "[학습 1965] Loss: 1.3475816639732607\n",
      "[학습 1966] Loss: 1.4348724367405914\n",
      "[학습 1967] Loss: 1.6774197258566719\n",
      "[학습 1968] Loss: 1.2578379295176771\n",
      "[학습 1969] Loss: 1.530631229295634\n",
      "[학습 1970] Loss: 1.7695591077473054\n",
      "[학습 1971] Loss: 1.498411908541029\n",
      "[학습 1972] Loss: 1.5716858139741\n",
      "[학습 1973] Loss: 1.7376323367110842\n",
      "[학습 1974] Loss: 1.385307589179812\n",
      "[학습 1975] Loss: 1.460232582393943\n",
      "[학습 1976] Loss: 1.5382567557399964\n",
      "[학습 1977] Loss: 1.4361721658851296\n",
      "[학습 1978] Loss: 1.7433520209738713\n",
      "[학습 1979] Loss: 1.5111626632859703\n",
      "[학습 1980] Loss: 1.29830101508083\n",
      "[학습 1981] Loss: 1.454301436547853\n",
      "[학습 1982] Loss: 1.5608536527949315\n",
      "[학습 1983] Loss: 1.6721558114410948\n",
      "[학습 1984] Loss: 1.5060751235832626\n",
      "[학습 1985] Loss: 1.588666678476219\n",
      "[학습 1986] Loss: 1.572465380351589\n",
      "[학습 1987] Loss: 1.4850945045695487\n",
      "[학습 1988] Loss: 1.625180485877979\n",
      "[학습 1989] Loss: 1.6169754768033442\n",
      "[학습 1990] Loss: 1.4936760351198064\n",
      "[학습 1991] Loss: 1.4070587978504636\n",
      "[학습 1992] Loss: 1.694181790006128\n",
      "[학습 1993] Loss: 1.469862928860338\n",
      "[학습 1994] Loss: 1.446940427730902\n",
      "[학습 1995] Loss: 1.2371278013894973\n",
      "[학습 1996] Loss: 1.3840516022375624\n",
      "[학습 1997] Loss: 1.4467811648340858\n",
      "[학습 1998] Loss: 1.6752760909175919\n",
      "[학습 1999] Loss: 1.4309492460507456\n",
      "[학습 2000] Loss: 1.588560130038776\n",
      "[학습 2001] Loss: 1.6098377962242834\n",
      "[학습 2002] Loss: 1.3276263545669684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습 2003] Loss: 1.4479661109001882\n",
      "[학습 2004] Loss: 1.5108378514257652\n",
      "[학습 2005] Loss: 1.3026945145585456\n",
      "[학습 2006] Loss: 1.2375329500469316\n",
      "[학습 2007] Loss: 1.5271641284698894\n",
      "[학습 2008] Loss: 1.6715531942136197\n",
      "[학습 2009] Loss: 1.62984654754417\n",
      "[학습 2010] Loss: 1.2759898333350639\n",
      "[학습 2011] Loss: 1.2912032085282066\n",
      "[학습 2012] Loss: 1.467293581901904\n",
      "[학습 2013] Loss: 1.2751430813472018\n",
      "[학습 2014] Loss: 1.614960388950296\n",
      "[학습 2015] Loss: 1.4827791631498672\n",
      "[학습 2016] Loss: 1.515048344218392\n",
      "[학습 2017] Loss: 1.44550607652738\n",
      "[학습 2018] Loss: 1.4907818022567556\n",
      "[학습 2019] Loss: 1.4110729076270192\n",
      "[학습 2020] Loss: 1.5244760657090097\n",
      "[학습 2021] Loss: 1.285463214323389\n",
      "[학습 2022] Loss: 1.4035855391073793\n",
      "[학습 2023] Loss: 1.6348814270905294\n",
      "[학습 2024] Loss: 1.7215787212095994\n",
      "[학습 2025] Loss: 1.5791458093114716\n",
      "[학습 2026] Loss: 1.5181580628819162\n",
      "[학습 2027] Loss: 1.3418586428027488\n",
      "[학습 2028] Loss: 1.4638443749564096\n",
      "[학습 2029] Loss: 1.316408499898518\n",
      "[학습 2030] Loss: 1.4365504509955076\n",
      "[학습 2031] Loss: 1.4373340233458671\n",
      "[학습 2032] Loss: 1.7460811191948031\n",
      "[학습 2033] Loss: 1.4372967338088307\n",
      "[학습 2034] Loss: 1.3642510674074566\n",
      "[학습 2035] Loss: 1.7808620897538143\n",
      "[학습 2036] Loss: 1.5967835025249912\n",
      "[학습 2037] Loss: 1.6995791596715162\n",
      "[학습 2038] Loss: 1.362685796370828\n",
      "[학습 2039] Loss: 1.5028322436621844\n",
      "[학습 2040] Loss: 1.529364259568194\n",
      "[학습 2041] Loss: 1.3642484883311636\n",
      "[학습 2042] Loss: 1.6039021489659218\n",
      "[학습 2043] Loss: 1.6263656483310347\n",
      "[학습 2044] Loss: 1.726896529920806\n",
      "[학습 2045] Loss: 1.4912673679961546\n",
      "[학습 2046] Loss: 1.3379707959676863\n",
      "[학습 2047] Loss: 1.5884736750112858\n",
      "[학습 2048] Loss: 1.325862203704644\n",
      "[학습 2049] Loss: 1.4740082397461702\n",
      "[학습 2050] Loss: 1.536295299494775\n",
      "[학습 2051] Loss: 1.5055517913651641\n",
      "[학습 2052] Loss: 1.309120680084771\n",
      "[학습 2053] Loss: 1.5238505504660491\n",
      "[학습 2054] Loss: 1.424905250887729\n",
      "[학습 2055] Loss: 1.4940985663224504\n",
      "[학습 2056] Loss: 1.451141449526367\n",
      "[학습 2057] Loss: 1.1564891662550023\n",
      "[학습 2058] Loss: 1.3944924182443696\n",
      "[학습 2059] Loss: 1.0914748897018303\n",
      "[학습 2060] Loss: 1.4981475566432707\n",
      "[학습 2061] Loss: 1.557396490475348\n",
      "[학습 2062] Loss: 1.5677236041363927\n",
      "[학습 2063] Loss: 1.2515472974968345\n",
      "[학습 2064] Loss: 1.6248934738243543\n",
      "[학습 2065] Loss: 1.4020822077525708\n",
      "[학습 2066] Loss: 1.3835993827671345\n",
      "[학습 2067] Loss: 1.40292357979247\n",
      "[학습 2068] Loss: 1.6263874644696177\n",
      "[학습 2069] Loss: 1.7376619842410692\n",
      "[학습 2070] Loss: 1.4774136195639\n",
      "[학습 2071] Loss: 1.6578689949307397\n",
      "[학습 2072] Loss: 1.4736499221935122\n",
      "[학습 2073] Loss: 1.46973609145768\n",
      "[학습 2074] Loss: 1.6604771987656761\n",
      "[학습 2075] Loss: 1.5791935818516112\n",
      "[학습 2076] Loss: 1.4307833662899978\n",
      "[학습 2077] Loss: 1.2736189184725908\n",
      "[학습 2078] Loss: 1.3949278232511135\n",
      "[학습 2079] Loss: 1.3352184099353355\n",
      "[학습 2080] Loss: 1.5216244816325848\n",
      "[학습 2081] Loss: 1.3004866790249903\n",
      "[학습 2082] Loss: 1.4316486389239407\n",
      "[학습 2083] Loss: 1.5468755116209099\n",
      "[학습 2084] Loss: 1.172167189698342\n",
      "[학습 2085] Loss: 1.4680517725520965\n",
      "[학습 2086] Loss: 1.6502920224841633\n",
      "[학습 2087] Loss: 1.5057230111380688\n",
      "[학습 2088] Loss: 1.7333615628763714\n",
      "[학습 2089] Loss: 1.4619479594681868\n",
      "[학습 2090] Loss: 1.5733349743334366\n",
      "[학습 2091] Loss: 1.5883421763912293\n",
      "[학습 2092] Loss: 1.4965137610840684\n",
      "[학습 2093] Loss: 1.2916465970293332\n",
      "[학습 2094] Loss: 1.4320514836401603\n",
      "[학습 2095] Loss: 1.4460629297546241\n",
      "[학습 2096] Loss: 1.3857348986726674\n",
      "[학습 2097] Loss: 1.4329874133081892\n",
      "[학습 2098] Loss: 1.4580152983850063\n",
      "[학습 2099] Loss: 1.5641353566344542\n",
      "[학습 2100] Loss: 1.5787488831501384\n",
      "[학습 2101] Loss: 1.6555505451132535\n",
      "[학습 2102] Loss: 1.4505372816769069\n",
      "[학습 2103] Loss: 1.4326298975792355\n",
      "[학습 2104] Loss: 1.5665701266113796\n",
      "[학습 2105] Loss: 1.419006115948668\n",
      "[학습 2106] Loss: 1.5329216741975842\n",
      "[학습 2107] Loss: 1.6268722664241908\n",
      "[학습 2108] Loss: 1.3570050934799363\n",
      "[학습 2109] Loss: 1.4318302244176755\n",
      "[학습 2110] Loss: 1.2806586173432168\n",
      "[학습 2111] Loss: 1.3273343582806532\n",
      "[학습 2112] Loss: 1.286103730691292\n",
      "[학습 2113] Loss: 1.4154393315005194\n",
      "[학습 2114] Loss: 1.5392431135014146\n",
      "[학습 2115] Loss: 1.3447961102303632\n",
      "[학습 2116] Loss: 1.32371371821461\n",
      "[학습 2117] Loss: 1.2079805920764835\n",
      "[학습 2118] Loss: 1.1409481498084926\n",
      "[학습 2119] Loss: 1.489618454621455\n",
      "[학습 2120] Loss: 1.8040420909325545\n",
      "[학습 2121] Loss: 1.598408003031323\n",
      "[학습 2122] Loss: 1.5879186744295288\n",
      "[학습 2123] Loss: 1.4901495364304043\n",
      "[학습 2124] Loss: 1.4003974941434922\n",
      "[학습 2125] Loss: 1.5375748011744577\n",
      "[학습 2126] Loss: 1.573946507590639\n",
      "[학습 2127] Loss: 1.46730046185584\n",
      "[학습 2128] Loss: 1.2990341572924466\n",
      "[학습 2129] Loss: 1.3477795501493088\n",
      "[학습 2130] Loss: 1.4318815799423696\n",
      "[학습 2131] Loss: 1.6706796705364477\n",
      "[학습 2132] Loss: 1.4997954066214385\n",
      "[학습 2133] Loss: 1.372081663149361\n",
      "[학습 2134] Loss: 1.3920368903389153\n",
      "[학습 2135] Loss: 1.7954809293344556\n",
      "[학습 2136] Loss: 1.3832319419139936\n",
      "[학습 2137] Loss: 1.4215522030587362\n",
      "[학습 2138] Loss: 1.48483410493365\n",
      "[학습 2139] Loss: 1.453554538056228\n",
      "[학습 2140] Loss: 1.5787383907484474\n",
      "[학습 2141] Loss: 1.3660954397490497\n",
      "[학습 2142] Loss: 1.4168583248108513\n",
      "[학습 2143] Loss: 1.6164772472207807\n",
      "[학습 2144] Loss: 1.2255614350743018\n",
      "[학습 2145] Loss: 1.4017538874505555\n",
      "[학습 2146] Loss: 1.5048248395362283\n",
      "[학습 2147] Loss: 1.6207296396184625\n",
      "[학습 2148] Loss: 1.464533950194434\n",
      "[학습 2149] Loss: 1.37663529873713\n",
      "[학습 2150] Loss: 1.425453229946392\n",
      "[학습 2151] Loss: 1.4193565189961561\n",
      "[학습 2152] Loss: 1.1872046043139275\n",
      "[학습 2153] Loss: 1.6554586393285777\n",
      "[학습 2154] Loss: 1.454336394554452\n",
      "[학습 2155] Loss: 1.3661146256759449\n",
      "[학습 2156] Loss: 1.4460636195296939\n",
      "[학습 2157] Loss: 1.257953726182919\n",
      "[학습 2158] Loss: 1.4840247428678868\n",
      "[학습 2159] Loss: 1.6680212290196446\n",
      "[학습 2160] Loss: 1.5681620534101262\n",
      "[학습 2161] Loss: 1.3766977072102153\n",
      "[학습 2162] Loss: 1.3458851941261796\n",
      "[학습 2163] Loss: 1.2877419236546341\n",
      "[학습 2164] Loss: 1.547427237607763\n",
      "[학습 2165] Loss: 1.5026181302891657\n",
      "[학습 2166] Loss: 1.7080044575880762\n",
      "[학습 2167] Loss: 1.6046785873641343\n",
      "[학습 2168] Loss: 1.507509986942259\n",
      "[학습 2169] Loss: 1.249038538731291\n",
      "[학습 2170] Loss: 1.32879479633299\n",
      "[학습 2171] Loss: 1.2591429591166061\n",
      "[학습 2172] Loss: 1.5299529598701258\n",
      "[학습 2173] Loss: 1.3797628570438911\n",
      "[학습 2174] Loss: 1.557293663041948\n",
      "[학습 2175] Loss: 1.481649703022458\n",
      "[학습 2176] Loss: 1.4940337544348055\n",
      "[학습 2177] Loss: 1.6270921509220346\n",
      "[학습 2178] Loss: 1.624192578295877\n",
      "[학습 2179] Loss: 1.3500941683959826\n",
      "[학습 2180] Loss: 1.3304855995687221\n",
      "[학습 2181] Loss: 1.454719426681465\n",
      "[학습 2182] Loss: 1.572166201945123\n",
      "[학습 2183] Loss: 1.3445735869998863\n",
      "[학습 2184] Loss: 1.6356370106093185\n",
      "[학습 2185] Loss: 1.326030829193314\n",
      "[학습 2186] Loss: 1.394526789894946\n",
      "[학습 2187] Loss: 1.2487319452905097\n",
      "[학습 2188] Loss: 1.7541034097588615\n",
      "[학습 2189] Loss: 1.378091735060214\n",
      "[학습 2190] Loss: 1.2223865393641222\n",
      "[학습 2191] Loss: 1.604036225891914\n",
      "[학습 2192] Loss: 1.3796513231732086\n",
      "[학습 2193] Loss: 1.4080235795700224\n",
      "[학습 2194] Loss: 1.2651791306249118\n",
      "[학습 2195] Loss: 1.482639485011237\n",
      "[학습 2196] Loss: 1.2160188751863406\n",
      "[학습 2197] Loss: 1.2329042341069045\n",
      "[학습 2198] Loss: 1.3949926569689666\n",
      "[학습 2199] Loss: 1.4132446510966605\n",
      "[학습 2200] Loss: 1.6481903056955316\n",
      "[학습 2201] Loss: 1.4893425064623402\n",
      "[학습 2202] Loss: 1.7611499456923907\n",
      "[학습 2203] Loss: 1.5202818352721927\n",
      "[학습 2204] Loss: 1.4842466526649418\n",
      "[학습 2205] Loss: 1.3937379337926603\n",
      "[학습 2206] Loss: 1.453106299361297\n",
      "[학습 2207] Loss: 1.4751667044673917\n",
      "[학습 2208] Loss: 1.4612758230794691\n",
      "[학습 2209] Loss: 1.4428376377747292\n",
      "[학습 2210] Loss: 1.6229526493269497\n",
      "[학습 2211] Loss: 1.3562746720890808\n",
      "[학습 2212] Loss: 1.308667621743168\n",
      "[학습 2213] Loss: 1.4402650635251804\n",
      "[학습 2214] Loss: 1.4954023374408179\n",
      "[학습 2215] Loss: 1.462180411190229\n",
      "[학습 2216] Loss: 1.589025285395283\n",
      "[학습 2217] Loss: 1.2468647447085255\n",
      "[학습 2218] Loss: 1.3289827532686922\n",
      "[학습 2219] Loss: 1.273375925935863\n",
      "[학습 2220] Loss: 1.3558783279604625\n",
      "[학습 2221] Loss: 1.4365892252957138\n",
      "[학습 2222] Loss: 1.6849896284856312\n",
      "[학습 2223] Loss: 1.4245199924937983\n",
      "[학습 2224] Loss: 1.4783994293450076\n",
      "[학습 2225] Loss: 1.2428066644115483\n",
      "[학습 2226] Loss: 1.3838267919858327\n",
      "[학습 2227] Loss: 1.441496548643684\n",
      "[학습 2228] Loss: 1.3099012939938168\n",
      "[학습 2229] Loss: 1.388250146373411\n",
      "[학습 2230] Loss: 1.383375899655433\n",
      "[학습 2231] Loss: 1.1658970996712146\n",
      "[학습 2232] Loss: 1.2712041859036862\n",
      "[학습 2233] Loss: 1.4046214497459142\n",
      "[학습 2234] Loss: 1.3887587170771594\n",
      "[학습 2235] Loss: 1.439735791095562\n",
      "[학습 2236] Loss: 1.3866729112962388\n",
      "[학습 2237] Loss: 1.4021494452709837\n",
      "[학습 2238] Loss: 1.3099853744028258\n",
      "[학습 2239] Loss: 1.3047003656285154\n",
      "[학습 2240] Loss: 1.5850207660053652\n",
      "[학습 2241] Loss: 1.2977917615404844\n",
      "[학습 2242] Loss: 1.489393459473625\n",
      "[학습 2243] Loss: 1.5308142272444187\n",
      "[학습 2244] Loss: 1.5863823782101798\n",
      "[학습 2245] Loss: 1.5102971741809939\n",
      "[학습 2246] Loss: 1.5079926082146442\n",
      "[학습 2247] Loss: 1.6340375709974864\n",
      "[학습 2248] Loss: 1.548428473939707\n",
      "[학습 2249] Loss: 1.3857610015126192\n",
      "[학습 2250] Loss: 1.3197901986779481\n",
      "[학습 2251] Loss: 1.1380832858837409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습 2252] Loss: 1.414512425004106\n",
      "[학습 2253] Loss: 1.300402904661608\n",
      "[학습 2254] Loss: 1.5395109424168851\n",
      "[학습 2255] Loss: 1.7688287260905273\n",
      "[학습 2256] Loss: 1.469031320040844\n",
      "[학습 2257] Loss: 1.2933325037255243\n",
      "[학습 2258] Loss: 1.4849824417208344\n",
      "[학습 2259] Loss: 1.4961732204582174\n",
      "[학습 2260] Loss: 1.2989090973864228\n",
      "[학습 2261] Loss: 1.4138672676632087\n",
      "[학습 2262] Loss: 1.307125031739079\n",
      "[학습 2263] Loss: 1.461310733117153\n",
      "[학습 2264] Loss: 1.380684441400772\n",
      "[학습 2265] Loss: 1.1161103342380037\n",
      "[학습 2266] Loss: 1.4307951189893275\n",
      "[학습 2267] Loss: 1.4393276589471733\n",
      "[학습 2268] Loss: 1.364586716861326\n",
      "[학습 2269] Loss: 1.2886861837091714\n",
      "[학습 2270] Loss: 1.2586452169875468\n",
      "[학습 2271] Loss: 1.422674301837285\n",
      "[학습 2272] Loss: 1.5300271535214174\n",
      "[학습 2273] Loss: 1.2881467227387304\n",
      "[학습 2274] Loss: 1.4445586563468458\n",
      "[학습 2275] Loss: 1.2758352573448042\n",
      "[학습 2276] Loss: 1.4470175901459488\n",
      "[학습 2277] Loss: 1.5575916844089626\n",
      "[학습 2278] Loss: 1.46833876773345\n",
      "[학습 2279] Loss: 1.3591429503468322\n",
      "[학습 2280] Loss: 1.6392178089784284\n",
      "[학습 2281] Loss: 1.5717430315547558\n",
      "[학습 2282] Loss: 1.427346179038434\n",
      "[학습 2283] Loss: 1.2405111128678867\n",
      "[학습 2284] Loss: 1.319004359739532\n",
      "[학습 2285] Loss: 1.4007531380166336\n",
      "[학습 2286] Loss: 1.315389376096531\n",
      "[학습 2287] Loss: 1.4748412140633196\n",
      "[학습 2288] Loss: 1.4651320806101495\n",
      "[학습 2289] Loss: 1.4206732736118965\n",
      "[학습 2290] Loss: 1.349011572094551\n",
      "[학습 2291] Loss: 1.1554488276527966\n",
      "[학습 2292] Loss: 1.360845315607725\n",
      "[학습 2293] Loss: 1.3109100237961622\n",
      "[학습 2294] Loss: 1.6992361675579093\n",
      "[학습 2295] Loss: 1.3415673641522268\n",
      "[학습 2296] Loss: 1.4084364913898186\n",
      "[학습 2297] Loss: 1.478259223356705\n",
      "[학습 2298] Loss: 1.75088581478354\n",
      "[학습 2299] Loss: 1.148433515298468\n",
      "[학습 2300] Loss: 1.3595775313530531\n",
      "[학습 2301] Loss: 1.427693334443945\n",
      "[학습 2302] Loss: 1.5195725767066242\n",
      "[학습 2303] Loss: 1.1362327728361266\n",
      "[학습 2304] Loss: 1.4039349458270476\n",
      "[학습 2305] Loss: 1.2711255491912214\n",
      "[학습 2306] Loss: 1.3952015982477952\n",
      "[학습 2307] Loss: 1.302237101727535\n",
      "[학습 2308] Loss: 1.4848403928893947\n",
      "[학습 2309] Loss: 1.4362272838383814\n",
      "[학습 2310] Loss: 1.4886486879004601\n",
      "[학습 2311] Loss: 1.45869340954758\n",
      "[학습 2312] Loss: 1.4911410780141403\n",
      "[학습 2313] Loss: 1.264374100663269\n",
      "[학습 2314] Loss: 1.2890248997537634\n",
      "[학습 2315] Loss: 1.1944004385006723\n",
      "[학습 2316] Loss: 1.8013347719971053\n",
      "[학습 2317] Loss: 1.1327365701508565\n",
      "[학습 2318] Loss: 1.6050858415272342\n",
      "[학습 2319] Loss: 1.3819316757635074\n",
      "[학습 2320] Loss: 1.2873008076954882\n",
      "[학습 2321] Loss: 1.2306838890150014\n",
      "[학습 2322] Loss: 1.4358424855925835\n",
      "[학습 2323] Loss: 1.2849594205159798\n",
      "[학습 2324] Loss: 1.3055237641727422\n",
      "[학습 2325] Loss: 1.3559429204989117\n",
      "[학습 2326] Loss: 1.6471339971118377\n",
      "[학습 2327] Loss: 1.4071033802804516\n",
      "[학습 2328] Loss: 1.210395278707543\n",
      "[학습 2329] Loss: 1.325670546726895\n",
      "[학습 2330] Loss: 1.7771905333959632\n",
      "[학습 2331] Loss: 1.6125554300620388\n",
      "[학습 2332] Loss: 1.2477092001172558\n",
      "[학습 2333] Loss: 1.2316859373304876\n",
      "[학습 2334] Loss: 1.5410626311976512\n",
      "[학습 2335] Loss: 1.3594891385812855\n",
      "[학습 2336] Loss: 1.528656467695906\n",
      "[학습 2337] Loss: 1.5474278657657903\n",
      "[학습 2338] Loss: 1.2425941333912969\n",
      "[학습 2339] Loss: 1.4208156128492249\n",
      "[학습 2340] Loss: 1.4192210521909643\n",
      "[학습 2341] Loss: 1.2466514619139657\n",
      "[학습 2342] Loss: 1.5809925163658702\n",
      "[학습 2343] Loss: 1.4036041290962078\n",
      "[학습 2344] Loss: 1.4421457147222856\n",
      "[학습 2345] Loss: 1.6114906047159498\n",
      "[학습 2346] Loss: 1.6404083576305566\n",
      "[학습 2347] Loss: 1.3309031278733618\n",
      "[학습 2348] Loss: 1.5575965395721618\n",
      "[학습 2349] Loss: 1.2844741551463317\n",
      "[학습 2350] Loss: 1.343734156092659\n",
      "[학습 2351] Loss: 1.2846571675376175\n",
      "[학습 2352] Loss: 1.4618412747659286\n",
      "[학습 2353] Loss: 1.5800988157162477\n",
      "[학습 2354] Loss: 1.2553339937216337\n",
      "[학습 2355] Loss: 1.630727506995531\n",
      "[학습 2356] Loss: 1.2786682686134736\n",
      "[학습 2357] Loss: 1.4024158452151532\n",
      "[학습 2358] Loss: 1.194183615868419\n",
      "[학습 2359] Loss: 1.4111886426499551\n",
      "[학습 2360] Loss: 1.338096394106084\n",
      "[학습 2361] Loss: 1.490606805135124\n",
      "[학습 2362] Loss: 1.503963484711683\n",
      "[학습 2363] Loss: 1.4781268945676271\n",
      "[학습 2364] Loss: 1.3084897304577594\n",
      "[학습 2365] Loss: 1.2830583179551962\n",
      "[학습 2366] Loss: 1.454615707373307\n",
      "[학습 2367] Loss: 1.1655976683546665\n",
      "[학습 2368] Loss: 1.5690661151082026\n",
      "[학습 2369] Loss: 1.3885783758276864\n",
      "[학습 2370] Loss: 1.3756661256054583\n",
      "[학습 2371] Loss: 1.491538709397937\n",
      "[학습 2372] Loss: 1.378505259938002\n",
      "[학습 2373] Loss: 1.5130740927310231\n",
      "[학습 2374] Loss: 1.3711068477639352\n",
      "[학습 2375] Loss: 1.374829190226639\n",
      "[학습 2376] Loss: 1.3618784050852815\n",
      "[학습 2377] Loss: 1.3960087511950992\n",
      "[학습 2378] Loss: 1.5145775677211577\n",
      "[학습 2379] Loss: 1.382985906232018\n",
      "[학습 2380] Loss: 1.2637915051946507\n",
      "[학습 2381] Loss: 1.319261848342901\n",
      "[학습 2382] Loss: 1.4650747446739\n",
      "[학습 2383] Loss: 1.2424503449602358\n",
      "[학습 2384] Loss: 1.1958850712153648\n",
      "[학습 2385] Loss: 1.319098056104683\n",
      "[학습 2386] Loss: 1.397672844963746\n",
      "[학습 2387] Loss: 1.5057429018937856\n",
      "[학습 2388] Loss: 1.5164945908941672\n",
      "[학습 2389] Loss: 1.4895578741024562\n",
      "[학습 2390] Loss: 1.348003183804114\n",
      "[학습 2391] Loss: 1.3507923466530707\n",
      "[학습 2392] Loss: 1.2157624113283818\n",
      "[학습 2393] Loss: 1.3082625576921862\n",
      "[학습 2394] Loss: 1.4877021067273057\n",
      "[학습 2395] Loss: 1.515562462799247\n",
      "[학습 2396] Loss: 1.4047732707914888\n",
      "[학습 2397] Loss: 1.3461993241894297\n",
      "[학습 2398] Loss: 1.4246281191597832\n",
      "[학습 2399] Loss: 1.4177242408123873\n",
      "[학습 2400] Loss: 1.4222853523075265\n",
      "[학습 2401] Loss: 1.533880526600937\n",
      "[학습 2402] Loss: 1.2807870641709294\n",
      "[학습 2403] Loss: 1.4670879875441818\n",
      "[학습 2404] Loss: 1.4845321109441374\n",
      "[학습 2405] Loss: 1.3507296634068633\n",
      "[학습 2406] Loss: 1.4137635854295523\n",
      "[학습 2407] Loss: 1.171169274325816\n",
      "[학습 2408] Loss: 1.2407042678249005\n",
      "[학습 2409] Loss: 1.4318727406986738\n",
      "[학습 2410] Loss: 1.3824496152158758\n",
      "[학습 2411] Loss: 1.4405502433090132\n",
      "[학습 2412] Loss: 1.3892160151112778\n",
      "[학습 2413] Loss: 1.515056779187155\n",
      "[학습 2414] Loss: 1.365175351269703\n",
      "[학습 2415] Loss: 1.3258061994799402\n",
      "[학습 2416] Loss: 1.5007015019084287\n",
      "[학습 2417] Loss: 1.4769937654747165\n",
      "[학습 2418] Loss: 1.527928825344722\n",
      "[학습 2419] Loss: 1.1110165737356794\n",
      "[학습 2420] Loss: 1.4965955279715057\n",
      "[학습 2421] Loss: 1.2688655670742888\n",
      "[학습 2422] Loss: 1.439029870209846\n",
      "[학습 2423] Loss: 1.5304940388928423\n",
      "[학습 2424] Loss: 1.3070962293549728\n",
      "[학습 2425] Loss: 1.2862809352058422\n",
      "[학습 2426] Loss: 1.649198533507\n",
      "[학습 2427] Loss: 1.18545265467897\n",
      "[학습 2428] Loss: 1.3776923814758881\n",
      "[학습 2429] Loss: 1.3301671388758376\n",
      "[학습 2430] Loss: 1.2303589586508101\n",
      "[학습 2431] Loss: 1.4805897713914442\n",
      "[학습 2432] Loss: 1.3535267557195312\n",
      "[학습 2433] Loss: 1.2574138094955902\n",
      "[학습 2434] Loss: 1.6088321938883923\n",
      "[학습 2435] Loss: 1.2410308539185746\n",
      "[학습 2436] Loss: 1.4394357718699666\n",
      "[학습 2437] Loss: 1.4469194215622765\n",
      "[학습 2438] Loss: 1.1941141256940246\n",
      "[학습 2439] Loss: 1.1428955848895344\n",
      "[학습 2440] Loss: 1.4194398935547163\n",
      "[학습 2441] Loss: 1.4732107022427567\n",
      "[학습 2442] Loss: 1.1533540463400196\n",
      "[학습 2443] Loss: 1.3910033246903355\n",
      "[학습 2444] Loss: 1.3741888234969548\n",
      "[학습 2445] Loss: 1.2714447862243003\n",
      "[학습 2446] Loss: 1.4362365071016836\n",
      "[학습 2447] Loss: 1.5039128023812054\n",
      "[학습 2448] Loss: 1.503196209916926\n",
      "[학습 2449] Loss: 1.3348693166353047\n",
      "[학습 2450] Loss: 1.3960412298787719\n",
      "[학습 2451] Loss: 1.4366070466741154\n",
      "[학습 2452] Loss: 1.5554954799343037\n",
      "[학습 2453] Loss: 1.2801820946326443\n",
      "[학습 2454] Loss: 1.4975782017645327\n",
      "[학습 2455] Loss: 1.3647751009194986\n",
      "[학습 2456] Loss: 1.223758539282515\n",
      "[학습 2457] Loss: 1.440601946035316\n",
      "[학습 2458] Loss: 1.5267559704868845\n",
      "[학습 2459] Loss: 1.3626768551798234\n",
      "[학습 2460] Loss: 1.34851779994962\n",
      "[학습 2461] Loss: 1.6186602520660722\n",
      "[학습 2462] Loss: 1.4358529186669358\n",
      "[학습 2463] Loss: 1.093764944240934\n",
      "[학습 2464] Loss: 1.171279059934678\n",
      "[학습 2465] Loss: 1.42208447308958\n",
      "[학습 2466] Loss: 1.2353686529867225\n",
      "[학습 2467] Loss: 1.41108166106905\n",
      "[학습 2468] Loss: 1.5464133863548704\n",
      "[학습 2469] Loss: 1.4833330823059583\n",
      "[학습 2470] Loss: 1.2338114986034376\n",
      "[학습 2471] Loss: 1.2946616834577533\n",
      "[학습 2472] Loss: 1.2958846540954212\n",
      "[학습 2473] Loss: 1.340387830175072\n",
      "[학습 2474] Loss: 1.2316754752644001\n",
      "[학습 2475] Loss: 1.3283902927674027\n",
      "[학습 2476] Loss: 1.2958995647922922\n",
      "[학습 2477] Loss: 1.255597512701189\n",
      "[학습 2478] Loss: 1.682246631149269\n",
      "[학습 2479] Loss: 1.595920877498599\n",
      "[학습 2480] Loss: 1.276516660566554\n",
      "[학습 2481] Loss: 1.2616801113134342\n",
      "[학습 2482] Loss: 1.3029157823402948\n",
      "[학습 2483] Loss: 1.3555920780774893\n",
      "[학습 2484] Loss: 1.1798117288414758\n",
      "[학습 2485] Loss: 1.2035073406224297\n",
      "[학습 2486] Loss: 1.155659401326396\n",
      "[학습 2487] Loss: 1.1594273537271056\n",
      "[학습 2488] Loss: 1.40267973049339\n",
      "[학습 2489] Loss: 1.215738372024546\n",
      "[학습 2490] Loss: 1.2474460556471194\n",
      "[학습 2491] Loss: 1.4223270531414909\n",
      "[학습 2492] Loss: 1.320752763011106\n",
      "[학습 2493] Loss: 1.6416867269938842\n",
      "[학습 2494] Loss: 1.3064101984123986\n",
      "[학습 2495] Loss: 1.0404812988665244\n",
      "[학습 2496] Loss: 1.2805154643061718\n",
      "[학습 2497] Loss: 1.6218654893291171\n",
      "[학습 2498] Loss: 1.2033824623504275\n",
      "[학습 2499] Loss: 1.4721816789098474\n",
      "[학습 2500] Loss: 1.3834718713112313\n",
      "[학습 2501] Loss: 1.4620053882384991\n",
      "[학습 2502] Loss: 1.5817121312795603\n",
      "[학습 2503] Loss: 1.4719507077942398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습 2504] Loss: 1.2755493848837984\n",
      "[학습 2505] Loss: 1.470598088642241\n",
      "[학습 2506] Loss: 1.3007882486674964\n",
      "[학습 2507] Loss: 1.3225471295165983\n",
      "[학습 2508] Loss: 1.3882931522015298\n",
      "[학습 2509] Loss: 1.3481722802056424\n",
      "[학습 2510] Loss: 1.49528872653947\n",
      "[학습 2511] Loss: 1.1773732090833524\n",
      "[학습 2512] Loss: 1.5177032522906426\n",
      "[학습 2513] Loss: 1.568940595737439\n",
      "[학습 2514] Loss: 1.399063807732378\n",
      "[학습 2515] Loss: 1.3597553382936334\n",
      "[학습 2516] Loss: 1.3902356909001492\n",
      "[학습 2517] Loss: 1.4048789100142303\n",
      "[학습 2518] Loss: 1.228562519896294\n",
      "[학습 2519] Loss: 1.3058233592397164\n",
      "[학습 2520] Loss: 1.3208167572996252\n",
      "[학습 2521] Loss: 1.572785460619616\n",
      "[학습 2522] Loss: 1.2634240494196758\n",
      "[학습 2523] Loss: 1.2349989172578737\n",
      "[학습 2524] Loss: 1.3278073623587927\n",
      "[학습 2525] Loss: 1.5177674698518784\n",
      "[학습 2526] Loss: 1.4767238752188894\n",
      "[학습 2527] Loss: 1.4420670445632229\n",
      "[학습 2528] Loss: 1.3371937255146096\n",
      "[학습 2529] Loss: 1.3568532046237716\n",
      "[학습 2530] Loss: 1.4545291233996216\n",
      "[학습 2531] Loss: 1.2242686342308318\n",
      "[학습 2532] Loss: 1.0572917164581197\n",
      "[학습 2533] Loss: 1.4200966965442223\n",
      "[학습 2534] Loss: 1.3476971345011806\n",
      "[학습 2535] Loss: 1.5299497102878103\n",
      "[학습 2536] Loss: 1.5447217735680554\n",
      "[학습 2537] Loss: 1.3888126962025493\n",
      "[학습 2538] Loss: 1.2778283602631109\n",
      "[학습 2539] Loss: 1.312100851657987\n",
      "[학습 2540] Loss: 1.144328875102846\n",
      "[학습 2541] Loss: 1.2016731325295704\n",
      "[학습 2542] Loss: 1.092612074519109\n",
      "[학습 2543] Loss: 1.3091935170180347\n",
      "[학습 2544] Loss: 1.179074436947146\n",
      "[학습 2545] Loss: 1.247205724393597\n",
      "[학습 2546] Loss: 1.3579162405796228\n",
      "[학습 2547] Loss: 1.4447460847561961\n",
      "[학습 2548] Loss: 1.383207291769262\n",
      "[학습 2549] Loss: 1.4549698117843843\n",
      "[학습 2550] Loss: 1.5332380341101075\n",
      "[학습 2551] Loss: 1.2728030307312663\n",
      "[학습 2552] Loss: 1.3495074614028826\n",
      "[학습 2553] Loss: 1.296508960117472\n",
      "[학습 2554] Loss: 1.3866226333560683\n",
      "[학습 2555] Loss: 1.2698786392725168\n",
      "[학습 2556] Loss: 1.3724454745763899\n",
      "[학습 2557] Loss: 1.2964136086976044\n",
      "[학습 2558] Loss: 1.0626178231909253\n",
      "[학습 2559] Loss: 1.3404944347306995\n",
      "[학습 2560] Loss: 1.454998546443062\n",
      "[학습 2561] Loss: 1.5242894380444647\n",
      "[학습 2562] Loss: 1.3981898309076692\n",
      "[학습 2563] Loss: 1.431541779886143\n",
      "[학습 2564] Loss: 1.3798989740854843\n",
      "[학습 2565] Loss: 1.2936684786819816\n",
      "[학습 2566] Loss: 1.2127270381484103\n",
      "[학습 2567] Loss: 1.41713653258663\n",
      "[학습 2568] Loss: 1.3717780599929918\n",
      "[학습 2569] Loss: 1.3120846693051096\n",
      "[학습 2570] Loss: 1.4670936963903938\n",
      "[학습 2571] Loss: 1.5476332790697689\n",
      "[학습 2572] Loss: 1.2327765440619283\n",
      "[학습 2573] Loss: 1.5325484019165632\n",
      "[학습 2574] Loss: 1.4583265261244236\n",
      "[학습 2575] Loss: 1.2348708722056074\n",
      "[학습 2576] Loss: 1.2166528212183887\n",
      "[학습 2577] Loss: 1.3075012336060745\n",
      "[학습 2578] Loss: 1.2042892910196812\n",
      "[학습 2579] Loss: 1.3396374493126717\n",
      "[학습 2580] Loss: 1.3606836351585345\n",
      "[학습 2581] Loss: 1.2757252369166283\n",
      "[학습 2582] Loss: 1.1204392693216896\n",
      "[학습 2583] Loss: 1.2750897249036592\n",
      "[학습 2584] Loss: 1.3851773447629971\n",
      "[학습 2585] Loss: 1.370597694975474\n",
      "[학습 2586] Loss: 1.361057584991354\n",
      "[학습 2587] Loss: 1.490780104000113\n",
      "[학습 2588] Loss: 1.3867204069114463\n",
      "[학습 2589] Loss: 1.3757270608282466\n",
      "[학습 2590] Loss: 1.342034354071157\n",
      "[학습 2591] Loss: 1.2105844313403522\n",
      "[학습 2592] Loss: 1.497442377118422\n",
      "[학습 2593] Loss: 1.3628755598056745\n",
      "[학습 2594] Loss: 1.5278518185053442\n",
      "[학습 2595] Loss: 1.3733983170943846\n",
      "[학습 2596] Loss: 1.2351151862713066\n",
      "[학습 2597] Loss: 1.2508970227675096\n",
      "[학습 2598] Loss: 1.2418803096512987\n",
      "[학습 2599] Loss: 1.3301539764665495\n",
      "[학습 2600] Loss: 1.3193025385870305\n",
      "[학습 2601] Loss: 1.2483802134532518\n",
      "[학습 2602] Loss: 1.5012657505960647\n",
      "[학습 2603] Loss: 1.3881264154689759\n",
      "[학습 2604] Loss: 1.3484049019192001\n",
      "[학습 2605] Loss: 1.2929094639900307\n",
      "[학습 2606] Loss: 1.331424979770036\n",
      "[학습 2607] Loss: 1.175895088607421\n",
      "[학습 2608] Loss: 1.4083513351588792\n",
      "[학습 2609] Loss: 1.3954164659737893\n",
      "[학습 2610] Loss: 1.4388121884563736\n",
      "[학습 2611] Loss: 1.4120158493433126\n",
      "[학습 2612] Loss: 1.1418284188247498\n",
      "[학습 2613] Loss: 1.0929197047349493\n",
      "[학습 2614] Loss: 1.1812814086059171\n",
      "[학습 2615] Loss: 1.591607952399521\n",
      "[학습 2616] Loss: 1.3012818475730066\n",
      "[학습 2617] Loss: 1.4782763146867746\n",
      "[학습 2618] Loss: 1.3110418783567852\n",
      "[학습 2619] Loss: 1.3228660713373233\n",
      "[학습 2620] Loss: 1.3566743124226253\n",
      "[학습 2621] Loss: 1.285808580858224\n",
      "[학습 2622] Loss: 1.0201347533808178\n",
      "[학습 2623] Loss: 1.1999075025955144\n",
      "[학습 2624] Loss: 1.452375849430592\n",
      "[학습 2625] Loss: 1.391334193085279\n",
      "[학습 2626] Loss: 1.5771419126892516\n",
      "[학습 2627] Loss: 1.3621431110137354\n",
      "[학습 2628] Loss: 1.0747871637112414\n",
      "[학습 2629] Loss: 1.364709137073691\n",
      "[학습 2630] Loss: 1.1617333017362168\n",
      "[학습 2631] Loss: 1.226815934390227\n",
      "[학습 2632] Loss: 1.2812797838350922\n",
      "[학습 2633] Loss: 1.4104082394834092\n",
      "[학습 2634] Loss: 1.4323563262998116\n",
      "[학습 2635] Loss: 1.1089134486261427\n",
      "[학습 2636] Loss: 1.589256173954154\n",
      "[학습 2637] Loss: 1.2958909201911297\n",
      "[학습 2638] Loss: 1.203101789209924\n",
      "[학습 2639] Loss: 1.3357297755738495\n",
      "[학습 2640] Loss: 1.1753488496136986\n",
      "[학습 2641] Loss: 1.3053933657744534\n",
      "[학습 2642] Loss: 1.2936163076748521\n",
      "[학습 2643] Loss: 1.268722101623468\n",
      "[학습 2644] Loss: 1.2465735386267318\n",
      "[학습 2645] Loss: 1.3204896263361738\n",
      "[학습 2646] Loss: 1.3811403011797114\n",
      "[학습 2647] Loss: 1.4207000123943339\n",
      "[학습 2648] Loss: 1.5077765936604022\n",
      "[학습 2649] Loss: 1.1672642136064368\n",
      "[학습 2650] Loss: 1.2001263917175524\n",
      "[학습 2651] Loss: 1.4267812651469973\n",
      "[학습 2652] Loss: 1.4076315574924316\n",
      "[학습 2653] Loss: 1.198231377480374\n",
      "[학습 2654] Loss: 1.3865069917115749\n",
      "[학습 2655] Loss: 1.176461105190187\n",
      "[학습 2656] Loss: 1.3098961301616685\n",
      "[학습 2657] Loss: 1.4081644274861767\n",
      "[학습 2658] Loss: 1.2092718550320602\n",
      "[학습 2659] Loss: 1.1720198398468584\n",
      "[학습 2660] Loss: 1.3966988296515916\n",
      "[학습 2661] Loss: 1.1972468417651845\n",
      "[학습 2662] Loss: 1.1536407677704341\n",
      "[학습 2663] Loss: 1.2693301109963933\n",
      "[학습 2664] Loss: 1.2677146100374157\n",
      "[학습 2665] Loss: 1.2393193488650451\n",
      "[학습 2666] Loss: 1.1972078327304927\n",
      "[학습 2667] Loss: 1.3249832682349663\n",
      "[학습 2668] Loss: 1.213928978982233\n",
      "[학습 2669] Loss: 1.4009320775955354\n",
      "[학습 2670] Loss: 1.3948032176685206\n",
      "[학습 2671] Loss: 1.1259762127526565\n",
      "[학습 2672] Loss: 1.363106686602827\n",
      "[학습 2673] Loss: 1.332566055246906\n",
      "[학습 2674] Loss: 1.1715160668232847\n",
      "[학습 2675] Loss: 1.4742294574861932\n",
      "[학습 2676] Loss: 1.3113504666890987\n",
      "[학습 2677] Loss: 1.313227544267335\n",
      "[학습 2678] Loss: 1.6255538502021705\n",
      "[학습 2679] Loss: 1.4579173636478968\n",
      "[학습 2680] Loss: 1.6242167120875552\n",
      "[학습 2681] Loss: 1.2616378689312517\n",
      "[학습 2682] Loss: 1.6020865435411469\n",
      "[학습 2683] Loss: 1.489248017410414\n",
      "[학습 2684] Loss: 1.4199824455831225\n",
      "[학습 2685] Loss: 1.4977502330978547\n",
      "[학습 2686] Loss: 1.4496696802139297\n",
      "[학습 2687] Loss: 1.2528018150836022\n",
      "[학습 2688] Loss: 1.171852438557265\n",
      "[학습 2689] Loss: 1.2061024558216131\n",
      "[학습 2690] Loss: 1.262218102672754\n",
      "[학습 2691] Loss: 1.5153682599359601\n",
      "[학습 2692] Loss: 1.3202835001944433\n",
      "[학습 2693] Loss: 1.2957627353809391\n",
      "[학습 2694] Loss: 1.3120562741910557\n",
      "[학습 2695] Loss: 1.3777673887834272\n",
      "[학습 2696] Loss: 1.3276174859994638\n",
      "[학습 2697] Loss: 1.612965033699818\n",
      "[학습 2698] Loss: 1.4403689664754427\n",
      "[학습 2699] Loss: 1.21302136017696\n",
      "[학습 2700] Loss: 1.0616979890565257\n",
      "[학습 2701] Loss: 1.444887020076439\n",
      "[학습 2702] Loss: 1.2406168063255973\n",
      "[학습 2703] Loss: 1.4913317863361326\n",
      "[학습 2704] Loss: 1.4585818860909987\n",
      "[학습 2705] Loss: 1.2976386449434774\n",
      "[학습 2706] Loss: 1.4224685223150357\n",
      "[학습 2707] Loss: 1.280144034901158\n",
      "[학습 2708] Loss: 1.409346388094034\n",
      "[학습 2709] Loss: 1.5526676614397457\n",
      "[학습 2710] Loss: 1.31898797946973\n",
      "[학습 2711] Loss: 1.2919443649615443\n",
      "[학습 2712] Loss: 1.1029326003897444\n",
      "[학습 2713] Loss: 1.2513040153068329\n",
      "[학습 2714] Loss: 1.2806230689344462\n",
      "[학습 2715] Loss: 1.1350670265087366\n",
      "[학습 2716] Loss: 1.1871212348284415\n",
      "[학습 2717] Loss: 1.2792548833602158\n",
      "[학습 2718] Loss: 1.4465153471862595\n",
      "[학습 2719] Loss: 1.2869523712259967\n",
      "[학습 2720] Loss: 1.2888888144941961\n",
      "[학습 2721] Loss: 1.3325200919876607\n",
      "[학습 2722] Loss: 1.1955407874510413\n",
      "[학습 2723] Loss: 1.4598684106112325\n",
      "[학습 2724] Loss: 1.4016617565418696\n",
      "[학습 2725] Loss: 1.27770984769127\n",
      "[학습 2726] Loss: 1.2273841445309701\n",
      "[학습 2727] Loss: 1.1764621737888474\n",
      "[학습 2728] Loss: 1.2680696908151188\n",
      "[학습 2729] Loss: 0.9208918908100526\n",
      "[학습 2730] Loss: 1.316080444838507\n",
      "[학습 2731] Loss: 1.1377119670655742\n",
      "[학습 2732] Loss: 1.3552835174930735\n",
      "[학습 2733] Loss: 1.1173915057280213\n",
      "[학습 2734] Loss: 1.174587372274252\n",
      "[학습 2735] Loss: 1.1815877063133415\n",
      "[학습 2736] Loss: 1.4229511422527752\n",
      "[학습 2737] Loss: 1.1459181670545862\n",
      "[학습 2738] Loss: 1.4058015090752463\n",
      "[학습 2739] Loss: 1.3025945015354248\n",
      "[학습 2740] Loss: 1.3846242588390902\n",
      "[학습 2741] Loss: 1.188879981008235\n",
      "[학습 2742] Loss: 1.2213941973036608\n",
      "[학습 2743] Loss: 1.2957225310224225\n",
      "[학습 2744] Loss: 1.2056030401729891\n",
      "[학습 2745] Loss: 1.1918378741531381\n",
      "[학습 2746] Loss: 1.2673242728487677\n",
      "[학습 2747] Loss: 1.3136130539122193\n",
      "[학습 2748] Loss: 1.2834517378744534\n",
      "[학습 2749] Loss: 1.3920969621596782\n",
      "[학습 2750] Loss: 1.213923579747715\n",
      "[학습 2751] Loss: 1.2903130568883399\n",
      "[학습 2752] Loss: 1.3265087071589494\n",
      "[학습 2753] Loss: 1.3074264778754092\n",
      "[학습 2754] Loss: 1.4177670793145956\n",
      "[학습 2755] Loss: 1.658154043752865\n",
      "[학습 2756] Loss: 1.249654950972178\n",
      "[학습 2757] Loss: 1.3512612821475807\n",
      "[학습 2758] Loss: 1.2410349891355101\n",
      "[학습 2759] Loss: 1.309861505067052\n",
      "[학습 2760] Loss: 1.3800135195261738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습 2761] Loss: 1.4262214940737081\n",
      "[학습 2762] Loss: 1.3564022732792091\n",
      "[학습 2763] Loss: 1.3545884739042169\n",
      "[학습 2764] Loss: 1.1184757116326016\n",
      "[학습 2765] Loss: 1.2893582325256694\n",
      "[학습 2766] Loss: 1.128361319693748\n",
      "[학습 2767] Loss: 1.2783374806969845\n",
      "[학습 2768] Loss: 1.148017819488515\n",
      "[학습 2769] Loss: 1.3347073464479637\n",
      "[학습 2770] Loss: 1.3730200179989915\n",
      "[학습 2771] Loss: 1.4848722756640138\n",
      "[학습 2772] Loss: 1.3648502619797716\n",
      "[학습 2773] Loss: 1.424267546459601\n",
      "[학습 2774] Loss: 1.0691890456984448\n",
      "[학습 2775] Loss: 1.2542001644893772\n",
      "[학습 2776] Loss: 1.2167216897774298\n",
      "[학습 2777] Loss: 1.2395693997584722\n",
      "[학습 2778] Loss: 1.471861850090913\n",
      "[학습 2779] Loss: 1.2745425170442102\n",
      "[학습 2780] Loss: 1.2606487781294076\n",
      "[학습 2781] Loss: 1.3492556777943077\n",
      "[학습 2782] Loss: 1.2347362331501663\n",
      "[학습 2783] Loss: 1.3511321951901498\n",
      "[학습 2784] Loss: 1.5868476432453442\n",
      "[학습 2785] Loss: 1.3693702177170877\n",
      "[학습 2786] Loss: 1.333862198674199\n",
      "[학습 2787] Loss: 1.4332810415859307\n",
      "[학습 2788] Loss: 1.2887259080606992\n",
      "[학습 2789] Loss: 1.3695900036189814\n",
      "[학습 2790] Loss: 1.1191004088897543\n",
      "[학습 2791] Loss: 1.3522566744824351\n",
      "[학습 2792] Loss: 1.3810937754626302\n",
      "[학습 2793] Loss: 1.1338158111936878\n",
      "[학습 2794] Loss: 1.1768721430300857\n",
      "[학습 2795] Loss: 1.0811097534145975\n",
      "[학습 2796] Loss: 1.3568821870764043\n",
      "[학습 2797] Loss: 1.1591053847781259\n",
      "[학습 2798] Loss: 1.355830932029037\n",
      "[학습 2799] Loss: 1.1929883422968888\n",
      "[학습 2800] Loss: 1.2806700309229766\n",
      "[학습 2801] Loss: 1.5361451634279917\n",
      "[학습 2802] Loss: 1.1603916141687285\n",
      "[학습 2803] Loss: 1.4770408738952552\n",
      "[학습 2804] Loss: 1.295606683310747\n",
      "[학습 2805] Loss: 1.441194323383371\n",
      "[학습 2806] Loss: 1.103689700200251\n",
      "[학습 2807] Loss: 1.2755390580508468\n",
      "[학습 2808] Loss: 1.4917196326464783\n",
      "[학습 2809] Loss: 1.295714601129983\n",
      "[학습 2810] Loss: 1.1237052264549687\n",
      "[학습 2811] Loss: 1.255239173689668\n",
      "[학습 2812] Loss: 1.3340249986786998\n",
      "[학습 2813] Loss: 1.4380245106169587\n",
      "[학습 2814] Loss: 1.3826700934017535\n",
      "[학습 2815] Loss: 1.454995670747448\n",
      "[학습 2816] Loss: 1.2055287309274212\n",
      "[학습 2817] Loss: 1.447331557722071\n",
      "[학습 2818] Loss: 1.2328198671774453\n",
      "[학습 2819] Loss: 1.205053791975848\n",
      "[학습 2820] Loss: 1.380183362784033\n",
      "[학습 2821] Loss: 1.303028165453722\n",
      "[학습 2822] Loss: 1.467328886464729\n",
      "[학습 2823] Loss: 1.2590684069474092\n",
      "[학습 2824] Loss: 1.3689095388515042\n",
      "[학습 2825] Loss: 1.3001730339385216\n",
      "[학습 2826] Loss: 1.3202662429906524\n",
      "[학습 2827] Loss: 1.371269167895632\n",
      "[학습 2828] Loss: 1.4677049315613897\n",
      "[학습 2829] Loss: 1.060077493184027\n",
      "[학습 2830] Loss: 1.0278786002083404\n",
      "[학습 2831] Loss: 1.4684459096527687\n",
      "[학습 2832] Loss: 1.4457573138169737\n",
      "[학습 2833] Loss: 1.298542137777331\n",
      "[학습 2834] Loss: 1.0439054242411319\n",
      "[학습 2835] Loss: 1.365767800430894\n",
      "[학습 2836] Loss: 1.114662380738871\n",
      "[학습 2837] Loss: 1.4008106569233103\n",
      "[학습 2838] Loss: 1.0891197584091419\n",
      "[학습 2839] Loss: 1.2154000781555012\n",
      "[학습 2840] Loss: 1.095301851012583\n",
      "[학습 2841] Loss: 1.4783603574246325\n",
      "[학습 2842] Loss: 1.3216390143095662\n",
      "[학습 2843] Loss: 1.4033507358667947\n",
      "[학습 2844] Loss: 1.1908774447351698\n",
      "[학습 2845] Loss: 1.3373821902009224\n",
      "[학습 2846] Loss: 1.2078048359328977\n",
      "[학습 2847] Loss: 1.4331272840386902\n",
      "[학습 2848] Loss: 1.1240608370303917\n",
      "[학습 2849] Loss: 1.0487974036406356\n",
      "[학습 2850] Loss: 1.0817474417438975\n",
      "[학습 2851] Loss: 1.3156283351413458\n",
      "[학습 2852] Loss: 1.4216989418514043\n",
      "[학습 2853] Loss: 1.2355045826329887\n",
      "[학습 2854] Loss: 1.5283895426580076\n",
      "[학습 2855] Loss: 1.3317264267833135\n",
      "[학습 2856] Loss: 1.4308241172929306\n",
      "[학습 2857] Loss: 1.2123485460154513\n",
      "[학습 2858] Loss: 1.3389982738774813\n",
      "[학습 2859] Loss: 1.2456366135363042\n",
      "[학습 2860] Loss: 1.2017488604455056\n",
      "[학습 2861] Loss: 1.341588232333454\n",
      "[학습 2862] Loss: 1.4284417191026983\n",
      "[학습 2863] Loss: 1.4731038429294336\n",
      "[학습 2864] Loss: 1.3762412624454015\n",
      "[학습 2865] Loss: 1.3169864244064542\n",
      "[학습 2866] Loss: 1.406842183063838\n",
      "[학습 2867] Loss: 1.5941148869877304\n",
      "[학습 2868] Loss: 1.3321714508909275\n",
      "[학습 2869] Loss: 1.0375161276140212\n",
      "[학습 2870] Loss: 1.3018970652094217\n",
      "[학습 2871] Loss: 1.254629626048766\n",
      "[학습 2872] Loss: 1.3158121479352474\n",
      "[학습 2873] Loss: 1.4440873483177585\n",
      "[학습 2874] Loss: 1.282199687826805\n",
      "[학습 2875] Loss: 1.3150252735572645\n",
      "[학습 2876] Loss: 1.5139915346264587\n",
      "[학습 2877] Loss: 1.3088170317331989\n",
      "[학습 2878] Loss: 1.050278371135613\n",
      "[학습 2879] Loss: 1.334733349199576\n",
      "[학습 2880] Loss: 0.9903038851647892\n",
      "[학습 2881] Loss: 1.411835540962851\n",
      "[학습 2882] Loss: 1.13263357437099\n",
      "[학습 2883] Loss: 1.2670832672656693\n",
      "[학습 2884] Loss: 1.2774008469108467\n",
      "[학습 2885] Loss: 1.0062754747307028\n",
      "[학습 2886] Loss: 1.198954107639243\n",
      "[학습 2887] Loss: 1.1151687145775433\n",
      "[학습 2888] Loss: 1.0517171095598101\n",
      "[학습 2889] Loss: 1.1585821349094456\n",
      "[학습 2890] Loss: 1.1646629436935907\n",
      "[학습 2891] Loss: 1.2798251068248296\n",
      "[학습 2892] Loss: 1.1154801242848533\n",
      "[학습 2893] Loss: 1.2767349410129432\n",
      "[학습 2894] Loss: 1.239598705932096\n",
      "[학습 2895] Loss: 1.2345155627398945\n",
      "[학습 2896] Loss: 1.0702694102452055\n",
      "[학습 2897] Loss: 1.3419901667944347\n",
      "[학습 2898] Loss: 1.2338876492674589\n",
      "[학습 2899] Loss: 1.3085938583461982\n",
      "[학습 2900] Loss: 1.2228412056200972\n",
      "[학습 2901] Loss: 1.2921059273843154\n",
      "[학습 2902] Loss: 1.312622887394245\n",
      "[학습 2903] Loss: 1.177508596734974\n",
      "[학습 2904] Loss: 1.4367922104277357\n",
      "[학습 2905] Loss: 1.1100268439691592\n",
      "[학습 2906] Loss: 1.1950395563234772\n",
      "[학습 2907] Loss: 1.2311374318890775\n",
      "[학습 2908] Loss: 1.4086103158974645\n",
      "[학습 2909] Loss: 1.1076726421958316\n",
      "[학습 2910] Loss: 1.285397234058625\n",
      "[학습 2911] Loss: 1.3359432725923728\n",
      "[학습 2912] Loss: 1.0316830672088055\n",
      "[학습 2913] Loss: 1.3459210612869361\n",
      "[학습 2914] Loss: 1.143058121290009\n",
      "[학습 2915] Loss: 1.2853009918468599\n",
      "[학습 2916] Loss: 1.1252024232737154\n",
      "[학습 2917] Loss: 1.0798991465636507\n",
      "[학습 2918] Loss: 1.5816934571524826\n",
      "[학습 2919] Loss: 1.1627222064989124\n",
      "[학습 2920] Loss: 1.2697614777992094\n",
      "[학습 2921] Loss: 1.1715786845863183\n",
      "[학습 2922] Loss: 1.1349477770699636\n",
      "[학습 2923] Loss: 1.1158475199848805\n",
      "[학습 2924] Loss: 1.3754486800295211\n",
      "[학습 2925] Loss: 1.260237398656727\n",
      "[학습 2926] Loss: 1.350233170683329\n",
      "[학습 2927] Loss: 1.2808706904171479\n",
      "[학습 2928] Loss: 1.336456340094568\n",
      "[학습 2929] Loss: 1.1678202344700954\n",
      "[학습 2930] Loss: 1.104520794590845\n",
      "[학습 2931] Loss: 1.380113159424668\n",
      "[학습 2932] Loss: 1.1941760326108346\n",
      "[학습 2933] Loss: 1.358401391131381\n",
      "[학습 2934] Loss: 1.1071048566507422\n",
      "[학습 2935] Loss: 1.3551573572574898\n",
      "[학습 2936] Loss: 1.2910307005624193\n",
      "[학습 2937] Loss: 1.4414905491140846\n",
      "[학습 2938] Loss: 1.206679953153189\n",
      "[학습 2939] Loss: 1.3268179579571444\n",
      "[학습 2940] Loss: 1.1831172204049194\n",
      "[학습 2941] Loss: 1.2579213561497575\n",
      "[학습 2942] Loss: 1.215093197899911\n",
      "[학습 2943] Loss: 1.268672110481996\n",
      "[학습 2944] Loss: 1.1779773639120201\n",
      "[학습 2945] Loss: 1.2201632634326678\n",
      "[학습 2946] Loss: 1.3291408488866814\n",
      "[학습 2947] Loss: 1.004365618947465\n",
      "[학습 2948] Loss: 1.3789835386526648\n",
      "[학습 2949] Loss: 1.1786725264069389\n",
      "[학습 2950] Loss: 1.1266779352525813\n",
      "[학습 2951] Loss: 1.2962902807031316\n",
      "[학습 2952] Loss: 0.9246936729418113\n",
      "[학습 2953] Loss: 1.086464757298145\n",
      "[학습 2954] Loss: 1.170303371421066\n",
      "[학습 2955] Loss: 1.2106753924741822\n",
      "[학습 2956] Loss: 1.3262466899439154\n",
      "[학습 2957] Loss: 1.3136074588210784\n",
      "[학습 2958] Loss: 1.2250136056564371\n",
      "[학습 2959] Loss: 1.2900805916240075\n",
      "[학습 2960] Loss: 1.3100507632362461\n",
      "[학습 2961] Loss: 1.2290051923393737\n",
      "[학습 2962] Loss: 1.3853764439111318\n",
      "[학습 2963] Loss: 1.2941805498186607\n",
      "[학습 2964] Loss: 1.3206176482772098\n",
      "[학습 2965] Loss: 1.3184340021079692\n",
      "[학습 2966] Loss: 1.2200681967151248\n",
      "[학습 2967] Loss: 1.2004272710464616\n",
      "[학습 2968] Loss: 1.351915329152923\n",
      "[학습 2969] Loss: 1.3186057669156483\n",
      "[학습 2970] Loss: 1.1873243180792945\n",
      "[학습 2971] Loss: 1.2978232048267773\n",
      "[학습 2972] Loss: 1.2045682893183427\n",
      "[학습 2973] Loss: 1.2302560661235757\n",
      "[학습 2974] Loss: 1.141704761699188\n",
      "[학습 2975] Loss: 1.1210591351635313\n",
      "[학습 2976] Loss: 1.0759881622970773\n",
      "[학습 2977] Loss: 1.2731162954370334\n",
      "[학습 2978] Loss: 1.2779099974613697\n",
      "[학습 2979] Loss: 1.2833485596304488\n",
      "[학습 2980] Loss: 1.4326695135436065\n",
      "[학습 2981] Loss: 1.3493741280011498\n",
      "[학습 2982] Loss: 1.3271508439359672\n",
      "[학습 2983] Loss: 1.2156703893128942\n",
      "[학습 2984] Loss: 1.3048464875477328\n",
      "[학습 2985] Loss: 1.1667982839000646\n",
      "[학습 2986] Loss: 1.1206841494489013\n",
      "[학습 2987] Loss: 1.2996401937449673\n",
      "[학습 2988] Loss: 1.1713091827822755\n",
      "[학습 2989] Loss: 1.3141456238525555\n",
      "[학습 2990] Loss: 1.3315552192892113\n",
      "[학습 2991] Loss: 1.2612626403270124\n",
      "[학습 2992] Loss: 1.225504931780982\n",
      "[학습 2993] Loss: 1.313769379245535\n",
      "[학습 2994] Loss: 1.1727704213259764\n",
      "[학습 2995] Loss: 1.1943912941840324\n",
      "[학습 2996] Loss: 1.2655538061479135\n",
      "[학습 2997] Loss: 1.151843703942343\n",
      "[학습 2998] Loss: 1.2426022754955528\n",
      "[학습 2999] Loss: 1.1946977768623706\n",
      "[학습 3000] Loss: 1.020980745222815\n",
      "[학습 3001] Loss: 1.2406918595466925\n",
      "[학습 3002] Loss: 1.5336346381408612\n",
      "[학습 3003] Loss: 1.3582876693388568\n",
      "[학습 3004] Loss: 1.2229512849686632\n",
      "[학습 3005] Loss: 1.1731860234180178\n",
      "[학습 3006] Loss: 1.5439438031801973\n",
      "[학습 3007] Loss: 1.4908484296271944\n",
      "[학습 3008] Loss: 1.37862104029712\n",
      "[학습 3009] Loss: 1.3711246571330482\n",
      "[학습 3010] Loss: 1.2513005884182928\n",
      "[학습 3011] Loss: 1.338446766939619\n",
      "[학습 3012] Loss: 1.5214280344517983\n",
      "[학습 3013] Loss: 1.2859989562107428\n",
      "[학습 3014] Loss: 1.305156082785931\n",
      "[학습 3015] Loss: 1.5174224340043463\n",
      "[학습 3016] Loss: 1.2847457812264003\n",
      "[학습 3017] Loss: 1.2319118920969534\n",
      "[학습 3018] Loss: 1.2993935512779728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습 3019] Loss: 1.384060747197533\n",
      "[학습 3020] Loss: 1.1380452013252915\n",
      "[학습 3021] Loss: 1.2642394230843834\n",
      "[학습 3022] Loss: 1.5772065613384059\n",
      "[학습 3023] Loss: 1.4725570639984096\n",
      "[학습 3024] Loss: 1.2257508173983072\n",
      "[학습 3025] Loss: 1.4099958204409422\n",
      "[학습 3026] Loss: 1.2040055083646175\n",
      "[학습 3027] Loss: 1.3125331291755196\n",
      "[학습 3028] Loss: 1.3165074071351048\n",
      "[학습 3029] Loss: 0.9925554540195367\n",
      "[학습 3030] Loss: 1.2288836104836436\n",
      "[학습 3031] Loss: 1.2360233013444832\n",
      "[학습 3032] Loss: 1.262070823395785\n",
      "[학습 3033] Loss: 1.7579770097657184\n",
      "[학습 3034] Loss: 1.1546355628281857\n",
      "[학습 3035] Loss: 1.0397644692425114\n",
      "[학습 3036] Loss: 1.1012701837000407\n",
      "[학습 3037] Loss: 1.2103800223437517\n",
      "[학습 3038] Loss: 1.3665723978101918\n",
      "[학습 3039] Loss: 1.5366635217161229\n",
      "[학습 3040] Loss: 1.3284629027733326\n",
      "[학습 3041] Loss: 1.2454999368899364\n",
      "[학습 3042] Loss: 1.1768287635643375\n",
      "[학습 3043] Loss: 1.2480252955952755\n",
      "[학습 3044] Loss: 1.2225951246244036\n",
      "[학습 3045] Loss: 1.4241984597952717\n",
      "[학습 3046] Loss: 1.1112023119554124\n",
      "[학습 3047] Loss: 1.4629118624416435\n",
      "[학습 3048] Loss: 1.240152817568884\n",
      "[학습 3049] Loss: 1.1636553845211424\n",
      "[학습 3050] Loss: 1.2985873098423721\n",
      "[학습 3051] Loss: 1.1533439085382\n",
      "[학습 3052] Loss: 1.049264921073138\n",
      "[학습 3053] Loss: 1.187774315087499\n",
      "[학습 3054] Loss: 1.1844978999595495\n",
      "[학습 3055] Loss: 1.2338518326291585\n",
      "[학습 3056] Loss: 1.2808126721825897\n",
      "[학습 3057] Loss: 1.2693509247094372\n",
      "[학습 3058] Loss: 1.3394782059535357\n",
      "[학습 3059] Loss: 1.1935180702622343\n",
      "[학습 3060] Loss: 1.1398559650671611\n",
      "[학습 3061] Loss: 1.2009564855314396\n",
      "[학습 3062] Loss: 1.3141917584645983\n",
      "[학습 3063] Loss: 1.5076855951409436\n",
      "[학습 3064] Loss: 1.228967469577215\n",
      "[학습 3065] Loss: 1.3393943188759954\n",
      "[학습 3066] Loss: 1.19646594998667\n",
      "[학습 3067] Loss: 1.2542444206312602\n",
      "[학습 3068] Loss: 1.2575781805176773\n",
      "[학습 3069] Loss: 1.308556205824346\n",
      "[학습 3070] Loss: 1.1885159103095904\n",
      "[학습 3071] Loss: 1.275918911914426\n",
      "[학습 3072] Loss: 1.2818183801031353\n",
      "[학습 3073] Loss: 1.3512699439425404\n",
      "[학습 3074] Loss: 1.2765962910189685\n",
      "[학습 3075] Loss: 1.2630639759665592\n",
      "[학습 3076] Loss: 1.263807679905068\n",
      "[학습 3077] Loss: 1.1915526762305202\n",
      "[학습 3078] Loss: 1.163284978773993\n",
      "[학습 3079] Loss: 1.3266031294250649\n",
      "[학습 3080] Loss: 1.1731229665438072\n",
      "[학습 3081] Loss: 1.4368553586576958\n",
      "[학습 3082] Loss: 1.243038435803314\n",
      "[학습 3083] Loss: 1.2543736581014586\n",
      "[학습 3084] Loss: 1.36812308818844\n",
      "[학습 3085] Loss: 1.2096794610364905\n",
      "[학습 3086] Loss: 1.0086808407944836\n",
      "[학습 3087] Loss: 1.1219473668969036\n",
      "[학습 3088] Loss: 1.3270253136894063\n",
      "[학습 3089] Loss: 1.1454545790896318\n",
      "[학습 3090] Loss: 1.275805884692531\n",
      "[학습 3091] Loss: 1.4095492268462193\n",
      "[학습 3092] Loss: 1.2863302559790264\n",
      "[학습 3093] Loss: 1.2911576731314802\n",
      "[학습 3094] Loss: 0.8608370015807526\n",
      "[학습 3095] Loss: 1.0220001275296178\n",
      "[학습 3096] Loss: 1.137756146233433\n",
      "[학습 3097] Loss: 1.3724026141639099\n",
      "[학습 3098] Loss: 1.1050018209885326\n",
      "[학습 3099] Loss: 1.1487308676991248\n",
      "[학습 3100] Loss: 1.2550509280247502\n",
      "[학습 3101] Loss: 1.2435581989501616\n",
      "[학습 3102] Loss: 1.171572081237032\n",
      "[학습 3103] Loss: 1.3364827881367471\n",
      "[학습 3104] Loss: 1.1214419211039253\n",
      "[학습 3105] Loss: 1.268752659528838\n",
      "[학습 3106] Loss: 1.0936751168501997\n",
      "[학습 3107] Loss: 1.0972538467183828\n",
      "[학습 3108] Loss: 0.9795449790698088\n",
      "[학습 3109] Loss: 1.2353747834888869\n",
      "[학습 3110] Loss: 1.4977484793892342\n",
      "[학습 3111] Loss: 1.1773635707389711\n",
      "[학습 3112] Loss: 1.179960396451069\n",
      "[학습 3113] Loss: 1.0606702813069075\n",
      "[학습 3114] Loss: 1.2406802595595354\n",
      "[학습 3115] Loss: 1.1126453405524854\n",
      "[학습 3116] Loss: 1.2071234702319327\n",
      "[학습 3117] Loss: 1.3450489284402602\n",
      "[학습 3118] Loss: 1.3496763676656558\n",
      "[학습 3119] Loss: 1.3597238938483716\n",
      "[학습 3120] Loss: 1.2737653999666143\n",
      "[학습 3121] Loss: 1.450055093410453\n",
      "[학습 3122] Loss: 1.240802778008039\n",
      "[학습 3123] Loss: 1.2565052115723412\n",
      "[학습 3124] Loss: 0.9814765755750132\n",
      "[학습 3125] Loss: 1.585291807573595\n",
      "[학습 3126] Loss: 1.1786754573845828\n",
      "[학습 3127] Loss: 1.0965091447969852\n",
      "[학습 3128] Loss: 1.2767639330650837\n",
      "[학습 3129] Loss: 1.353467604578068\n",
      "[학습 3130] Loss: 1.2918535237327011\n",
      "[학습 3131] Loss: 1.3647024544794266\n",
      "[학습 3132] Loss: 1.3530073295422325\n",
      "[학습 3133] Loss: 1.50588106735618\n",
      "[학습 3134] Loss: 1.267563122375474\n",
      "[학습 3135] Loss: 1.4949631619298642\n",
      "[학습 3136] Loss: 1.2454025918965803\n",
      "[학습 3137] Loss: 1.215377780522168\n",
      "[학습 3138] Loss: 1.2593924039261273\n",
      "[학습 3139] Loss: 1.1784703662714586\n",
      "[학습 3140] Loss: 1.337489179137699\n",
      "[학습 3141] Loss: 1.1711041314350081\n",
      "[학습 3142] Loss: 1.2677795314502305\n",
      "[학습 3143] Loss: 1.0876463627599906\n",
      "[학습 3144] Loss: 1.3299939197749688\n",
      "[학습 3145] Loss: 1.187596418582697\n",
      "[학습 3146] Loss: 1.2748835762038926\n",
      "[학습 3147] Loss: 1.2448862364234423\n",
      "[학습 3148] Loss: 1.4608088166998288\n",
      "[학습 3149] Loss: 1.395277592255608\n",
      "[학습 3150] Loss: 1.3850200097492513\n",
      "[학습 3151] Loss: 1.2963190896322718\n",
      "[학습 3152] Loss: 1.092713079312054\n",
      "[학습 3153] Loss: 1.2587972684048787\n",
      "[학습 3154] Loss: 1.0278581583556052\n",
      "[학습 3155] Loss: 1.2485580121914575\n",
      "[학습 3156] Loss: 1.3450955878411863\n",
      "[학습 3157] Loss: 1.1848419765509737\n",
      "[학습 3158] Loss: 1.1277698183975458\n",
      "[학습 3159] Loss: 1.1661103145706226\n",
      "[학습 3160] Loss: 1.3473371667755123\n",
      "[학습 3161] Loss: 1.1034505075441459\n",
      "[학습 3162] Loss: 1.2578680503072766\n",
      "[학습 3163] Loss: 1.3033109135544398\n",
      "[학습 3164] Loss: 1.1970188701376605\n",
      "[학습 3165] Loss: 1.1473036833325394\n",
      "[학습 3166] Loss: 1.2420909229601365\n",
      "[학습 3167] Loss: 1.1071845054594367\n",
      "[학습 3168] Loss: 1.3452622592389807\n",
      "[학습 3169] Loss: 1.318000678997433\n",
      "[학습 3170] Loss: 1.3770833615715952\n",
      "[학습 3171] Loss: 1.2418631529026096\n",
      "[학습 3172] Loss: 1.3293588843990432\n",
      "[학습 3173] Loss: 1.1387224647036942\n",
      "[학습 3174] Loss: 1.2639791246342071\n",
      "[학습 3175] Loss: 1.0289164760693972\n",
      "[학습 3176] Loss: 1.309443493657107\n",
      "[학습 3177] Loss: 1.0821023560081238\n",
      "[학습 3178] Loss: 1.0889761501374613\n",
      "[학습 3179] Loss: 1.2231712355589366\n",
      "[학습 3180] Loss: 1.2689132805562808\n",
      "[학습 3181] Loss: 1.3937934772667624\n",
      "[학습 3182] Loss: 1.1635560773639702\n",
      "[학습 3183] Loss: 1.2328145794910712\n",
      "[학습 3184] Loss: 1.2653347603916265\n",
      "[학습 3185] Loss: 1.2678505619302802\n",
      "[학습 3186] Loss: 1.22558603549213\n",
      "[학습 3187] Loss: 1.1693184055381287\n",
      "[학습 3188] Loss: 1.1607684024317724\n",
      "[학습 3189] Loss: 1.2538204117092895\n",
      "[학습 3190] Loss: 1.1390878373220659\n",
      "[학습 3191] Loss: 1.265784805020431\n",
      "[학습 3192] Loss: 1.268130600943186\n",
      "[학습 3193] Loss: 0.9013078140047283\n",
      "[학습 3194] Loss: 1.2119448339614098\n",
      "[학습 3195] Loss: 1.4235048582901704\n",
      "[학습 3196] Loss: 1.2315753920490542\n",
      "[학습 3197] Loss: 1.0886832066593028\n",
      "[학습 3198] Loss: 1.4703847204203244\n",
      "[학습 3199] Loss: 1.2158766549182412\n",
      "[학습 3200] Loss: 1.2775313250588958\n",
      "[학습 3201] Loss: 1.23688767678435\n",
      "[학습 3202] Loss: 1.3203327795624744\n",
      "[학습 3203] Loss: 1.2853565616452465\n",
      "[학습 3204] Loss: 1.3128220689595584\n",
      "[학습 3205] Loss: 1.2402990976751918\n",
      "[학습 3206] Loss: 1.1513290826109346\n",
      "[학습 3207] Loss: 1.3069139987773013\n",
      "[학습 3208] Loss: 1.4876421596969223\n",
      "[학습 3209] Loss: 1.1882480562628308\n",
      "[학습 3210] Loss: 1.0610550268824155\n",
      "[학습 3211] Loss: 1.4795304827567914\n",
      "[학습 3212] Loss: 1.3898062843284713\n",
      "[학습 3213] Loss: 1.1718291469464355\n",
      "[학습 3214] Loss: 1.3041086119546774\n",
      "[학습 3215] Loss: 1.3084814492989008\n",
      "[학습 3216] Loss: 1.2499240085847994\n",
      "[학습 3217] Loss: 1.3342278858634666\n",
      "[학습 3218] Loss: 1.4375535630494942\n",
      "[학습 3219] Loss: 1.2251074874628602\n",
      "[학습 3220] Loss: 1.0607217900655328\n",
      "[학습 3221] Loss: 1.5153841713836573\n",
      "[학습 3222] Loss: 1.0018462640706935\n",
      "[학습 3223] Loss: 1.3574626507822773\n",
      "[학습 3224] Loss: 1.420681035630655\n",
      "[학습 3225] Loss: 1.0634466099107571\n",
      "[학습 3226] Loss: 1.0644754880220284\n",
      "[학습 3227] Loss: 1.3221514513954373\n",
      "[학습 3228] Loss: 1.0315491610809957\n",
      "[학습 3229] Loss: 1.258029030082573\n",
      "[학습 3230] Loss: 1.1384698089062697\n",
      "[학습 3231] Loss: 1.2485157236383015\n",
      "[학습 3232] Loss: 1.2148990575891316\n",
      "[학습 3233] Loss: 1.1261522642607198\n",
      "[학습 3234] Loss: 1.2282136531076953\n",
      "[학습 3235] Loss: 1.122008209683382\n",
      "[학습 3236] Loss: 1.3374431880264603\n",
      "[학습 3237] Loss: 1.2718866347311555\n",
      "[학습 3238] Loss: 1.1481561816247303\n",
      "[학습 3239] Loss: 1.1802638627752735\n",
      "[학습 3240] Loss: 1.4301404703580272\n",
      "[학습 3241] Loss: 1.322259803553518\n",
      "[학습 3242] Loss: 1.1921124103972411\n",
      "[학습 3243] Loss: 1.3110436111866175\n",
      "[학습 3244] Loss: 1.2989398342803369\n",
      "[학습 3245] Loss: 1.2665662804301148\n",
      "[학습 3246] Loss: 1.2199138036169743\n",
      "[학습 3247] Loss: 1.109162036202703\n",
      "[학습 3248] Loss: 1.164917674226011\n",
      "[학습 3249] Loss: 1.2632087189087942\n",
      "[학습 3250] Loss: 1.2242355053264118\n",
      "[학습 3251] Loss: 1.1560040748794902\n",
      "[학습 3252] Loss: 1.1885338960309837\n",
      "[학습 3253] Loss: 1.3633613538175298\n",
      "[학습 3254] Loss: 1.263717502202985\n",
      "[학습 3255] Loss: 1.2369857472100152\n",
      "[학습 3256] Loss: 1.0636232283721723\n",
      "[학습 3257] Loss: 1.114481530565093\n",
      "[학습 3258] Loss: 1.2691672701819023\n",
      "[학습 3259] Loss: 1.1219755942052543\n",
      "[학습 3260] Loss: 1.0471724374926092\n",
      "[학습 3261] Loss: 1.3458601623870248\n",
      "[학습 3262] Loss: 1.3146576573060418\n",
      "[학습 3263] Loss: 1.238481262722214\n",
      "[학습 3264] Loss: 1.1987804098701218\n",
      "[학습 3265] Loss: 1.08900710727624\n",
      "[학습 3266] Loss: 1.3099425959545397\n",
      "[학습 3267] Loss: 1.0107506720302104\n",
      "[학습 3268] Loss: 0.9961861635494177\n",
      "[학습 3269] Loss: 1.269708139424003\n",
      "[학습 3270] Loss: 1.3420688985310516\n",
      "[학습 3271] Loss: 1.336133038970935\n",
      "[학습 3272] Loss: 1.1893168062093233\n",
      "[학습 3273] Loss: 1.231580333183953\n",
      "[학습 3274] Loss: 1.3831134285519722\n",
      "[학습 3275] Loss: 1.2674677453957655\n",
      "[학습 3276] Loss: 1.1253972325705606\n",
      "[학습 3277] Loss: 1.358361958498938\n",
      "[학습 3278] Loss: 1.3139324252780031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습 3279] Loss: 1.1553438474029418\n",
      "[학습 3280] Loss: 1.3925748362876704\n",
      "[학습 3281] Loss: 1.2427826905406072\n",
      "[학습 3282] Loss: 1.2145864781431532\n",
      "[학습 3283] Loss: 1.0642347717853295\n",
      "[학습 3284] Loss: 1.1606142148729757\n",
      "[학습 3285] Loss: 0.9424539082069487\n",
      "[학습 3286] Loss: 1.2175467125385342\n",
      "[학습 3287] Loss: 1.228270095695001\n",
      "[학습 3288] Loss: 1.1688438822161396\n",
      "[학습 3289] Loss: 0.9254851081811453\n",
      "[학습 3290] Loss: 1.305971708148092\n",
      "[학습 3291] Loss: 1.1664212915651497\n",
      "[학습 3292] Loss: 1.2346470985475526\n",
      "[학습 3293] Loss: 1.200827178814856\n",
      "[학습 3294] Loss: 1.3165039872501205\n",
      "[학습 3295] Loss: 1.2625998196421897\n",
      "[학습 3296] Loss: 1.1032695581574146\n",
      "[학습 3297] Loss: 1.3957366004643257\n",
      "[학습 3298] Loss: 1.1040057319785563\n",
      "[학습 3299] Loss: 1.2192435369577734\n",
      "[학습 3300] Loss: 1.160598335759365\n",
      "[학습 3301] Loss: 0.9993326346373679\n",
      "[학습 3302] Loss: 1.3322468026860153\n",
      "[학습 3303] Loss: 1.3162813660920256\n",
      "[학습 3304] Loss: 1.191077758654024\n",
      "[학습 3305] Loss: 0.9841626289512121\n",
      "[학습 3306] Loss: 1.4326668021565154\n",
      "[학습 3307] Loss: 1.2540188485838453\n",
      "[학습 3308] Loss: 1.172025404539053\n",
      "[학습 3309] Loss: 1.202621613653392\n",
      "[학습 3310] Loss: 1.0331118005803734\n",
      "[학습 3311] Loss: 1.234791055035557\n",
      "[학습 3312] Loss: 1.2573016260821483\n",
      "[학습 3313] Loss: 1.0750876739172979\n",
      "[학습 3314] Loss: 1.3213415627285263\n",
      "[학습 3315] Loss: 1.1821329207731663\n",
      "[학습 3316] Loss: 1.1566553606268901\n",
      "[학습 3317] Loss: 0.918652665151281\n",
      "[학습 3318] Loss: 1.3715138598821417\n",
      "[학습 3319] Loss: 1.1834367358195044\n",
      "[학습 3320] Loss: 1.004429226897898\n",
      "[학습 3321] Loss: 1.329643073616091\n",
      "[학습 3322] Loss: 1.3071298310089154\n",
      "[학습 3323] Loss: 1.038072404776561\n",
      "[학습 3324] Loss: 1.0829910438792816\n",
      "[학습 3325] Loss: 1.2736449026946226\n",
      "[학습 3326] Loss: 1.2834481101527444\n",
      "[학습 3327] Loss: 1.369657504807563\n",
      "[학습 3328] Loss: 1.4142079010768043\n",
      "[학습 3329] Loss: 1.2790841414322003\n",
      "[학습 3330] Loss: 1.156202898343737\n",
      "[학습 3331] Loss: 1.1790190718873743\n",
      "[학습 3332] Loss: 1.0435827024924311\n",
      "[학습 3333] Loss: 0.8863122081290018\n",
      "[학습 3334] Loss: 1.2764289223589091\n",
      "[학습 3335] Loss: 1.159741457351618\n",
      "[학습 3336] Loss: 1.2499072255532586\n",
      "[학습 3337] Loss: 1.27860021071633\n",
      "[학습 3338] Loss: 1.2483106663328747\n",
      "[학습 3339] Loss: 1.0300154183642598\n",
      "[학습 3340] Loss: 1.1999442656884145\n",
      "[학습 3341] Loss: 1.1390116231530272\n",
      "[학습 3342] Loss: 1.302087111354054\n",
      "[학습 3343] Loss: 1.0806144411027814\n",
      "[학습 3344] Loss: 1.1715985848451975\n",
      "[학습 3345] Loss: 1.2245149894118443\n",
      "[학습 3346] Loss: 1.2254496597293676\n",
      "[학습 3347] Loss: 1.1511355545910362\n",
      "[학습 3348] Loss: 1.1598048630917719\n",
      "[학습 3349] Loss: 1.2218491283275856\n",
      "[학습 3350] Loss: 1.3335449033882605\n",
      "[학습 3351] Loss: 1.2654712960236854\n",
      "[학습 3352] Loss: 1.1539409480214176\n",
      "[학습 3353] Loss: 1.0341967397738532\n",
      "[학습 3354] Loss: 1.1788891706527256\n",
      "[학습 3355] Loss: 1.3317286980089365\n",
      "[학습 3356] Loss: 1.0505854463901125\n",
      "[학습 3357] Loss: 1.1877015651272862\n",
      "[학습 3358] Loss: 1.3305616694777485\n",
      "[학습 3359] Loss: 0.8432771344850692\n",
      "[학습 3360] Loss: 1.3368053102857516\n",
      "[학습 3361] Loss: 1.2750937762390937\n",
      "[학습 3362] Loss: 1.2867131317481018\n",
      "[학습 3363] Loss: 1.2803462320368584\n",
      "[학습 3364] Loss: 1.2863061912309781\n",
      "[학습 3365] Loss: 1.2593282643979515\n",
      "[학습 3366] Loss: 1.160139355125728\n",
      "[학습 3367] Loss: 1.085447905741244\n",
      "[학습 3368] Loss: 1.3639126931552612\n",
      "[학습 3369] Loss: 1.1391675381783857\n",
      "[학습 3370] Loss: 1.2138892897712856\n",
      "[학습 3371] Loss: 1.227074128631115\n",
      "[학습 3372] Loss: 1.2149888371778135\n",
      "[학습 3373] Loss: 1.1658659724521936\n",
      "[학습 3374] Loss: 1.4004356543143572\n",
      "[학습 3375] Loss: 1.1184649990807116\n",
      "[학습 3376] Loss: 1.2213073673922108\n",
      "[학습 3377] Loss: 1.2650602567442208\n",
      "[학습 3378] Loss: 1.1805141377143118\n",
      "[학습 3379] Loss: 0.9977033289898355\n",
      "[학습 3380] Loss: 1.3802327324751125\n",
      "[학습 3381] Loss: 1.226136845361317\n",
      "[학습 3382] Loss: 1.4503713749079734\n",
      "[학습 3383] Loss: 1.0931986971683496\n",
      "[학습 3384] Loss: 1.2464390028088999\n",
      "[학습 3385] Loss: 1.056999240599425\n",
      "[학습 3386] Loss: 1.1786137264624719\n",
      "[학습 3387] Loss: 1.149409040394763\n",
      "[학습 3388] Loss: 1.1694546880394339\n",
      "[학습 3389] Loss: 1.1271453499612403\n",
      "[학습 3390] Loss: 1.067575593478329\n",
      "[학습 3391] Loss: 1.0791981741739494\n",
      "[학습 3392] Loss: 1.3109091782870415\n",
      "[학습 3393] Loss: 1.148094165964111\n",
      "[학습 3394] Loss: 1.3230013641785174\n",
      "[학습 3395] Loss: 1.0097485312553827\n",
      "[학습 3396] Loss: 1.3237048314608995\n",
      "[학습 3397] Loss: 1.1564751126618034\n",
      "[학습 3398] Loss: 1.2669694892842651\n",
      "[학습 3399] Loss: 1.14016410620986\n",
      "[학습 3400] Loss: 1.222854770514924\n",
      "[학습 3401] Loss: 1.1440179859240915\n",
      "[학습 3402] Loss: 1.1125657059522966\n",
      "[학습 3403] Loss: 1.3068532334558716\n",
      "[학습 3404] Loss: 1.3398451057319576\n",
      "[학습 3405] Loss: 1.3581760210576692\n",
      "[학습 3406] Loss: 1.2801580584948054\n",
      "[학습 3407] Loss: 1.0949978567010425\n",
      "[학습 3408] Loss: 1.2039626357718356\n",
      "[학습 3409] Loss: 0.9872956618514129\n",
      "[학습 3410] Loss: 1.223916118345826\n",
      "[학습 3411] Loss: 1.3279980846486927\n",
      "[학습 3412] Loss: 1.0618777113952071\n",
      "[학습 3413] Loss: 0.9938374580438\n",
      "[학습 3414] Loss: 1.310863207033136\n",
      "[학습 3415] Loss: 1.156948116380942\n",
      "[학습 3416] Loss: 1.0989400756889114\n",
      "[학습 3417] Loss: 1.2103221941823545\n",
      "[학습 3418] Loss: 1.3612444531930565\n",
      "[학습 3419] Loss: 1.0345340211153873\n",
      "[학습 3420] Loss: 0.8677180485043882\n",
      "[학습 3421] Loss: 0.9379785225248802\n",
      "[학습 3422] Loss: 1.3030903296444334\n",
      "[학습 3423] Loss: 1.2128361348051997\n",
      "[학습 3424] Loss: 1.0773189881210867\n",
      "[학습 3425] Loss: 1.028507689354952\n",
      "[학습 3426] Loss: 1.2923536166744074\n",
      "[학습 3427] Loss: 1.1364772499066205\n",
      "[학습 3428] Loss: 1.2858912950207741\n",
      "[학습 3429] Loss: 1.0316521318898189\n",
      "[학습 3430] Loss: 1.5373906811737552\n",
      "[학습 3431] Loss: 1.1000064979519195\n",
      "[학습 3432] Loss: 0.9452222099972818\n",
      "[학습 3433] Loss: 1.2585647864057272\n",
      "[학습 3434] Loss: 1.2132544220983275\n",
      "[학습 3435] Loss: 1.1644690925273873\n",
      "[학습 3436] Loss: 1.139230759494254\n",
      "[학습 3437] Loss: 1.2069935629576378\n",
      "[학습 3438] Loss: 1.1761224233712286\n",
      "[학습 3439] Loss: 1.3415421346993022\n",
      "[학습 3440] Loss: 1.1571706585771784\n",
      "[학습 3441] Loss: 1.0483844067662538\n",
      "[학습 3442] Loss: 1.2877398476024697\n",
      "[학습 3443] Loss: 1.326128974275552\n",
      "[학습 3444] Loss: 1.054499147332684\n",
      "[학습 3445] Loss: 1.163226834033587\n",
      "[학습 3446] Loss: 0.9897754722931089\n",
      "[학습 3447] Loss: 1.0311572406171425\n",
      "[학습 3448] Loss: 1.1422191516557034\n",
      "[학습 3449] Loss: 0.9785062903722164\n",
      "[학습 3450] Loss: 1.248158014214732\n",
      "[학습 3451] Loss: 1.1676804405867847\n",
      "[학습 3452] Loss: 1.3601707401631242\n",
      "[학습 3453] Loss: 1.1602576182938664\n",
      "[학습 3454] Loss: 0.975999049933906\n",
      "[학습 3455] Loss: 1.1683940990816237\n",
      "[학습 3456] Loss: 1.1127882987233315\n",
      "[학습 3457] Loss: 1.1885404378609945\n",
      "[학습 3458] Loss: 1.0835610140441276\n",
      "[학습 3459] Loss: 1.1458413996159498\n",
      "[학습 3460] Loss: 1.2303528521838403\n",
      "[학습 3461] Loss: 1.308275819204349\n",
      "[학습 3462] Loss: 1.1211237703395331\n",
      "[학습 3463] Loss: 1.099404831927475\n",
      "[학습 3464] Loss: 1.0068485923859143\n",
      "[학습 3465] Loss: 0.9233946139422489\n",
      "[학습 3466] Loss: 1.195365325443441\n",
      "[학습 3467] Loss: 1.1289459133963897\n",
      "[학습 3468] Loss: 1.1832912323975873\n",
      "[학습 3469] Loss: 1.0277552849765266\n",
      "[학습 3470] Loss: 1.2048555276784818\n",
      "[학습 3471] Loss: 1.1609412278652564\n",
      "[학습 3472] Loss: 1.2536781546294877\n",
      "[학습 3473] Loss: 1.1859568908540767\n",
      "[학습 3474] Loss: 1.2022379951985795\n",
      "[학습 3475] Loss: 1.3416746218694073\n",
      "[학습 3476] Loss: 1.2584871641732902\n",
      "[학습 3477] Loss: 0.9579470449912205\n",
      "[학습 3478] Loss: 1.2058709054620917\n",
      "[학습 3479] Loss: 1.2107535230531417\n",
      "[학습 3480] Loss: 1.0203277272608653\n",
      "[학습 3481] Loss: 1.2456077827870755\n",
      "[학습 3482] Loss: 1.000758039602011\n",
      "[학습 3483] Loss: 1.0407681491807392\n",
      "[학습 3484] Loss: 1.0832110411245452\n",
      "[학습 3485] Loss: 1.209789422219702\n",
      "[학습 3486] Loss: 1.2603820188473915\n",
      "[학습 3487] Loss: 1.1963530748819848\n",
      "[학습 3488] Loss: 1.0273011734206594\n",
      "[학습 3489] Loss: 1.2741070733510533\n",
      "[학습 3490] Loss: 1.1285407372652698\n",
      "[학습 3491] Loss: 1.066972099209667\n",
      "[학습 3492] Loss: 1.0791112752594554\n",
      "[학습 3493] Loss: 1.2179211680869495\n",
      "[학습 3494] Loss: 1.4686781784691396\n",
      "[학습 3495] Loss: 1.1057167684621452\n",
      "[학습 3496] Loss: 1.2847737337806007\n",
      "[학습 3497] Loss: 1.2730499829934336\n",
      "[학습 3498] Loss: 1.3606006542972482\n",
      "[학습 3499] Loss: 1.2556487715338196\n",
      "[학습 3500] Loss: 1.2988658595758926\n",
      "[학습 3501] Loss: 1.0198551210939653\n",
      "[학습 3502] Loss: 1.228682100029779\n",
      "[학습 3503] Loss: 1.319393778511827\n",
      "[학습 3504] Loss: 1.183905178046366\n",
      "[학습 3505] Loss: 1.1322275635994634\n",
      "[학습 3506] Loss: 1.100965591244248\n",
      "[학습 3507] Loss: 0.889318576670236\n",
      "[학습 3508] Loss: 1.2721419003535095\n",
      "[학습 3509] Loss: 1.0351166652481758\n",
      "[학습 3510] Loss: 1.6487254008951482\n",
      "[학습 3511] Loss: 1.0428350641935276\n",
      "[학습 3512] Loss: 1.1217397705871957\n",
      "[학습 3513] Loss: 1.052583084808277\n",
      "[학습 3514] Loss: 1.066747794592658\n",
      "[학습 3515] Loss: 1.0957457499435532\n",
      "[학습 3516] Loss: 0.9685589622240672\n",
      "[학습 3517] Loss: 0.9797710192333463\n",
      "[학습 3518] Loss: 1.1476253127924259\n",
      "[학습 3519] Loss: 1.1422696495658666\n",
      "[학습 3520] Loss: 1.245811850716368\n",
      "[학습 3521] Loss: 1.0546685073080595\n",
      "[학습 3522] Loss: 1.2763065998369512\n",
      "[학습 3523] Loss: 1.3201778292414037\n",
      "[학습 3524] Loss: 0.9874679270742597\n",
      "[학습 3525] Loss: 1.0973832772769025\n",
      "[학습 3526] Loss: 1.0347707251129261\n",
      "[학습 3527] Loss: 1.1326276795023658\n",
      "[학습 3528] Loss: 1.3050759566740089\n",
      "[학습 3529] Loss: 1.060331932234822\n",
      "[학습 3530] Loss: 1.2393286595533146\n",
      "[학습 3531] Loss: 1.25129597213794\n",
      "[학습 3532] Loss: 1.3571756396910215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습 3533] Loss: 1.0779021978926542\n",
      "[학습 3534] Loss: 1.086492251227125\n",
      "[학습 3535] Loss: 1.169031189889534\n",
      "[학습 3536] Loss: 1.149639471307216\n",
      "[학습 3537] Loss: 1.1034634753731434\n",
      "[학습 3538] Loss: 1.2261716521306891\n",
      "[학습 3539] Loss: 1.0626906365878332\n",
      "[학습 3540] Loss: 1.1907960363349277\n",
      "[학습 3541] Loss: 1.308245279798082\n",
      "[학습 3542] Loss: 1.171310812578387\n",
      "[학습 3543] Loss: 0.9449537676439027\n",
      "[학습 3544] Loss: 1.0112284205045987\n",
      "[학습 3545] Loss: 1.3808642634947008\n",
      "[학습 3546] Loss: 1.0579353662585231\n",
      "[학습 3547] Loss: 1.0680646591219825\n",
      "[학습 3548] Loss: 0.9045699649505257\n",
      "[학습 3549] Loss: 1.41567059137585\n",
      "[학습 3550] Loss: 1.063579969286733\n",
      "[학습 3551] Loss: 1.1376483383855356\n",
      "[학습 3552] Loss: 1.196886666975755\n",
      "[학습 3553] Loss: 1.0266891216470921\n",
      "[학습 3554] Loss: 1.0734625972942922\n",
      "[학습 3555] Loss: 1.3602342335116986\n",
      "[학습 3556] Loss: 1.1605533074956995\n",
      "[학습 3557] Loss: 1.11110132929567\n",
      "[학습 3558] Loss: 1.192260232059431\n",
      "[학습 3559] Loss: 1.0199986313643818\n",
      "[학습 3560] Loss: 1.122909872848655\n",
      "[학습 3561] Loss: 1.3254097235539957\n",
      "[학습 3562] Loss: 1.1808685818993618\n",
      "[학습 3563] Loss: 1.082827970740204\n",
      "[학습 3564] Loss: 1.2020775831713413\n",
      "[학습 3565] Loss: 1.0028270276343847\n",
      "[학습 3566] Loss: 1.0622018228791135\n",
      "[학습 3567] Loss: 1.1245978634660643\n",
      "[학습 3568] Loss: 1.195369259777698\n",
      "[학습 3569] Loss: 1.2061509973678113\n",
      "[학습 3570] Loss: 1.1065447930164916\n",
      "[학습 3571] Loss: 1.139548964963849\n",
      "[학습 3572] Loss: 1.1631619769671873\n",
      "[학습 3573] Loss: 1.0078241684268574\n",
      "[학습 3574] Loss: 1.3199000548464124\n",
      "[학습 3575] Loss: 1.2166705526906116\n",
      "[학습 3576] Loss: 1.0003606688081736\n",
      "[학습 3577] Loss: 1.1969048090782164\n",
      "[학습 3578] Loss: 1.0245359367125113\n",
      "[학습 3579] Loss: 1.059752453894052\n",
      "[학습 3580] Loss: 1.2279019487391412\n",
      "[학습 3581] Loss: 1.000945723314294\n",
      "[학습 3582] Loss: 0.9744291252559133\n",
      "[학습 3583] Loss: 1.3045806673692621\n",
      "[학습 3584] Loss: 1.0427691896061226\n",
      "[학습 3585] Loss: 1.013182891994746\n",
      "[학습 3586] Loss: 1.2048224156967675\n",
      "[학습 3587] Loss: 1.1985821628795765\n",
      "[학습 3588] Loss: 1.0117280509900843\n",
      "[학습 3589] Loss: 0.9252517724743555\n",
      "[학습 3590] Loss: 1.2712547754281582\n",
      "[학습 3591] Loss: 1.0786195497924806\n",
      "[학습 3592] Loss: 1.272048184987939\n",
      "[학습 3593] Loss: 1.195665245579622\n",
      "[학습 3594] Loss: 1.1773794993910744\n",
      "[학습 3595] Loss: 0.9561189919660555\n",
      "[학습 3596] Loss: 1.369784402145222\n",
      "[학습 3597] Loss: 1.531008741643932\n",
      "[학습 3598] Loss: 1.1209298878829812\n",
      "[학습 3599] Loss: 1.3586242228589103\n",
      "[학습 3600] Loss: 1.1207137530858797\n",
      "[학습 3601] Loss: 1.2496664054852795\n",
      "[학습 3602] Loss: 1.3114340579522548\n",
      "[학습 3603] Loss: 1.235618485664198\n",
      "[학습 3604] Loss: 1.0340982875667988\n",
      "[학습 3605] Loss: 0.9461941061743928\n",
      "[학습 3606] Loss: 1.0253404998743643\n",
      "[학습 3607] Loss: 1.11518184118958\n",
      "[학습 3608] Loss: 1.1743407017023009\n",
      "[학습 3609] Loss: 1.1242062807712017\n",
      "[학습 3610] Loss: 0.90352324541211\n",
      "[학습 3611] Loss: 1.2159010438662097\n",
      "[학습 3612] Loss: 1.1798084589360067\n",
      "[학습 3613] Loss: 0.9732270403809374\n",
      "[학습 3614] Loss: 1.354075169633124\n",
      "[학습 3615] Loss: 1.3028973661056191\n",
      "[학습 3616] Loss: 1.1462221134689514\n",
      "[학습 3617] Loss: 1.2063481336797317\n",
      "[학습 3618] Loss: 1.036982867226932\n",
      "[학습 3619] Loss: 1.2602008337268538\n",
      "[학습 3620] Loss: 1.1725891422657764\n",
      "[학습 3621] Loss: 1.1729418544317378\n",
      "[학습 3622] Loss: 1.3713667833432248\n",
      "[학습 3623] Loss: 1.1257123014094728\n",
      "[학습 3624] Loss: 1.0684978692625375\n",
      "[학습 3625] Loss: 1.3368589219852196\n",
      "[학습 3626] Loss: 1.498811708933863\n",
      "[학습 3627] Loss: 1.2269104010450647\n",
      "[학습 3628] Loss: 1.181523137344176\n",
      "[학습 3629] Loss: 1.1799653011274673\n",
      "[학습 3630] Loss: 1.2625699613448926\n",
      "[학습 3631] Loss: 0.87335484565874\n",
      "[학습 3632] Loss: 1.2747268476603504\n",
      "[학습 3633] Loss: 1.0685596764664962\n",
      "[학습 3634] Loss: 1.1500651590760276\n",
      "[학습 3635] Loss: 1.3319760751129774\n",
      "[학습 3636] Loss: 1.0751646152688132\n",
      "[학습 3637] Loss: 1.3769224875049948\n",
      "[학습 3638] Loss: 1.2097012651760612\n",
      "[학습 3639] Loss: 1.0951000724885451\n",
      "[학습 3640] Loss: 1.1532307939470334\n",
      "[학습 3641] Loss: 1.3256729080030822\n",
      "[학습 3642] Loss: 1.2488162120736348\n",
      "[학습 3643] Loss: 1.0018457385791457\n",
      "[학습 3644] Loss: 1.2234595778470645\n",
      "[학습 3645] Loss: 1.0813269981732834\n",
      "[학습 3646] Loss: 1.1274192195311392\n",
      "[학습 3647] Loss: 1.1957384208496515\n",
      "[학습 3648] Loss: 1.2917137254354907\n",
      "[학습 3649] Loss: 1.1313075082150108\n",
      "[학습 3650] Loss: 1.2534880314210044\n",
      "[학습 3651] Loss: 1.0571291973646146\n",
      "[학습 3652] Loss: 1.4237350242915283\n",
      "[학습 3653] Loss: 1.3111431873601413\n",
      "[학습 3654] Loss: 1.3085670073947318\n",
      "[학습 3655] Loss: 1.1335759678166277\n",
      "[학습 3656] Loss: 1.041804940242195\n",
      "[학습 3657] Loss: 1.2764851145864657\n",
      "[학습 3658] Loss: 1.2786839526308074\n",
      "[학습 3659] Loss: 1.2316499493543978\n",
      "[학습 3660] Loss: 1.2174974014495326\n",
      "[학습 3661] Loss: 1.1830564492975464\n",
      "[학습 3662] Loss: 1.098314598343785\n",
      "[학습 3663] Loss: 1.2089947417454467\n",
      "[학습 3664] Loss: 1.0178731099490417\n",
      "[학습 3665] Loss: 1.256027586852246\n",
      "[학습 3666] Loss: 0.9998841337656146\n",
      "[학습 3667] Loss: 1.341709174163533\n",
      "[학습 3668] Loss: 1.1981846553432174\n",
      "[학습 3669] Loss: 1.0299459038146817\n",
      "[학습 3670] Loss: 1.344094696494552\n",
      "[학습 3671] Loss: 1.202359494362896\n",
      "[학습 3672] Loss: 1.164893659843155\n",
      "[학습 3673] Loss: 0.9887512259024543\n",
      "[학습 3674] Loss: 0.8748137371383313\n",
      "[학습 3675] Loss: 1.3016319998031605\n",
      "[학습 3676] Loss: 1.1512907061826991\n",
      "[학습 3677] Loss: 1.3242741272776755\n",
      "[학습 3678] Loss: 1.3201342155029374\n",
      "[학습 3679] Loss: 1.4630748186997056\n",
      "[학습 3680] Loss: 1.0643850698018524\n",
      "[학습 3681] Loss: 1.033637763092109\n",
      "[학습 3682] Loss: 1.0960671435121991\n",
      "[학습 3683] Loss: 1.0360917567105046\n",
      "[학습 3684] Loss: 1.1530512943105993\n",
      "[학습 3685] Loss: 0.9631865053687347\n",
      "[학습 3686] Loss: 1.3039222757474573\n",
      "[학습 3687] Loss: 0.9483537737898111\n",
      "[학습 3688] Loss: 1.08032877240899\n",
      "[학습 3689] Loss: 1.338388660128475\n",
      "[학습 3690] Loss: 1.0986658741767896\n",
      "[학습 3691] Loss: 1.2346486422348508\n",
      "[학습 3692] Loss: 1.1401603780830611\n",
      "[학습 3693] Loss: 1.086495585922368\n",
      "[학습 3694] Loss: 1.1758347645975793\n",
      "[학습 3695] Loss: 0.9323221407981643\n",
      "[학습 3696] Loss: 0.9979394626972743\n",
      "[학습 3697] Loss: 1.3356942005957335\n",
      "[학습 3698] Loss: 1.0014400022682781\n",
      "[학습 3699] Loss: 1.1795992058508502\n",
      "[학습 3700] Loss: 1.148817619501422\n",
      "[학습 3701] Loss: 1.1577567990924849\n",
      "[학습 3702] Loss: 1.1974930701665798\n",
      "[학습 3703] Loss: 1.3140744412657093\n",
      "[학습 3704] Loss: 0.9834339387857609\n",
      "[학습 3705] Loss: 1.0871690303981572\n",
      "[학습 3706] Loss: 0.9630882702764676\n",
      "[학습 3707] Loss: 0.9454283453176076\n",
      "[학습 3708] Loss: 1.224754974303162\n",
      "[학습 3709] Loss: 1.20630350696965\n",
      "[학습 3710] Loss: 0.8878097532843635\n",
      "[학습 3711] Loss: 1.081282420015348\n",
      "[학습 3712] Loss: 1.0153833233096556\n",
      "[학습 3713] Loss: 1.0475160285613294\n",
      "[학습 3714] Loss: 1.051339648401591\n",
      "[학습 3715] Loss: 0.9745613197356883\n",
      "[학습 3716] Loss: 0.9379727150040185\n",
      "[학습 3717] Loss: 1.1473782701826816\n",
      "[학습 3718] Loss: 1.206311727783732\n",
      "[학습 3719] Loss: 1.2105669461319957\n",
      "[학습 3720] Loss: 1.3528700065028176\n",
      "[학습 3721] Loss: 1.0856633106923457\n",
      "[학습 3722] Loss: 1.3203181214821627\n",
      "[학습 3723] Loss: 0.9629649090267904\n",
      "[학습 3724] Loss: 1.038260487474225\n",
      "[학습 3725] Loss: 1.0436555565572012\n",
      "[학습 3726] Loss: 1.1859533068711554\n",
      "[학습 3727] Loss: 0.9553442550750484\n",
      "[학습 3728] Loss: 1.24345338310863\n",
      "[학습 3729] Loss: 1.1952340304870477\n",
      "[학습 3730] Loss: 1.0682770877182828\n",
      "[학습 3731] Loss: 1.2177147642555513\n",
      "[학습 3732] Loss: 1.2701674182308713\n",
      "[학습 3733] Loss: 1.2057890444496184\n",
      "[학습 3734] Loss: 1.2464358934285085\n",
      "[학습 3735] Loss: 1.289847994921683\n",
      "[학습 3736] Loss: 1.185752040312299\n",
      "[학습 3737] Loss: 1.425216195235858\n",
      "[학습 3738] Loss: 0.8456298557481307\n",
      "[학습 3739] Loss: 1.2749875528468102\n",
      "[학습 3740] Loss: 1.3722340817257213\n",
      "[학습 3741] Loss: 1.1267948049399008\n",
      "[학습 3742] Loss: 1.0894919164234793\n",
      "[학습 3743] Loss: 1.061213693898252\n",
      "[학습 3744] Loss: 0.839863031266039\n",
      "[학습 3745] Loss: 1.2396100551215152\n",
      "[학습 3746] Loss: 1.193759056426325\n",
      "[학습 3747] Loss: 1.2188207438961225\n",
      "[학습 3748] Loss: 1.2887746046954198\n",
      "[학습 3749] Loss: 1.3044500649292172\n",
      "[학습 3750] Loss: 1.1653941463844029\n",
      "[학습 3751] Loss: 0.9429350481180708\n",
      "[학습 3752] Loss: 1.090087124688774\n",
      "[학습 3753] Loss: 0.8918989694927287\n",
      "[학습 3754] Loss: 1.3552180922593735\n",
      "[학습 3755] Loss: 1.2598310516213522\n",
      "[학습 3756] Loss: 1.252019646315774\n",
      "[학습 3757] Loss: 1.0652578041079417\n",
      "[학습 3758] Loss: 0.9757767142113778\n",
      "[학습 3759] Loss: 1.0218574504736084\n",
      "[학습 3760] Loss: 1.2949691895644093\n",
      "[학습 3761] Loss: 1.0835685771668266\n",
      "[학습 3762] Loss: 1.0474880282479864\n",
      "[학습 3763] Loss: 1.1702534883337485\n",
      "[학습 3764] Loss: 1.0327280426088294\n",
      "[학습 3765] Loss: 1.2072135853232426\n",
      "[학습 3766] Loss: 1.1180879926002798\n",
      "[학습 3767] Loss: 1.472470299377627\n",
      "[학습 3768] Loss: 1.160906532859149\n",
      "[학습 3769] Loss: 1.1608485572913245\n",
      "[학습 3770] Loss: 0.9976473858625293\n",
      "[학습 3771] Loss: 1.2737985113221455\n",
      "[학습 3772] Loss: 1.0510056282825542\n",
      "[학습 3773] Loss: 1.3546335387094566\n",
      "[학습 3774] Loss: 1.0876642725873793\n",
      "[학습 3775] Loss: 1.137356468189733\n",
      "[학습 3776] Loss: 1.047468041875096\n",
      "[학습 3777] Loss: 1.1158232708176974\n",
      "[학습 3778] Loss: 1.3799282675041715\n",
      "[학습 3779] Loss: 1.1389777976295046\n",
      "[학습 3780] Loss: 1.0857226639267925\n",
      "[학습 3781] Loss: 1.2753003239653833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습 3782] Loss: 1.2521055642769179\n",
      "[학습 3783] Loss: 1.132172054626539\n",
      "[학습 3784] Loss: 1.1414738287947492\n",
      "[학습 3785] Loss: 1.2209430451511643\n",
      "[학습 3786] Loss: 1.0066210151716612\n",
      "[학습 3787] Loss: 1.2086549435237748\n",
      "[학습 3788] Loss: 1.2169840291997644\n",
      "[학습 3789] Loss: 1.2377377479815002\n",
      "[학습 3790] Loss: 1.0841372819623516\n",
      "[학습 3791] Loss: 1.0283431408129229\n",
      "[학습 3792] Loss: 1.1845340209663104\n",
      "[학습 3793] Loss: 1.112818771143742\n",
      "[학습 3794] Loss: 0.9862468104504548\n",
      "[학습 3795] Loss: 0.9704188515574728\n",
      "[학습 3796] Loss: 1.0665047935151541\n",
      "[학습 3797] Loss: 1.078922245474186\n",
      "[학습 3798] Loss: 1.107087441999273\n",
      "[학습 3799] Loss: 1.1548735629166926\n",
      "[학습 3800] Loss: 1.1900005915250793\n",
      "[학습 3801] Loss: 1.2067822730787123\n",
      "[학습 3802] Loss: 1.2132663582879493\n",
      "[학습 3803] Loss: 1.2764054905902924\n",
      "[학습 3804] Loss: 1.2260909368068076\n",
      "[학습 3805] Loss: 1.0985343790411048\n",
      "[학습 3806] Loss: 1.5210722484734396\n",
      "[학습 3807] Loss: 1.2834914882464015\n",
      "[학습 3808] Loss: 1.04154493615254\n",
      "[학습 3809] Loss: 1.1582951287415324\n",
      "[학습 3810] Loss: 1.0539030469162214\n",
      "[학습 3811] Loss: 0.9767727618885311\n",
      "[학습 3812] Loss: 1.085596015850143\n",
      "[학습 3813] Loss: 1.1670931465528689\n",
      "[학습 3814] Loss: 1.1460893490991677\n",
      "[학습 3815] Loss: 1.0672764360182392\n",
      "[학습 3816] Loss: 1.2957424294949942\n",
      "[학습 3817] Loss: 1.0570921903740658\n",
      "[학습 3818] Loss: 1.1146790345208366\n",
      "[학습 3819] Loss: 1.1983430447422594\n",
      "[학습 3820] Loss: 1.0602379021521968\n",
      "[학습 3821] Loss: 1.0636254134348677\n",
      "[학습 3822] Loss: 0.9603251920234682\n",
      "[학습 3823] Loss: 0.9178506560375542\n",
      "[학습 3824] Loss: 1.1526746163548163\n",
      "[학습 3825] Loss: 1.1433180263117821\n",
      "[학습 3826] Loss: 1.390641639338192\n",
      "[학습 3827] Loss: 1.3386130380715813\n",
      "[학습 3828] Loss: 1.165402409555413\n",
      "[학습 3829] Loss: 1.1503538849934203\n",
      "[학습 3830] Loss: 1.0258329741057484\n",
      "[학습 3831] Loss: 1.0901498915267585\n",
      "[학습 3832] Loss: 0.8492729773212696\n",
      "[학습 3833] Loss: 1.3274614150778141\n",
      "[학습 3834] Loss: 0.9833540684996126\n",
      "[학습 3835] Loss: 1.125728095341389\n",
      "[학습 3836] Loss: 0.9042240338554106\n",
      "[학습 3837] Loss: 1.1339435563171374\n",
      "[학습 3838] Loss: 0.9798964051478286\n",
      "[학습 3839] Loss: 0.8995277639036294\n",
      "[학습 3840] Loss: 1.2145836381833819\n",
      "[학습 3841] Loss: 1.0867720133715266\n",
      "[학습 3842] Loss: 1.0986026637190067\n",
      "[학습 3843] Loss: 1.1839043738478923\n",
      "[학습 3844] Loss: 1.0034732236740251\n",
      "[학습 3845] Loss: 1.0986594342555023\n",
      "[학습 3846] Loss: 1.1076331670727464\n",
      "[학습 3847] Loss: 1.1001574445165028\n",
      "[학습 3848] Loss: 1.1540600869623319\n",
      "[학습 3849] Loss: 1.0404318691052212\n",
      "[학습 3850] Loss: 0.982455692046395\n",
      "[학습 3851] Loss: 1.1680279017124162\n",
      "[학습 3852] Loss: 1.0626137396493716\n",
      "[학습 3853] Loss: 1.3444097036027551\n",
      "[학습 3854] Loss: 1.0767938121945784\n",
      "[학습 3855] Loss: 1.328859325620964\n",
      "[학습 3856] Loss: 1.1388508813781584\n",
      "[학습 3857] Loss: 1.3384236922787296\n",
      "[학습 3858] Loss: 1.1488369956028435\n",
      "[학습 3859] Loss: 1.147306015897299\n",
      "[학습 3860] Loss: 1.1899460508152457\n",
      "[학습 3861] Loss: 1.1357818556386827\n",
      "[학습 3862] Loss: 1.1205582282697168\n",
      "[학습 3863] Loss: 0.9859774078434856\n",
      "[학습 3864] Loss: 1.3330052033282749\n",
      "[학습 3865] Loss: 1.2279314347160912\n",
      "[학습 3866] Loss: 1.0916560754401374\n",
      "[학습 3867] Loss: 1.1923761171084835\n",
      "[학습 3868] Loss: 0.9628264601599952\n",
      "[학습 3869] Loss: 1.1034296261726724\n",
      "[학습 3870] Loss: 1.2565181902885514\n",
      "[학습 3871] Loss: 1.1935343312924889\n",
      "[학습 3872] Loss: 1.2115712063166777\n",
      "[학습 3873] Loss: 1.12407983455497\n",
      "[학습 3874] Loss: 1.0865582334002177\n",
      "[학습 3875] Loss: 1.3113536569405415\n",
      "[학습 3876] Loss: 1.0372127003332756\n",
      "[학습 3877] Loss: 1.2399218553139582\n",
      "[학습 3878] Loss: 0.9461008050821635\n",
      "[학습 3879] Loss: 1.0127075630684608\n",
      "[학습 3880] Loss: 1.2370950527601137\n",
      "[학습 3881] Loss: 1.062868333426851\n",
      "[학습 3882] Loss: 1.252686394017148\n",
      "[학습 3883] Loss: 1.2910035797650756\n",
      "[학습 3884] Loss: 1.2162793035570767\n",
      "[학습 3885] Loss: 1.1016994242854736\n",
      "[학습 3886] Loss: 1.0855534130624336\n",
      "[학습 3887] Loss: 0.7880871086779493\n",
      "[학습 3888] Loss: 1.1817227212777561\n",
      "[학습 3889] Loss: 0.9945713582405097\n",
      "[학습 3890] Loss: 1.107659076284761\n",
      "[학습 3891] Loss: 0.9882145643616812\n",
      "[학습 3892] Loss: 1.1056894993820705\n",
      "[학습 3893] Loss: 1.3189622471946085\n",
      "[학습 3894] Loss: 1.11315435402534\n",
      "[학습 3895] Loss: 0.981671269043434\n",
      "[학습 3896] Loss: 1.1873447634770078\n",
      "[학습 3897] Loss: 1.1452916689236485\n",
      "[학습 3898] Loss: 1.088179204815053\n",
      "[학습 3899] Loss: 1.0455713858472886\n",
      "[학습 3900] Loss: 1.2914441672419763\n",
      "[학습 3901] Loss: 1.1683913765028515\n",
      "[학습 3902] Loss: 1.1065622022632962\n",
      "[학습 3903] Loss: 1.0499896908861572\n",
      "[학습 3904] Loss: 1.0812163239671022\n",
      "[학습 3905] Loss: 1.1428871025296892\n",
      "[학습 3906] Loss: 1.2829023069789962\n",
      "[학습 3907] Loss: 1.1396101824215334\n",
      "[학습 3908] Loss: 1.323289414984267\n",
      "[학습 3909] Loss: 0.961969613848977\n",
      "[학습 3910] Loss: 1.1602655168305922\n",
      "[학습 3911] Loss: 1.1481889246294112\n",
      "[학습 3912] Loss: 1.0626165252877606\n",
      "[학습 3913] Loss: 1.0895573621337897\n",
      "[학습 3914] Loss: 1.1060462175555554\n",
      "[학습 3915] Loss: 1.1646898074333272\n",
      "[학습 3916] Loss: 1.1624086068735326\n",
      "[학습 3917] Loss: 1.1897867159142823\n",
      "[학습 3918] Loss: 1.0021444560358086\n",
      "[학습 3919] Loss: 1.1907030576393565\n",
      "[학습 3920] Loss: 1.149421088733759\n",
      "[학습 3921] Loss: 1.250661475178785\n",
      "[학습 3922] Loss: 1.025523195436228\n",
      "[학습 3923] Loss: 1.125250216494033\n",
      "[학습 3924] Loss: 1.0806595682499545\n",
      "[학습 3925] Loss: 1.260486565577408\n",
      "[학습 3926] Loss: 1.1541121149496631\n",
      "[학습 3927] Loss: 1.3167209223866052\n",
      "[학습 3928] Loss: 1.1336486051784143\n",
      "[학습 3929] Loss: 1.1389005664553022\n",
      "[학습 3930] Loss: 1.080689793381838\n",
      "[학습 3931] Loss: 1.4325656229388202\n",
      "[학습 3932] Loss: 1.0245776780784448\n",
      "[학습 3933] Loss: 1.0772629195064227\n",
      "[학습 3934] Loss: 1.010517100983087\n",
      "[학습 3935] Loss: 1.1640869589534188\n",
      "[학습 3936] Loss: 1.1174963664324726\n",
      "[학습 3937] Loss: 0.8901296808659971\n",
      "[학습 3938] Loss: 1.3192209593955786\n",
      "[학습 3939] Loss: 1.0865956111164323\n",
      "[학습 3940] Loss: 1.1673649087402311\n",
      "[학습 3941] Loss: 1.1610626844003757\n",
      "[학습 3942] Loss: 0.8997333912822154\n",
      "[학습 3943] Loss: 1.179211416945639\n",
      "[학습 3944] Loss: 1.300663067476358\n",
      "[학습 3945] Loss: 1.0706903455909191\n",
      "[학습 3946] Loss: 1.147047913262729\n",
      "[학습 3947] Loss: 1.0676000499527913\n",
      "[학습 3948] Loss: 1.1727592968300422\n",
      "[학습 3949] Loss: 1.1218864001746207\n",
      "[학습 3950] Loss: 1.3023263082959675\n",
      "[학습 3951] Loss: 1.185480267354815\n",
      "[학습 3952] Loss: 0.9774145070293931\n",
      "[학습 3953] Loss: 1.2537353585922866\n",
      "[학습 3954] Loss: 1.086379119857621\n",
      "[학습 3955] Loss: 1.158383799389383\n",
      "[학습 3956] Loss: 1.1856663839351813\n",
      "[학습 3957] Loss: 0.9416136447682885\n",
      "[학습 3958] Loss: 1.1798636555073356\n",
      "[학습 3959] Loss: 1.1077163011575692\n",
      "[학습 3960] Loss: 1.1553509824776693\n",
      "[학습 3961] Loss: 1.3887631429458063\n",
      "[학습 3962] Loss: 1.1714995155901948\n",
      "[학습 3963] Loss: 0.9948860055030364\n",
      "[학습 3964] Loss: 1.0711817793933889\n",
      "[학습 3965] Loss: 1.1136909443854908\n",
      "[학습 3966] Loss: 0.9942685023323306\n",
      "[학습 3967] Loss: 1.2141657439800464\n",
      "[학습 3968] Loss: 1.0642499669620684\n",
      "[학습 3969] Loss: 1.0650965376802335\n",
      "[학습 3970] Loss: 1.193218551565035\n",
      "[학습 3971] Loss: 1.1377078407064958\n",
      "[학습 3972] Loss: 1.1882255841040785\n",
      "[학습 3973] Loss: 1.1057666984061363\n",
      "[학습 3974] Loss: 1.2549669633856455\n",
      "[학습 3975] Loss: 1.0972996488124898\n",
      "[학습 3976] Loss: 1.1014850014528295\n",
      "[학습 3977] Loss: 0.9836700949124787\n",
      "[학습 3978] Loss: 1.2269067051444467\n",
      "[학습 3979] Loss: 0.8993095538137328\n",
      "[학습 3980] Loss: 0.818555258101599\n",
      "[학습 3981] Loss: 1.1583292529138967\n",
      "[학습 3982] Loss: 1.1774225015075785\n",
      "[학습 3983] Loss: 1.0238523523300957\n",
      "[학습 3984] Loss: 1.226999323071152\n",
      "[학습 3985] Loss: 1.1107201877691901\n",
      "[학습 3986] Loss: 1.121197794693506\n",
      "[학습 3987] Loss: 1.0745341782578348\n",
      "[학습 3988] Loss: 0.9907183682090374\n",
      "[학습 3989] Loss: 1.143649958562432\n",
      "[학습 3990] Loss: 1.0820658635006555\n",
      "[학습 3991] Loss: 0.9613344072583953\n",
      "[학습 3992] Loss: 0.9711710067223879\n",
      "[학습 3993] Loss: 1.2165619328304205\n",
      "[학습 3994] Loss: 0.8801988639140358\n",
      "[학습 3995] Loss: 0.9847736146212017\n",
      "[학습 3996] Loss: 1.2158462820914229\n",
      "[학습 3997] Loss: 1.0968584495807747\n",
      "[학습 3998] Loss: 1.2427553058623242\n",
      "[학습 3999] Loss: 1.1468491144612227\n",
      "[학습 4000] Loss: 1.080000912459067\n",
      "[학습 4001] Loss: 1.0108100685290484\n",
      "[학습 4002] Loss: 0.9665947833352603\n",
      "[학습 4003] Loss: 0.8446751780896696\n",
      "[학습 4004] Loss: 0.9775038545252062\n",
      "[학습 4005] Loss: 1.261304732154893\n",
      "[학습 4006] Loss: 0.9400761692361636\n",
      "[학습 4007] Loss: 1.2085853823750126\n",
      "[학습 4008] Loss: 1.0373830278566478\n",
      "[학습 4009] Loss: 1.1920763200731337\n",
      "[학습 4010] Loss: 1.2712540238051155\n",
      "[학습 4011] Loss: 1.4563453947216838\n",
      "[학습 4012] Loss: 1.1248568717475556\n",
      "[학습 4013] Loss: 1.1612304348819842\n",
      "[학습 4014] Loss: 1.1806435286702421\n",
      "[학습 4015] Loss: 1.101828746882472\n",
      "[학습 4016] Loss: 0.9011609316723177\n",
      "[학습 4017] Loss: 1.1998624642762852\n",
      "[학습 4018] Loss: 1.089082400495153\n",
      "[학습 4019] Loss: 1.3809634429670679\n",
      "[학습 4020] Loss: 1.0345024420910713\n",
      "[학습 4021] Loss: 1.0287101414493858\n",
      "[학습 4022] Loss: 1.1410217952047719\n",
      "[학습 4023] Loss: 1.3903869464690088\n",
      "[학습 4024] Loss: 1.1039111855374124\n",
      "[학습 4025] Loss: 1.1043376676513088\n",
      "[학습 4026] Loss: 1.0556935136367778\n",
      "[학습 4027] Loss: 1.161012323423121\n",
      "[학습 4028] Loss: 1.036864439396477\n",
      "[학습 4029] Loss: 0.912992696608016\n",
      "[학습 4030] Loss: 1.107909835639952\n",
      "[학습 4031] Loss: 1.1378764914027801\n",
      "[학습 4032] Loss: 1.0981822034163553\n",
      "[학습 4033] Loss: 1.402225544148066\n",
      "[학습 4034] Loss: 1.1052278868112158\n",
      "[학습 4035] Loss: 1.1141128439955248\n",
      "[학습 4036] Loss: 1.2285828739087068\n",
      "[학습 4037] Loss: 1.1222294889085276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습 4038] Loss: 0.9851126361786546\n",
      "[학습 4039] Loss: 1.2405918221378236\n",
      "[학습 4040] Loss: 1.048953257938839\n",
      "[학습 4041] Loss: 1.1420176531247614\n",
      "[학습 4042] Loss: 1.209491919249682\n",
      "[학습 4043] Loss: 1.1594759179903233\n",
      "[학습 4044] Loss: 1.071894000797792\n",
      "[학습 4045] Loss: 1.003655792731829\n",
      "[학습 4046] Loss: 1.2966026438456848\n",
      "[학습 4047] Loss: 1.158869473097485\n",
      "[학습 4048] Loss: 1.0744873653750766\n",
      "[학습 4049] Loss: 1.1321908395356424\n",
      "[학습 4050] Loss: 0.9780036855279672\n",
      "[학습 4051] Loss: 1.1251840123791326\n",
      "[학습 4052] Loss: 1.0110872199403707\n",
      "[학습 4053] Loss: 0.8552407505443537\n",
      "[학습 4054] Loss: 1.1289161024070848\n",
      "[학습 4055] Loss: 1.180435489976425\n",
      "[학습 4056] Loss: 1.1016094691173375\n",
      "[학습 4057] Loss: 1.1527785858467459\n",
      "[학습 4058] Loss: 1.1375496435470969\n",
      "[학습 4059] Loss: 0.9015371303897208\n",
      "[학습 4060] Loss: 1.1621254697591983\n",
      "[학습 4061] Loss: 1.2232401960705528\n",
      "[학습 4062] Loss: 1.0241473469730276\n",
      "[학습 4063] Loss: 1.0889269016819674\n",
      "[학습 4064] Loss: 1.2151907251820793\n",
      "[학습 4065] Loss: 1.1225671698988657\n",
      "[학습 4066] Loss: 1.1225350207599327\n",
      "[학습 4067] Loss: 1.0975375559240927\n",
      "[학습 4068] Loss: 0.899797406650342\n",
      "[학습 4069] Loss: 1.1809202337156917\n",
      "[학습 4070] Loss: 1.2137434572076957\n",
      "[학습 4071] Loss: 0.9672893472596581\n",
      "[학습 4072] Loss: 0.8005308788080506\n",
      "[학습 4073] Loss: 1.040695838311868\n",
      "[학습 4074] Loss: 1.0339452315264872\n",
      "[학습 4075] Loss: 1.182894550096581\n",
      "[학습 4076] Loss: 1.076362014189318\n",
      "[학습 4077] Loss: 1.1137691850503761\n",
      "[학습 4078] Loss: 0.9616659951271873\n",
      "[학습 4079] Loss: 1.039128875499977\n",
      "[학습 4080] Loss: 1.252609088619841\n",
      "[학습 4081] Loss: 1.0791799107467048\n",
      "[학습 4082] Loss: 1.0996202831465782\n",
      "[학습 4083] Loss: 0.9968078562143535\n",
      "[학습 4084] Loss: 1.0117819907288463\n",
      "[학습 4085] Loss: 1.0031419284066123\n",
      "[학습 4086] Loss: 1.063136336883246\n",
      "[학습 4087] Loss: 1.1756063560835621\n",
      "[학습 4088] Loss: 1.3396548020398598\n",
      "[학습 4089] Loss: 0.9959347365484299\n",
      "[학습 4090] Loss: 1.1660456822570773\n",
      "[학습 4091] Loss: 1.0533558266667276\n",
      "[학습 4092] Loss: 1.1246756237869737\n",
      "[학습 4093] Loss: 1.0152720908044766\n",
      "[학습 4094] Loss: 0.8460448749990089\n",
      "[학습 4095] Loss: 1.0590435648841237\n",
      "[학습 4096] Loss: 1.3581584709734313\n",
      "[학습 4097] Loss: 1.0237179286340656\n",
      "[학습 4098] Loss: 1.089838650680167\n",
      "[학습 4099] Loss: 1.1674338295822648\n",
      "[학습 4100] Loss: 0.9443240442610648\n",
      "[학습 4101] Loss: 1.3069153258780892\n",
      "[학습 4102] Loss: 0.836099026726173\n",
      "[학습 4103] Loss: 1.2208340505686042\n",
      "[학습 4104] Loss: 0.9512676326928715\n",
      "[학습 4105] Loss: 0.9589681608013652\n",
      "[학습 4106] Loss: 1.0404048072267595\n",
      "[학습 4107] Loss: 1.127987369549914\n",
      "[학습 4108] Loss: 1.1776886222103484\n",
      "[학습 4109] Loss: 1.0649435086862105\n",
      "[학습 4110] Loss: 1.2052167073143096\n",
      "[학습 4111] Loss: 1.1120724162563214\n",
      "[학습 4112] Loss: 1.1791134196040118\n",
      "[학습 4113] Loss: 1.0899106631878954\n",
      "[학습 4114] Loss: 1.134387098941178\n",
      "[학습 4115] Loss: 1.1443334699781966\n",
      "[학습 4116] Loss: 1.1156021062159496\n",
      "[학습 4117] Loss: 0.9595114677986445\n",
      "[학습 4118] Loss: 1.0368936260580583\n",
      "[학습 4119] Loss: 1.0209617251878422\n",
      "[학습 4120] Loss: 1.2273192455099888\n",
      "[학습 4121] Loss: 0.9773765737083144\n",
      "[학습 4122] Loss: 1.1639951416023373\n",
      "[학습 4123] Loss: 0.9878036399626364\n",
      "[학습 4124] Loss: 0.9156210194826128\n",
      "[학습 4125] Loss: 0.9773479184176143\n",
      "[학습 4126] Loss: 1.0628872189287228\n",
      "[학습 4127] Loss: 1.1435997675169072\n",
      "[학습 4128] Loss: 1.0184112430204386\n",
      "[학습 4129] Loss: 1.1436209380418745\n",
      "[학습 4130] Loss: 0.9278897732215664\n",
      "[학습 4131] Loss: 1.1474965104020753\n",
      "[학습 4132] Loss: 0.9137995530550843\n",
      "[학습 4133] Loss: 1.1087495768979307\n",
      "[학습 4134] Loss: 1.141978357335491\n",
      "[학습 4135] Loss: 1.209116428259439\n",
      "[학습 4136] Loss: 1.2043075337341598\n",
      "[학습 4137] Loss: 1.0111719071026588\n",
      "[학습 4138] Loss: 1.1143396642305192\n",
      "[학습 4139] Loss: 1.1754318590876975\n",
      "[학습 4140] Loss: 1.0364278023985463\n",
      "[학습 4141] Loss: 1.0201369823010076\n",
      "[학습 4142] Loss: 1.1026137526147373\n",
      "[학습 4143] Loss: 1.1411232516750502\n",
      "[학습 4144] Loss: 0.9759672007989899\n",
      "[학습 4145] Loss: 0.9034127849454932\n",
      "[학습 4146] Loss: 1.0501032504081096\n",
      "[학습 4147] Loss: 1.2558761540014516\n",
      "[학습 4148] Loss: 1.3432979721617686\n",
      "[학습 4149] Loss: 0.9378649114353189\n",
      "[학습 4150] Loss: 1.0678312150256246\n",
      "[학습 4151] Loss: 1.00197641803569\n",
      "[학습 4152] Loss: 1.1535346887288682\n",
      "[학습 4153] Loss: 1.0666832347884767\n",
      "[학습 4154] Loss: 0.9508371086196052\n",
      "[학습 4155] Loss: 1.0436084448992065\n",
      "[학습 4156] Loss: 1.0222626715088747\n",
      "[학습 4157] Loss: 1.2281911886020485\n",
      "[학습 4158] Loss: 1.1161574983203735\n",
      "[학습 4159] Loss: 1.2975630081824079\n",
      "[학습 4160] Loss: 1.054035380265921\n",
      "[학습 4161] Loss: 1.009112205437488\n",
      "[학습 4162] Loss: 1.0026633600681594\n",
      "[학습 4163] Loss: 1.0503262927708945\n",
      "[학습 4164] Loss: 1.0993291227766806\n",
      "[학습 4165] Loss: 1.199772969893702\n",
      "[학습 4166] Loss: 0.9929012607334451\n",
      "[학습 4167] Loss: 1.0932518434002743\n",
      "[학습 4168] Loss: 1.0823215505018515\n",
      "[학습 4169] Loss: 1.1519610115061372\n",
      "[학습 4170] Loss: 0.9925069189885918\n",
      "[학습 4171] Loss: 1.125110482186556\n",
      "[학습 4172] Loss: 1.056546974321617\n",
      "[학습 4173] Loss: 0.9572911185639791\n",
      "[학습 4174] Loss: 0.9172215061946491\n",
      "[학습 4175] Loss: 1.303124308890267\n",
      "[학습 4176] Loss: 1.3515663666908204\n",
      "[학습 4177] Loss: 1.2226815063737704\n",
      "[학습 4178] Loss: 1.2950428898804436\n",
      "[학습 4179] Loss: 1.1022316408687036\n",
      "[학습 4180] Loss: 1.3304539214270121\n",
      "[학습 4181] Loss: 1.0771290906163902\n",
      "[학습 4182] Loss: 1.1290884333752587\n",
      "[학습 4183] Loss: 1.0195187104833672\n",
      "[학습 4184] Loss: 1.3112978744011285\n",
      "[학습 4185] Loss: 1.3079243332806767\n",
      "[학습 4186] Loss: 1.0475646141625992\n",
      "[학습 4187] Loss: 0.9868155044941598\n",
      "[학습 4188] Loss: 0.9756243086771152\n",
      "[학습 4189] Loss: 1.216495867164986\n",
      "[학습 4190] Loss: 1.1941767018260312\n",
      "[학습 4191] Loss: 1.1874165683781677\n",
      "[학습 4192] Loss: 1.215533369366773\n",
      "[학습 4193] Loss: 1.3987400029074246\n",
      "[학습 4194] Loss: 1.2750849790869463\n",
      "[학습 4195] Loss: 1.1213373618816398\n",
      "[학습 4196] Loss: 1.1221249981441235\n",
      "[학습 4197] Loss: 0.9621026918857606\n",
      "[학습 4198] Loss: 1.2621666756752163\n",
      "[학습 4199] Loss: 0.9609984498623467\n",
      "[학습 4200] Loss: 1.0211998046614974\n",
      "[학습 4201] Loss: 0.9413757886181484\n",
      "[학습 4202] Loss: 1.0461784648593158\n",
      "[학습 4203] Loss: 1.0632461262199502\n",
      "[학습 4204] Loss: 0.9093397506610517\n",
      "[학습 4205] Loss: 1.2083337098756433\n",
      "[학습 4206] Loss: 1.092922214333299\n",
      "[학습 4207] Loss: 1.0718064808762087\n",
      "[학습 4208] Loss: 1.14376644337398\n",
      "[학습 4209] Loss: 0.9746035797866025\n",
      "[학습 4210] Loss: 1.04172195444495\n",
      "[학습 4211] Loss: 1.0982670591670596\n",
      "[학습 4212] Loss: 1.3098531728086613\n",
      "[학습 4213] Loss: 1.1339661902877345\n",
      "[학습 4214] Loss: 1.1922707555489884\n",
      "[학습 4215] Loss: 1.0413110727446016\n",
      "[학습 4216] Loss: 0.9645753844440391\n",
      "[학습 4217] Loss: 0.9753716017207822\n",
      "[학습 4218] Loss: 1.0249622466658437\n",
      "[학습 4219] Loss: 0.9992140357177763\n",
      "[학습 4220] Loss: 1.1444657379933945\n",
      "[학습 4221] Loss: 0.9619718809376385\n",
      "[학습 4222] Loss: 0.9753373647109109\n",
      "[학습 4223] Loss: 1.4317769780983953\n",
      "[학습 4224] Loss: 1.0506872495043629\n",
      "[학습 4225] Loss: 0.9859611196201076\n",
      "[학습 4226] Loss: 0.9836917599796148\n",
      "[학습 4227] Loss: 1.1332072348586641\n",
      "[학습 4228] Loss: 1.1118217243268795\n",
      "[학습 4229] Loss: 0.9595399327382943\n",
      "[학습 4230] Loss: 0.926606403166779\n",
      "[학습 4231] Loss: 1.1663433818829545\n",
      "[학습 4232] Loss: 0.8707870834175953\n",
      "[학습 4233] Loss: 0.8299637845275779\n",
      "[학습 4234] Loss: 1.0934226540408571\n",
      "[학습 4235] Loss: 1.1756041898785545\n",
      "[학습 4236] Loss: 1.134431597726938\n",
      "[학습 4237] Loss: 1.08076218571416\n",
      "[학습 4238] Loss: 0.8668648464336869\n",
      "[학습 4239] Loss: 1.4004258296036642\n",
      "[학습 4240] Loss: 1.116985483448369\n",
      "[학습 4241] Loss: 1.2507814970782254\n",
      "[학습 4242] Loss: 1.0794884370099844\n",
      "[학습 4243] Loss: 1.0332550178307776\n",
      "[학습 4244] Loss: 1.069872964649119\n",
      "[학습 4245] Loss: 1.2356983472700835\n",
      "[학습 4246] Loss: 1.1368864153573264\n",
      "[학습 4247] Loss: 1.035350658602555\n",
      "[학습 4248] Loss: 1.0496240632712952\n",
      "[학습 4249] Loss: 1.1367205690624527\n",
      "[학습 4250] Loss: 0.9752803108731382\n",
      "[학습 4251] Loss: 0.9732735399799026\n",
      "[학습 4252] Loss: 1.0991475139952929\n",
      "[학습 4253] Loss: 0.9811629154152187\n",
      "[학습 4254] Loss: 1.2170794136632177\n",
      "[학습 4255] Loss: 1.2311053632818478\n",
      "[학습 4256] Loss: 1.1120527018028938\n",
      "[학습 4257] Loss: 1.0254444316950437\n",
      "[학습 4258] Loss: 1.1313609780233562\n",
      "[학습 4259] Loss: 1.1795773384130754\n",
      "[학습 4260] Loss: 1.061800118899632\n",
      "[학습 4261] Loss: 1.1993971733379603\n",
      "[학습 4262] Loss: 1.187432135290591\n",
      "[학습 4263] Loss: 1.1823801233038236\n",
      "[학습 4264] Loss: 1.0299264037046827\n",
      "[학습 4265] Loss: 0.9704044675244995\n",
      "[학습 4266] Loss: 1.1210638521894174\n",
      "[학습 4267] Loss: 1.0195184787888374\n",
      "[학습 4268] Loss: 1.2269062282294556\n",
      "[학습 4269] Loss: 1.1883217672500557\n",
      "[학습 4270] Loss: 1.0297578834176562\n",
      "[학습 4271] Loss: 1.1093446259415158\n",
      "[학습 4272] Loss: 1.2315948905106209\n",
      "[학습 4273] Loss: 1.3365651221759594\n",
      "[학습 4274] Loss: 1.3041007263136157\n",
      "[학습 4275] Loss: 1.0934993040102132\n",
      "[학습 4276] Loss: 1.25567150527691\n",
      "[학습 4277] Loss: 1.0389616645012532\n",
      "[학습 4278] Loss: 1.2161231353389013\n",
      "[학습 4279] Loss: 1.017916946499239\n",
      "[학습 4280] Loss: 0.9852541829545565\n",
      "[학습 4281] Loss: 1.142415143421498\n",
      "[학습 4282] Loss: 1.0272023525707845\n",
      "[학습 4283] Loss: 1.1369914047460854\n",
      "[학습 4284] Loss: 1.2467527831530303\n",
      "[학습 4285] Loss: 0.8461317159228987\n",
      "[학습 4286] Loss: 1.1670676435896052\n",
      "[학습 4287] Loss: 1.1495389557333722\n",
      "[학습 4288] Loss: 1.046083960465426\n",
      "[학습 4289] Loss: 1.20882998251042\n",
      "[학습 4290] Loss: 1.0103845590710119\n",
      "[학습 4291] Loss: 1.0792978758320553\n",
      "[학습 4292] Loss: 1.1970062471518697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습 4293] Loss: 1.0093807016569998\n",
      "[학습 4294] Loss: 0.9751423532852825\n",
      "[학습 4295] Loss: 1.263492978919415\n",
      "[학습 4296] Loss: 0.9673554769876165\n",
      "[학습 4297] Loss: 1.098605887661229\n",
      "[학습 4298] Loss: 1.1460221081484046\n",
      "[학습 4299] Loss: 0.9071700386385652\n",
      "[학습 4300] Loss: 1.2566427131115208\n",
      "[학습 4301] Loss: 1.2404919295717725\n",
      "[학습 4302] Loss: 1.0494905349706776\n",
      "[학습 4303] Loss: 1.0795802193319914\n",
      "[학습 4304] Loss: 1.034826029114078\n",
      "[학습 4305] Loss: 0.8564796045478996\n",
      "[학습 4306] Loss: 1.0574371266104798\n",
      "[학습 4307] Loss: 1.418641635695586\n",
      "[학습 4308] Loss: 1.0989480652573778\n",
      "[학습 4309] Loss: 1.1218865779052953\n",
      "[학습 4310] Loss: 1.1581789654854895\n",
      "[학습 4311] Loss: 1.053638565241403\n",
      "[학습 4312] Loss: 1.1767935290215747\n",
      "[학습 4313] Loss: 1.2422924093441197\n",
      "[학습 4314] Loss: 1.3574887581373931\n",
      "[학습 4315] Loss: 1.290441403173305\n",
      "[학습 4316] Loss: 0.9703797579522595\n",
      "[학습 4317] Loss: 1.0665475483035807\n",
      "[학습 4318] Loss: 1.1456553873230992\n",
      "[학습 4319] Loss: 0.9524425740857191\n",
      "[학습 4320] Loss: 0.9739662812496688\n",
      "[학습 4321] Loss: 1.205523256655394\n",
      "[학습 4322] Loss: 1.1851435174719471\n",
      "[학습 4323] Loss: 0.9741146949058944\n",
      "[학습 4324] Loss: 1.01207956517081\n",
      "[학습 4325] Loss: 1.0044332730962298\n",
      "[학습 4326] Loss: 0.9533842815353393\n",
      "[학습 4327] Loss: 0.8361669023087711\n",
      "[학습 4328] Loss: 1.3064081501025766\n",
      "[학습 4329] Loss: 0.9764115674940251\n",
      "[학습 4330] Loss: 1.0620419089165816\n",
      "[학습 4331] Loss: 1.2499832557271369\n",
      "[학습 4332] Loss: 1.1631715153134492\n",
      "[학습 4333] Loss: 1.0168461308696954\n",
      "[학습 4334] Loss: 0.8695377172222984\n",
      "[학습 4335] Loss: 1.0163022110277522\n",
      "[학습 4336] Loss: 0.9823218983038572\n",
      "[학습 4337] Loss: 1.2567049104148722\n",
      "[학습 4338] Loss: 1.2771012891522295\n",
      "[학습 4339] Loss: 1.045328044918224\n",
      "[학습 4340] Loss: 1.0278316223492245\n",
      "[학습 4341] Loss: 1.084915824052965\n",
      "[학습 4342] Loss: 1.0777123195521776\n",
      "[학습 4343] Loss: 1.051315288744921\n",
      "[학습 4344] Loss: 1.057361855121374\n",
      "[학습 4345] Loss: 1.2873817442893198\n",
      "[학습 4346] Loss: 0.9026555401141022\n",
      "[학습 4347] Loss: 0.9334590459352929\n",
      "[학습 4348] Loss: 1.1013842717016298\n",
      "[학습 4349] Loss: 1.1870074050728883\n",
      "[학습 4350] Loss: 1.166754236158727\n",
      "[학습 4351] Loss: 1.164852257302693\n",
      "[학습 4352] Loss: 0.9935962877224013\n",
      "[학습 4353] Loss: 1.06557944483176\n",
      "[학습 4354] Loss: 1.2891826998007037\n",
      "[학습 4355] Loss: 1.1031134264044904\n",
      "[학습 4356] Loss: 1.0298563993407985\n",
      "[학습 4357] Loss: 1.0311341324239962\n",
      "[학습 4358] Loss: 1.0374302144672876\n",
      "[학습 4359] Loss: 0.9723477786096352\n",
      "[학습 4360] Loss: 0.9617518008466166\n",
      "[학습 4361] Loss: 1.0262541255743587\n",
      "[학습 4362] Loss: 0.972314719533539\n",
      "[학습 4363] Loss: 1.0443265966467001\n",
      "[학습 4364] Loss: 1.1279837098577892\n",
      "[학습 4365] Loss: 0.9536586330494555\n",
      "[학습 4366] Loss: 1.090121122319145\n",
      "[학습 4367] Loss: 0.9818004959024875\n",
      "[학습 4368] Loss: 1.1522928161828612\n",
      "[학습 4369] Loss: 0.960191946258885\n",
      "[학습 4370] Loss: 0.9678846603335751\n",
      "[학습 4371] Loss: 1.1625835671298008\n",
      "[학습 4372] Loss: 1.19685890280511\n",
      "[학습 4373] Loss: 1.1258556902871633\n",
      "[학습 4374] Loss: 1.0323526474322424\n",
      "[학습 4375] Loss: 0.9858216077858972\n",
      "[학습 4376] Loss: 1.306332896048081\n",
      "[학습 4377] Loss: 1.135384330457971\n",
      "[학습 4378] Loss: 1.1754133725125344\n",
      "[학습 4379] Loss: 1.2241103896224013\n",
      "[학습 4380] Loss: 0.9980732307329676\n",
      "[학습 4381] Loss: 1.0777949897215224\n",
      "[학습 4382] Loss: 0.9696037490607093\n",
      "[학습 4383] Loss: 1.0131589820553661\n",
      "[학습 4384] Loss: 0.9395250000138909\n",
      "[학습 4385] Loss: 1.2208960172651155\n",
      "[학습 4386] Loss: 1.2181250419562906\n",
      "[학습 4387] Loss: 1.1458266268683481\n",
      "[학습 4388] Loss: 1.206663626441573\n",
      "[학습 4389] Loss: 0.9380088303595735\n",
      "[학습 4390] Loss: 0.9554974135720119\n",
      "[학습 4391] Loss: 1.1444355422812726\n",
      "[학습 4392] Loss: 1.2619802529918351\n",
      "[학습 4393] Loss: 1.0015263312359963\n",
      "[학습 4394] Loss: 1.1159584649416425\n",
      "[학습 4395] Loss: 1.1256548738886263\n",
      "[학습 4396] Loss: 1.0776479941929578\n",
      "[학습 4397] Loss: 0.9719438768562536\n",
      "[학습 4398] Loss: 0.9559757779767764\n",
      "[학습 4399] Loss: 1.0948914796284632\n",
      "[학습 4400] Loss: 0.9856458218282461\n",
      "[학습 4401] Loss: 1.1297307355458586\n",
      "[학습 4402] Loss: 1.1341245897678272\n",
      "[학습 4403] Loss: 0.9566514897649486\n",
      "[학습 4404] Loss: 0.9957664911865804\n",
      "[학습 4405] Loss: 1.0989612247460239\n",
      "[학습 4406] Loss: 0.9805600842185727\n",
      "[학습 4407] Loss: 1.2803595297081136\n",
      "[학습 4408] Loss: 0.8924642517161934\n",
      "[학습 4409] Loss: 1.0939257356033372\n",
      "[학습 4410] Loss: 1.333224603647699\n",
      "[학습 4411] Loss: 0.9650063030856815\n",
      "[학습 4412] Loss: 0.9706220017645788\n",
      "[학습 4413] Loss: 1.2521163300066895\n",
      "[학습 4414] Loss: 1.012975729969134\n",
      "[학습 4415] Loss: 1.2017262087025644\n",
      "[학습 4416] Loss: 1.09791377666151\n",
      "[학습 4417] Loss: 1.1890286415175964\n",
      "[학습 4418] Loss: 0.9087782167519062\n",
      "[학습 4419] Loss: 0.8077552993525714\n",
      "[학습 4420] Loss: 0.8864569030128923\n",
      "[학습 4421] Loss: 1.1052118294011748\n",
      "[학습 4422] Loss: 1.0429006914212808\n",
      "[학습 4423] Loss: 1.2696892092677055\n",
      "[학습 4424] Loss: 1.0639585962929656\n",
      "[학습 4425] Loss: 0.9774773866883527\n",
      "[학습 4426] Loss: 1.1327487364001807\n",
      "[학습 4427] Loss: 0.9432170961933986\n",
      "[학습 4428] Loss: 0.9254172022047885\n",
      "[학습 4429] Loss: 1.0210463164339032\n",
      "[학습 4430] Loss: 0.9858438873943208\n",
      "[학습 4431] Loss: 1.0428633756427221\n",
      "[학습 4432] Loss: 1.158282844460702\n",
      "[학습 4433] Loss: 0.9837050695844165\n",
      "[학습 4434] Loss: 0.9601395908454907\n",
      "[학습 4435] Loss: 1.165363187959463\n",
      "[학습 4436] Loss: 1.087719252804331\n",
      "[학습 4437] Loss: 1.0178354158228442\n",
      "[학습 4438] Loss: 0.8173083758437323\n",
      "[학습 4439] Loss: 1.009570480898019\n",
      "[학습 4440] Loss: 1.016761453403799\n",
      "[학습 4441] Loss: 1.0047483904330439\n",
      "[학습 4442] Loss: 0.9862074717866511\n",
      "[학습 4443] Loss: 0.9702469962147027\n",
      "[학습 4444] Loss: 1.0947293800139328\n",
      "[학습 4445] Loss: 0.9852235842688042\n",
      "[학습 4446] Loss: 0.9519018445768177\n",
      "[학습 4447] Loss: 1.0033846213086592\n",
      "[학습 4448] Loss: 1.1016592638036302\n",
      "[학습 4449] Loss: 1.0529364357791002\n",
      "[학습 4450] Loss: 1.289573225456328\n",
      "[학습 4451] Loss: 0.8671404572246176\n",
      "[학습 4452] Loss: 0.9456469191815989\n",
      "[학습 4453] Loss: 1.197964826980947\n",
      "[학습 4454] Loss: 1.2381568298368921\n",
      "[학습 4455] Loss: 0.9288393857227565\n",
      "[학습 4456] Loss: 0.9720329512885714\n",
      "[학습 4457] Loss: 1.2221422963104507\n",
      "[학습 4458] Loss: 1.1600951964824295\n",
      "[학습 4459] Loss: 1.0975157874237933\n",
      "[학습 4460] Loss: 1.1622068564227142\n",
      "[학습 4461] Loss: 1.0522218891605033\n",
      "[학습 4462] Loss: 0.9498316883317721\n",
      "[학습 4463] Loss: 1.0560935303698358\n",
      "[학습 4464] Loss: 0.9868015555502219\n",
      "[학습 4465] Loss: 1.3057682358499414\n",
      "[학습 4466] Loss: 1.2936344265586228\n",
      "[학습 4467] Loss: 1.2359677969163818\n",
      "[학습 4468] Loss: 0.9955218306921283\n",
      "[학습 4469] Loss: 1.1331814113432275\n",
      "[학습 4470] Loss: 1.1014592889483579\n",
      "[학습 4471] Loss: 1.210223680696315\n",
      "[학습 4472] Loss: 1.15176869230909\n",
      "[학습 4473] Loss: 0.9407799055351704\n",
      "[학습 4474] Loss: 1.175117295767658\n",
      "[학습 4475] Loss: 1.0763851995884899\n",
      "[학습 4476] Loss: 0.9994210115077725\n",
      "[학습 4477] Loss: 0.9839550913111039\n",
      "[학습 4478] Loss: 0.8321341033451908\n",
      "[학습 4479] Loss: 1.0106833588079427\n",
      "[학습 4480] Loss: 1.1859108569199395\n",
      "[학습 4481] Loss: 0.9139844703176307\n",
      "[학습 4482] Loss: 1.0896431904294837\n",
      "[학습 4483] Loss: 0.9687781895364602\n",
      "[학습 4484] Loss: 0.9021974398384794\n",
      "[학습 4485] Loss: 1.2029548936679841\n",
      "[학습 4486] Loss: 1.0636449792406761\n",
      "[학습 4487] Loss: 1.0973951835437985\n",
      "[학습 4488] Loss: 1.1346172171441022\n",
      "[학습 4489] Loss: 1.1849147636269235\n",
      "[학습 4490] Loss: 0.9008335738623248\n",
      "[학습 4491] Loss: 0.8791828991578806\n",
      "[학습 4492] Loss: 1.0172158413937507\n",
      "[학습 4493] Loss: 1.049279645726106\n",
      "[학습 4494] Loss: 0.85973455355277\n",
      "[학습 4495] Loss: 0.9298777238484978\n",
      "[학습 4496] Loss: 0.9690925425367459\n",
      "[학습 4497] Loss: 1.0615568659582015\n",
      "[학습 4498] Loss: 0.9450520171332076\n",
      "[학습 4499] Loss: 1.1072541204115394\n",
      "[학습 4500] Loss: 1.205552456245024\n",
      "[학습 4501] Loss: 1.1173742795241468\n",
      "[학습 4502] Loss: 1.1076026498302949\n",
      "[학습 4503] Loss: 0.9690881766294293\n",
      "[학습 4504] Loss: 1.0378116563158795\n",
      "[학습 4505] Loss: 1.0478638289148463\n",
      "[학습 4506] Loss: 1.0404000530929811\n",
      "[학습 4507] Loss: 1.2626435704277394\n",
      "[학습 4508] Loss: 1.2796084088451785\n",
      "[학습 4509] Loss: 1.2105409350577268\n",
      "[학습 4510] Loss: 1.0849083704511757\n",
      "[학습 4511] Loss: 1.1445389350210002\n",
      "[학습 4512] Loss: 0.8215739417295721\n",
      "[학습 4513] Loss: 1.1941562330100106\n",
      "[학습 4514] Loss: 0.9720646841258075\n",
      "[학습 4515] Loss: 1.1248188033296578\n",
      "[학습 4516] Loss: 1.1059670780285895\n",
      "[학습 4517] Loss: 0.8981822102868918\n",
      "[학습 4518] Loss: 1.0311269260982996\n",
      "[학습 4519] Loss: 1.0243494548269976\n",
      "[학습 4520] Loss: 1.3172808439395303\n",
      "[학습 4521] Loss: 1.1253061336812473\n",
      "[학습 4522] Loss: 1.1472572503170155\n",
      "[학습 4523] Loss: 1.2060155895467348\n",
      "[학습 4524] Loss: 1.0073501048845481\n",
      "[학습 4525] Loss: 1.306029245029658\n",
      "[학습 4526] Loss: 0.9646429757501384\n",
      "[학습 4527] Loss: 1.18897665932319\n",
      "[학습 4528] Loss: 1.2383740365195055\n",
      "[학습 4529] Loss: 0.9311349247838235\n",
      "[학습 4530] Loss: 0.9513653484942582\n",
      "[학습 4531] Loss: 1.1424513468893496\n",
      "[학습 4532] Loss: 1.060110013027244\n",
      "[학습 4533] Loss: 0.9330582482751185\n",
      "[학습 4534] Loss: 0.7395943844812356\n",
      "[학습 4535] Loss: 1.0049348248915293\n",
      "[학습 4536] Loss: 0.9830295493973888\n",
      "[학습 4537] Loss: 1.1416542187608933\n",
      "[학습 4538] Loss: 0.9917765385729566\n",
      "[학습 4539] Loss: 1.1972134774922398\n",
      "[학습 4540] Loss: 1.0446613720824314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습 4541] Loss: 1.05604318523126\n",
      "[학습 4542] Loss: 0.8654555925025735\n",
      "[학습 4543] Loss: 1.110273571229021\n",
      "[학습 4544] Loss: 1.1266125303933825\n",
      "[학습 4545] Loss: 1.019763308616926\n",
      "[학습 4546] Loss: 1.1607993501866434\n",
      "[학습 4547] Loss: 0.9094957747308675\n",
      "[학습 4548] Loss: 0.9644901150090713\n",
      "[학습 4549] Loss: 1.056020846038688\n",
      "[학습 4550] Loss: 0.9815641146214885\n",
      "[학습 4551] Loss: 1.1433436606485634\n",
      "[학습 4552] Loss: 0.9510223577907531\n",
      "[학습 4553] Loss: 1.0251755325694722\n",
      "[학습 4554] Loss: 1.076495908399147\n",
      "[학습 4555] Loss: 1.2310339421263237\n",
      "[학습 4556] Loss: 1.0677610194205396\n",
      "[학습 4557] Loss: 0.9405738736970118\n",
      "[학습 4558] Loss: 1.106473449037264\n",
      "[학습 4559] Loss: 0.9241184814592954\n",
      "[학습 4560] Loss: 0.9184927061875706\n",
      "[학습 4561] Loss: 1.140603439145178\n",
      "[학습 4562] Loss: 1.0474761241532675\n",
      "[학습 4563] Loss: 1.0548312130449866\n",
      "[학습 4564] Loss: 0.9876619461476507\n",
      "[학습 4565] Loss: 1.033986123333114\n",
      "[학습 4566] Loss: 1.1570140897625318\n",
      "[학습 4567] Loss: 1.0881978940035146\n",
      "[학습 4568] Loss: 1.181692306960668\n",
      "[학습 4569] Loss: 0.9725728651892644\n",
      "[학습 4570] Loss: 1.1539091716680718\n",
      "[학습 4571] Loss: 1.2063539118799809\n",
      "[학습 4572] Loss: 0.9083266853979424\n",
      "[학습 4573] Loss: 0.970896967466946\n",
      "[학습 4574] Loss: 0.9576454842376215\n",
      "[학습 4575] Loss: 1.0282887883883844\n",
      "[학습 4576] Loss: 1.0593843904475648\n",
      "[학습 4577] Loss: 0.9744953959472502\n",
      "[학습 4578] Loss: 1.0322842763234\n",
      "[학습 4579] Loss: 1.2055518028663612\n",
      "[학습 4580] Loss: 1.1833675483448567\n",
      "[학습 4581] Loss: 1.0981118349036585\n",
      "[학습 4582] Loss: 0.8986320977680597\n",
      "[학습 4583] Loss: 1.1244817762050618\n",
      "[학습 4584] Loss: 1.0355547594562051\n",
      "[학습 4585] Loss: 1.0049989510986208\n",
      "[학습 4586] Loss: 1.1062906255840277\n",
      "[학습 4587] Loss: 1.3714657262036343\n",
      "[학습 4588] Loss: 0.8781426530342941\n",
      "[학습 4589] Loss: 1.3254151145774489\n",
      "[학습 4590] Loss: 1.0087991789352548\n",
      "[학습 4591] Loss: 1.1063023893019652\n",
      "[학습 4592] Loss: 0.9665605380994009\n",
      "[학습 4593] Loss: 1.1089654154311264\n",
      "[학습 4594] Loss: 1.1693186354598402\n",
      "[학습 4595] Loss: 1.2832188844900632\n",
      "[학습 4596] Loss: 0.9725611512647641\n",
      "[학습 4597] Loss: 0.9950337009047422\n",
      "[학습 4598] Loss: 0.9205270194305538\n",
      "[학습 4599] Loss: 0.8651740399008756\n",
      "[학습 4600] Loss: 1.0912302686570947\n",
      "[학습 4601] Loss: 1.000656527925787\n",
      "[학습 4602] Loss: 1.1103836384761985\n",
      "[학습 4603] Loss: 1.1618472429057494\n",
      "[학습 4604] Loss: 1.0896195589391269\n",
      "[학습 4605] Loss: 1.2508011455663486\n",
      "[학습 4606] Loss: 0.9599993242276057\n",
      "[학습 4607] Loss: 1.1928283438344058\n",
      "[학습 4608] Loss: 1.048887617069242\n",
      "[학습 4609] Loss: 0.9037613321147532\n",
      "[학습 4610] Loss: 1.0543210653131732\n",
      "[학습 4611] Loss: 1.071166114732685\n",
      "[학습 4612] Loss: 0.9916091888903895\n",
      "[학습 4613] Loss: 1.1190571304911177\n",
      "[학습 4614] Loss: 1.3020699775657456\n",
      "[학습 4615] Loss: 1.0712184549716957\n",
      "[학습 4616] Loss: 1.1339617216101363\n",
      "[학습 4617] Loss: 1.0045928245533111\n",
      "[학습 4618] Loss: 1.0944755297669708\n",
      "[학습 4619] Loss: 1.0720619791145831\n",
      "[학습 4620] Loss: 1.259470476507842\n",
      "[학습 4621] Loss: 1.1649440073578399\n",
      "[학습 4622] Loss: 1.1405585087755536\n",
      "[학습 4623] Loss: 0.9622489320438978\n",
      "[학습 4624] Loss: 1.1340979825201873\n",
      "[학습 4625] Loss: 0.8587288358078908\n",
      "[학습 4626] Loss: 1.1582168176761363\n",
      "[학습 4627] Loss: 1.1378351137418994\n",
      "[학습 4628] Loss: 0.975270635664915\n",
      "[학습 4629] Loss: 1.054055668785778\n",
      "[학습 4630] Loss: 1.2212515741606855\n",
      "[학습 4631] Loss: 0.9361731186364222\n",
      "[학습 4632] Loss: 0.9483763803365821\n",
      "[학습 4633] Loss: 1.0619191514843933\n",
      "[학습 4634] Loss: 1.0951648394003635\n",
      "[학습 4635] Loss: 1.1348293481847134\n",
      "[학습 4636] Loss: 1.1177470820507762\n",
      "[학습 4637] Loss: 1.1592754622354369\n",
      "[학습 4638] Loss: 0.9639742379392864\n",
      "[학습 4639] Loss: 1.023404991159198\n",
      "[학습 4640] Loss: 0.8823560836311884\n",
      "[학습 4641] Loss: 0.9812146982629508\n",
      "[학습 4642] Loss: 1.2432544531515486\n",
      "[학습 4643] Loss: 0.9908482706386341\n",
      "[학습 4644] Loss: 1.0001556715983975\n",
      "[학습 4645] Loss: 0.9617706523629969\n",
      "[학습 4646] Loss: 0.9263852902879693\n",
      "[학습 4647] Loss: 1.0142735322995526\n",
      "[학습 4648] Loss: 0.8871998022723139\n",
      "[학습 4649] Loss: 0.8790497402036742\n",
      "[학습 4650] Loss: 1.0138559200625785\n",
      "[학습 4651] Loss: 1.0759906400811112\n",
      "[학습 4652] Loss: 1.1338966495835878\n",
      "[학습 4653] Loss: 0.9862717595081932\n",
      "[학습 4654] Loss: 0.9996623673336319\n",
      "[학습 4655] Loss: 1.2186288989548681\n",
      "[학습 4656] Loss: 0.9046673210440147\n",
      "[학습 4657] Loss: 1.1325846681502145\n",
      "[학습 4658] Loss: 1.0870821682844523\n",
      "[학습 4659] Loss: 1.2625596629674563\n",
      "[학습 4660] Loss: 1.1018757307855855\n",
      "[학습 4661] Loss: 1.109457624814488\n",
      "[학습 4662] Loss: 1.1674979342693175\n",
      "[학습 4663] Loss: 1.0627688942387823\n",
      "[학습 4664] Loss: 1.3150822112662137\n",
      "[학습 4665] Loss: 1.1563648950988\n",
      "[학습 4666] Loss: 0.993015353722698\n",
      "[학습 4667] Loss: 0.9826783646727191\n",
      "[학습 4668] Loss: 0.9404217734954236\n",
      "[학습 4669] Loss: 0.9849301746300688\n",
      "[학습 4670] Loss: 1.069630671531752\n",
      "[학습 4671] Loss: 1.1742467801123768\n",
      "[학습 4672] Loss: 1.076646845110911\n",
      "[학습 4673] Loss: 1.0024633179799243\n",
      "[학습 4674] Loss: 0.8949228051916686\n",
      "[학습 4675] Loss: 0.965405999902567\n",
      "[학습 4676] Loss: 1.063210282827729\n",
      "[학습 4677] Loss: 1.100506227085504\n",
      "[학습 4678] Loss: 1.1192604982226626\n",
      "[학습 4679] Loss: 1.049125111130328\n",
      "[학습 4680] Loss: 1.0225527050346868\n",
      "[학습 4681] Loss: 0.8396195385553321\n",
      "[학습 4682] Loss: 0.9200140262843667\n",
      "[학습 4683] Loss: 1.039208685517436\n",
      "[학습 4684] Loss: 1.0256594449772232\n",
      "[학습 4685] Loss: 1.3652682993941447\n",
      "[학습 4686] Loss: 1.110329060857322\n",
      "[학습 4687] Loss: 1.2937483582990092\n",
      "[학습 4688] Loss: 0.9944495362673504\n",
      "[학습 4689] Loss: 1.083764842919763\n",
      "[학습 4690] Loss: 1.0019930168080895\n",
      "[학습 4691] Loss: 0.8873863926655199\n",
      "[학습 4692] Loss: 0.9532363330144814\n",
      "[학습 4693] Loss: 1.0180292553101709\n",
      "[학습 4694] Loss: 1.2031107116196906\n",
      "[학습 4695] Loss: 0.9088566741254597\n",
      "[학습 4696] Loss: 1.0035870726960134\n",
      "[학습 4697] Loss: 0.9547756080754641\n",
      "[학습 4698] Loss: 0.9551338691362363\n",
      "[학습 4699] Loss: 0.8940775598552898\n",
      "[학습 4700] Loss: 1.1691418543367709\n",
      "[학습 4701] Loss: 1.1700933046301751\n",
      "[학습 4702] Loss: 1.0560350324441767\n",
      "[학습 4703] Loss: 0.9649298801802761\n",
      "[학습 4704] Loss: 1.1025975771588734\n",
      "[학습 4705] Loss: 0.9845226144247305\n",
      "[학습 4706] Loss: 1.0559069229432951\n",
      "[학습 4707] Loss: 0.9659992228421806\n",
      "[학습 4708] Loss: 1.0151300958503062\n",
      "[학습 4709] Loss: 1.1143202059138488\n",
      "[학습 4710] Loss: 1.175638567278215\n",
      "[학습 4711] Loss: 0.8320819853231227\n",
      "[학습 4712] Loss: 0.9927904989837253\n",
      "[학습 4713] Loss: 0.9484826588486106\n",
      "[학습 4714] Loss: 1.1878756678167024\n",
      "[학습 4715] Loss: 0.9825032613424469\n",
      "[학습 4716] Loss: 0.8574515010017859\n",
      "[학습 4717] Loss: 0.8604456102961859\n",
      "[학습 4718] Loss: 1.1776272180940353\n",
      "[학습 4719] Loss: 1.0802694089880676\n",
      "[학습 4720] Loss: 0.9249267787398172\n",
      "[학습 4721] Loss: 1.0991728951899005\n",
      "[학습 4722] Loss: 1.2202737248759867\n",
      "[학습 4723] Loss: 0.9326733205737286\n",
      "[학습 4724] Loss: 1.0242870566954585\n",
      "[학습 4725] Loss: 1.2007888793724484\n",
      "[학습 4726] Loss: 1.1666351824481365\n",
      "[학습 4727] Loss: 1.1736912267269521\n",
      "[학습 4728] Loss: 1.05297060001842\n",
      "[학습 4729] Loss: 0.8920155619719664\n",
      "[학습 4730] Loss: 1.1428383452143016\n",
      "[학습 4731] Loss: 1.006014308716532\n",
      "[학습 4732] Loss: 1.157446381792933\n",
      "[학습 4733] Loss: 1.3202453044384306\n",
      "[학습 4734] Loss: 0.9546093080456086\n",
      "[학습 4735] Loss: 1.0521089745631043\n",
      "[학습 4736] Loss: 1.1885837649361064\n",
      "[학습 4737] Loss: 1.2235660213716335\n",
      "[학습 4738] Loss: 1.1081918361847187\n",
      "[학습 4739] Loss: 1.1227647356189125\n",
      "[학습 4740] Loss: 1.022000419350997\n",
      "[학습 4741] Loss: 0.8129312649617009\n",
      "[학습 4742] Loss: 1.0417119040582121\n",
      "[학습 4743] Loss: 1.0302443251798687\n",
      "[학습 4744] Loss: 0.9921552912511171\n",
      "[학습 4745] Loss: 0.9164002578552973\n",
      "[학습 4746] Loss: 1.0527470538144337\n",
      "[학습 4747] Loss: 0.9791457050566854\n",
      "[학습 4748] Loss: 1.180516314834328\n",
      "[학습 4749] Loss: 1.1939606925763406\n",
      "[학습 4750] Loss: 1.1159325235209854\n",
      "[학습 4751] Loss: 0.9720773552201467\n",
      "[학습 4752] Loss: 1.1004901901649222\n",
      "[학습 4753] Loss: 0.884148535281134\n",
      "[학습 4754] Loss: 1.094513103519284\n",
      "[학습 4755] Loss: 1.0271719828310015\n",
      "[학습 4756] Loss: 1.0298176159834616\n",
      "[학습 4757] Loss: 0.9925315737742809\n",
      "[학습 4758] Loss: 0.9916201235946716\n",
      "[학습 4759] Loss: 1.2692250770489195\n",
      "[학습 4760] Loss: 1.019297861357532\n",
      "[학습 4761] Loss: 0.9362928907965196\n",
      "[학습 4762] Loss: 0.9452317350336995\n",
      "[학습 4763] Loss: 1.0752735079358238\n",
      "[학습 4764] Loss: 0.9771998146341461\n",
      "[학습 4765] Loss: 0.8447342746601086\n",
      "[학습 4766] Loss: 1.219465966604358\n",
      "[학습 4767] Loss: 1.058585823685746\n",
      "[학습 4768] Loss: 1.0072382196051235\n",
      "[학습 4769] Loss: 1.168409087258067\n",
      "[학습 4770] Loss: 0.8818659609316596\n",
      "[학습 4771] Loss: 1.1926812785369683\n",
      "[학습 4772] Loss: 0.9036670511428372\n",
      "[학습 4773] Loss: 0.8457206906646562\n",
      "[학습 4774] Loss: 1.0940685650484334\n",
      "[학습 4775] Loss: 1.1883185105335243\n",
      "[학습 4776] Loss: 0.7928604640457447\n",
      "[학습 4777] Loss: 0.88452705199362\n",
      "[학습 4778] Loss: 0.9277160402829437\n",
      "[학습 4779] Loss: 1.0624418933053916\n",
      "[학습 4780] Loss: 0.9340644557237894\n",
      "[학습 4781] Loss: 1.245818552565802\n",
      "[학습 4782] Loss: 1.1415620839627527\n",
      "[학습 4783] Loss: 1.032603529943871\n",
      "[학습 4784] Loss: 1.0773002226252757\n",
      "[학습 4785] Loss: 1.0233015797146345\n",
      "[학습 4786] Loss: 0.9070231243555447\n",
      "[학습 4787] Loss: 0.9270724732887571\n",
      "[학습 4788] Loss: 1.080464805475906\n",
      "[학습 4789] Loss: 0.9951310462631825\n",
      "[학습 4790] Loss: 0.9248054884974815\n",
      "[학습 4791] Loss: 1.3153773663773471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습 4792] Loss: 1.070326067357505\n",
      "[학습 4793] Loss: 1.1827548531231884\n",
      "[학습 4794] Loss: 0.8767172106112544\n",
      "[학습 4795] Loss: 1.1542057278570288\n",
      "[학습 4796] Loss: 0.9347924246667239\n",
      "[학습 4797] Loss: 1.075203278196745\n",
      "[학습 4798] Loss: 0.9474546877423583\n",
      "[학습 4799] Loss: 1.0803655613961418\n",
      "[학습 4800] Loss: 1.1405613887022488\n",
      "[학습 4801] Loss: 0.6719771611609309\n",
      "[학습 4802] Loss: 0.8091305187553126\n",
      "[학습 4803] Loss: 1.0387156569946518\n",
      "[학습 4804] Loss: 1.1523807815668152\n",
      "[학습 4805] Loss: 1.1368235078921447\n",
      "[학습 4806] Loss: 0.9985944346930887\n",
      "[학습 4807] Loss: 0.9639668999430034\n",
      "[학습 4808] Loss: 1.036357405795208\n",
      "[학습 4809] Loss: 1.0598048377231992\n",
      "[학습 4810] Loss: 0.9895796956973971\n",
      "[학습 4811] Loss: 1.0944174576743566\n",
      "[학습 4812] Loss: 0.7701891588639037\n",
      "[학습 4813] Loss: 0.9362106847553516\n",
      "[학습 4814] Loss: 1.4028045460608416\n",
      "[학습 4815] Loss: 0.7822402571211015\n",
      "[학습 4816] Loss: 0.9884530921596254\n",
      "[학습 4817] Loss: 0.9193331859768494\n",
      "[학습 4818] Loss: 0.8737618741952861\n",
      "[학습 4819] Loss: 1.0454784596860176\n",
      "[학습 4820] Loss: 0.967427707398233\n",
      "[학습 4821] Loss: 1.1377066770521003\n",
      "[학습 4822] Loss: 1.1162462133470612\n",
      "[학습 4823] Loss: 0.9103673072466808\n",
      "[학습 4824] Loss: 0.8316646976037458\n",
      "[학습 4825] Loss: 1.0892903770169775\n",
      "[학습 4826] Loss: 0.8160395484673875\n",
      "[학습 4827] Loss: 0.9433604835181967\n",
      "[학습 4828] Loss: 1.132594904518339\n",
      "[학습 4829] Loss: 1.1333242016554623\n",
      "[학습 4830] Loss: 0.9134502682715757\n",
      "[학습 4831] Loss: 1.0384773298437353\n",
      "[학습 4832] Loss: 0.8833961878424905\n",
      "[학습 4833] Loss: 0.8920831027682109\n",
      "[학습 4834] Loss: 0.8772682757380088\n",
      "[학습 4835] Loss: 0.9914710132868464\n",
      "[학습 4836] Loss: 1.067720907284943\n",
      "[학습 4837] Loss: 1.0277511715637495\n",
      "[학습 4838] Loss: 1.0676135807473028\n",
      "[학습 4839] Loss: 1.0617795977047735\n",
      "[학습 4840] Loss: 1.0289357097685796\n",
      "[학습 4841] Loss: 0.946184384139847\n",
      "[학습 4842] Loss: 0.8892217229711852\n",
      "[학습 4843] Loss: 0.964417589360675\n",
      "[학습 4844] Loss: 0.8661719223536116\n",
      "[학습 4845] Loss: 1.0779596185672085\n",
      "[학습 4846] Loss: 1.2734791524680515\n",
      "[학습 4847] Loss: 1.026590886152909\n",
      "[학습 4848] Loss: 1.1992896183733186\n",
      "[학습 4849] Loss: 1.0014026312877933\n",
      "[학습 4850] Loss: 0.9107580951322174\n",
      "[학습 4851] Loss: 1.2188806929208584\n",
      "[학습 4852] Loss: 0.9819562209642804\n",
      "[학습 4853] Loss: 1.0990999081779191\n",
      "[학습 4854] Loss: 1.125397306305274\n",
      "[학습 4855] Loss: 1.3485541380640882\n",
      "[학습 4856] Loss: 1.097558341925319\n",
      "[학습 4857] Loss: 0.7880313187197211\n",
      "[학습 4858] Loss: 1.1072028069127746\n",
      "[학습 4859] Loss: 0.9763788337410627\n",
      "[학습 4860] Loss: 0.9111962379809452\n",
      "[학습 4861] Loss: 1.1201014883270057\n",
      "[학습 4862] Loss: 1.1169881824184673\n",
      "[학습 4863] Loss: 1.0079401072821095\n",
      "[학습 4864] Loss: 1.0209894668063617\n",
      "[학습 4865] Loss: 1.086704673786729\n",
      "[학습 4866] Loss: 0.8721679376465795\n",
      "[학습 4867] Loss: 1.151260430067985\n",
      "[학습 4868] Loss: 1.1026991539596518\n",
      "[학습 4869] Loss: 0.9950254790539901\n",
      "[학습 4870] Loss: 1.0226941547375756\n",
      "[학습 4871] Loss: 0.9026412349975537\n",
      "[학습 4872] Loss: 1.0431226365383726\n",
      "[학습 4873] Loss: 1.1591739761161235\n",
      "[학습 4874] Loss: 1.0761780745704894\n",
      "[학습 4875] Loss: 1.1096120432476957\n",
      "[학습 4876] Loss: 1.1637642441996692\n",
      "[학습 4877] Loss: 1.0607562456727666\n",
      "[학습 4878] Loss: 0.9976229515405868\n",
      "[학습 4879] Loss: 1.005981343092993\n",
      "[학습 4880] Loss: 1.0769375428991668\n",
      "[학습 4881] Loss: 1.0535018046634963\n",
      "[학습 4882] Loss: 0.9790876899078416\n",
      "[학습 4883] Loss: 1.1308747404287294\n",
      "[학습 4884] Loss: 0.9515741249729501\n",
      "[학습 4885] Loss: 0.8679957910998851\n",
      "[학습 4886] Loss: 1.234779835660428\n",
      "[학습 4887] Loss: 1.0125879509363782\n",
      "[학습 4888] Loss: 1.1636358786086982\n",
      "[학습 4889] Loss: 1.026031651219399\n",
      "[학습 4890] Loss: 0.9037032456754432\n",
      "[학습 4891] Loss: 0.8702617792682484\n",
      "[학습 4892] Loss: 1.1482518247502937\n",
      "[학습 4893] Loss: 0.9816045005623237\n",
      "[학습 4894] Loss: 1.044855987372072\n",
      "[학습 4895] Loss: 0.9807546384992836\n",
      "[학습 4896] Loss: 0.9516252988645401\n",
      "[학습 4897] Loss: 1.05013936581523\n",
      "[학습 4898] Loss: 1.0737757192877928\n",
      "[학습 4899] Loss: 0.9192844512831158\n",
      "[학습 4900] Loss: 1.0618798527622015\n",
      "[학습 4901] Loss: 0.9721763370452788\n",
      "[학습 4902] Loss: 0.8808634738453032\n",
      "[학습 4903] Loss: 1.2092847044406811\n",
      "[학습 4904] Loss: 1.0734602110741154\n",
      "[학습 4905] Loss: 0.9985954599876325\n",
      "[학습 4906] Loss: 1.1409002562210533\n",
      "[학습 4907] Loss: 1.0002115897014097\n",
      "[학습 4908] Loss: 1.0973888424633729\n",
      "[학습 4909] Loss: 1.0178436398952815\n",
      "[학습 4910] Loss: 1.0422221022971716\n",
      "[학습 4911] Loss: 0.951632219942656\n",
      "[학습 4912] Loss: 1.0408728075876612\n",
      "[학습 4913] Loss: 1.0562307189150806\n",
      "[학습 4914] Loss: 0.9865207859876555\n",
      "[학습 4915] Loss: 0.8954847175940022\n",
      "[학습 4916] Loss: 1.048392217324745\n",
      "[학습 4917] Loss: 1.071924563985531\n",
      "[학습 4918] Loss: 0.9833913674274767\n",
      "[학습 4919] Loss: 1.0006623470806877\n",
      "[학습 4920] Loss: 1.2904283793717395\n",
      "[학습 4921] Loss: 1.0557148034607884\n",
      "[학습 4922] Loss: 1.2244715178142571\n",
      "[학습 4923] Loss: 1.0033699977365325\n",
      "[학습 4924] Loss: 0.7825872868553787\n",
      "[학습 4925] Loss: 1.3203785320670796\n",
      "[학습 4926] Loss: 0.9440210741941141\n",
      "[학습 4927] Loss: 1.026958612415866\n",
      "[학습 4928] Loss: 1.1803869764411985\n",
      "[학습 4929] Loss: 0.9676133426562759\n",
      "[학습 4930] Loss: 1.116851696998931\n",
      "[학습 4931] Loss: 0.8691478484608186\n",
      "[학습 4932] Loss: 1.1164508644424977\n",
      "[학습 4933] Loss: 1.0597802602947823\n",
      "[학습 4934] Loss: 1.0598881124738357\n",
      "[학습 4935] Loss: 1.1729396461656751\n",
      "[학습 4936] Loss: 0.9635541988971749\n",
      "[학습 4937] Loss: 1.327557636069686\n",
      "[학습 4938] Loss: 0.8108167660396238\n",
      "[학습 4939] Loss: 1.043564383379019\n",
      "[학습 4940] Loss: 0.8723491982546424\n",
      "[학습 4941] Loss: 1.1304770072819137\n",
      "[학습 4942] Loss: 1.0556180093911447\n",
      "[학습 4943] Loss: 1.1041557614947453\n",
      "[학습 4944] Loss: 0.9643878269648312\n",
      "[학습 4945] Loss: 1.0364055819129516\n",
      "[학습 4946] Loss: 0.9858302497037069\n",
      "[학습 4947] Loss: 0.8374210649841026\n",
      "[학습 4948] Loss: 1.1413513491615894\n",
      "[학습 4949] Loss: 1.1205838382450577\n",
      "[학습 4950] Loss: 1.1046491871796564\n",
      "[학습 4951] Loss: 1.002290027915081\n",
      "[학습 4952] Loss: 0.9362574862776105\n",
      "[학습 4953] Loss: 0.9591316446966812\n",
      "[학습 4954] Loss: 1.0522174105621582\n",
      "[학습 4955] Loss: 0.8957918447044204\n",
      "[학습 4956] Loss: 0.9970157653639277\n",
      "[학습 4957] Loss: 0.898198273278919\n",
      "[학습 4958] Loss: 0.8656913501643553\n",
      "[학습 4959] Loss: 1.0888199300652865\n",
      "[학습 4960] Loss: 1.1222860574225368\n",
      "[학습 4961] Loss: 1.179710818007657\n",
      "[학습 4962] Loss: 1.1923723951950982\n",
      "[학습 4963] Loss: 1.0786465115013073\n",
      "[학습 4964] Loss: 0.9602693262172116\n",
      "[학습 4965] Loss: 0.9816928389994343\n",
      "[학습 4966] Loss: 1.2323792279105887\n",
      "[학습 4967] Loss: 0.9364291932661328\n",
      "[학습 4968] Loss: 1.0980998642932527\n",
      "[학습 4969] Loss: 0.8150835596331443\n",
      "[학습 4970] Loss: 0.9701542379320471\n",
      "[학습 4971] Loss: 0.974598600786033\n",
      "[학습 4972] Loss: 1.0540058574799105\n",
      "[학습 4973] Loss: 0.8424570395403415\n",
      "[학습 4974] Loss: 1.174994295186235\n",
      "[학습 4975] Loss: 1.1231597735596226\n",
      "[학습 4976] Loss: 0.8397815505933304\n",
      "[학습 4977] Loss: 0.8375214386637766\n",
      "[학습 4978] Loss: 0.9773017144453837\n",
      "[학습 4979] Loss: 0.9857409703458438\n",
      "[학습 4980] Loss: 1.0009343059572116\n",
      "[학습 4981] Loss: 0.903905214960522\n",
      "[학습 4982] Loss: 0.998744490892136\n",
      "[학습 4983] Loss: 1.1964651885263116\n",
      "[학습 4984] Loss: 0.9489232859464832\n",
      "[학습 4985] Loss: 0.9926601493261623\n",
      "[학습 4986] Loss: 1.0926332004239774\n",
      "[학습 4987] Loss: 0.8430933078083851\n",
      "[학습 4988] Loss: 1.0127897757271684\n",
      "[학습 4989] Loss: 0.9578359953418898\n",
      "[학습 4990] Loss: 1.0626034967698716\n",
      "[학습 4991] Loss: 0.8986534251508806\n",
      "[학습 4992] Loss: 1.123769913271626\n",
      "[학습 4993] Loss: 1.2477762681734794\n",
      "[학습 4994] Loss: 0.9680514019244576\n",
      "[학습 4995] Loss: 0.9466578381517298\n",
      "[학습 4996] Loss: 0.8951459471044828\n",
      "[학습 4997] Loss: 0.999785446871731\n",
      "[학습 4998] Loss: 1.160026667507154\n",
      "[학습 4999] Loss: 1.0416701175158403\n",
      "[학습 5000] Loss: 1.0047321128137208\n",
      "[학습 5001] Loss: 0.9601138006734373\n",
      "[학습 5002] Loss: 1.1300807307121068\n",
      "[학습 5003] Loss: 0.8605177090876824\n",
      "[학습 5004] Loss: 1.0786300526762749\n",
      "[학습 5005] Loss: 1.1538038227737821\n",
      "[학습 5006] Loss: 1.1519114652569922\n",
      "[학습 5007] Loss: 0.9196717866445409\n",
      "[학습 5008] Loss: 0.850237929890436\n",
      "[학습 5009] Loss: 1.1093697510524896\n",
      "[학습 5010] Loss: 1.015403443103958\n",
      "[학습 5011] Loss: 0.9896253725548598\n",
      "[학습 5012] Loss: 1.079436380081332\n",
      "[학습 5013] Loss: 0.994253609384223\n",
      "[학습 5014] Loss: 0.9256740967526347\n",
      "[학습 5015] Loss: 1.043852814218448\n",
      "[학습 5016] Loss: 0.927774825579494\n",
      "[학습 5017] Loss: 0.9396642752916384\n",
      "[학습 5018] Loss: 0.9191901824325797\n",
      "[학습 5019] Loss: 0.9237266954968305\n",
      "[학습 5020] Loss: 1.1775838467034108\n",
      "[학습 5021] Loss: 1.2097787373233944\n",
      "[학습 5022] Loss: 1.2780415169724808\n",
      "[학습 5023] Loss: 1.184387003479901\n",
      "[학습 5024] Loss: 0.9676917271490203\n",
      "[학습 5025] Loss: 0.949956141582586\n",
      "[학습 5026] Loss: 1.0341705498549882\n",
      "[학습 5027] Loss: 0.9746446729151743\n",
      "[학습 5028] Loss: 1.076049235014577\n",
      "[학습 5029] Loss: 1.0730175802326094\n",
      "[학습 5030] Loss: 1.0038467572202758\n",
      "[학습 5031] Loss: 1.0489989291767987\n",
      "[학습 5032] Loss: 1.07497773797976\n",
      "[학습 5033] Loss: 1.152417868127501\n",
      "[학습 5034] Loss: 1.0048042660465126\n",
      "[학습 5035] Loss: 1.2150153300462787\n",
      "[학습 5036] Loss: 0.9815089843915669\n",
      "[학습 5037] Loss: 1.0532527100114246\n",
      "[학습 5038] Loss: 0.929495755879022\n",
      "[학습 5039] Loss: 1.0038487907148792\n",
      "[학습 5040] Loss: 1.0829517535564548\n",
      "[학습 5041] Loss: 1.0891088312422472\n",
      "[학습 5042] Loss: 1.0850044708232343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습 5043] Loss: 0.9621205822932911\n",
      "[학습 5044] Loss: 1.0974263648989402\n",
      "[학습 5045] Loss: 1.0194698204129538\n",
      "[학습 5046] Loss: 1.2702994160444752\n",
      "[학습 5047] Loss: 1.198722649905554\n",
      "[학습 5048] Loss: 1.078777036420429\n",
      "[학습 5049] Loss: 1.0331679125285598\n",
      "[학습 5050] Loss: 0.9677013820191306\n",
      "[학습 5051] Loss: 1.0860376595654295\n",
      "[학습 5052] Loss: 0.9449792594496188\n",
      "[학습 5053] Loss: 0.9860274670141042\n",
      "[학습 5054] Loss: 1.0700388895029946\n",
      "[학습 5055] Loss: 1.0377840155071547\n",
      "[학습 5056] Loss: 1.1793670439030182\n",
      "[학습 5057] Loss: 0.9172893671073971\n",
      "[학습 5058] Loss: 0.8532936168025779\n",
      "[학습 5059] Loss: 0.8854633641606463\n",
      "[학습 5060] Loss: 0.9121859337199651\n",
      "[학습 5061] Loss: 1.0318489224656857\n",
      "[학습 5062] Loss: 1.0141470577781455\n",
      "[학습 5063] Loss: 1.0439027391837032\n",
      "[학습 5064] Loss: 0.974283482067024\n",
      "[학습 5065] Loss: 0.84128528453277\n",
      "[학습 5066] Loss: 1.114149777404733\n",
      "[학습 5067] Loss: 1.254391561923021\n",
      "[학습 5068] Loss: 1.0495786502805846\n",
      "[학습 5069] Loss: 1.0997228346541077\n",
      "[학습 5070] Loss: 1.0907281727064129\n",
      "[학습 5071] Loss: 0.9779861625604133\n",
      "[학습 5072] Loss: 1.0474114996270436\n",
      "[학습 5073] Loss: 0.9308364987516131\n",
      "[학습 5074] Loss: 0.9808486436686522\n",
      "[학습 5075] Loss: 0.8511394079239832\n",
      "[학습 5076] Loss: 1.0501122728693553\n",
      "[학습 5077] Loss: 1.1171697383628996\n",
      "[학습 5078] Loss: 1.0837992665102612\n",
      "[학습 5079] Loss: 1.0632458789667985\n",
      "[학습 5080] Loss: 1.0852742299189455\n",
      "[학습 5081] Loss: 1.2207336821521861\n",
      "[학습 5082] Loss: 1.0294261920025414\n",
      "[학습 5083] Loss: 1.1977120385992246\n",
      "[학습 5084] Loss: 0.9605990332813028\n",
      "[학습 5085] Loss: 1.074908386526488\n",
      "[학습 5086] Loss: 0.8602367108922542\n",
      "[학습 5087] Loss: 0.9911699789558662\n",
      "[학습 5088] Loss: 1.0354513327796888\n",
      "[학습 5089] Loss: 0.9476235854181959\n",
      "[학습 5090] Loss: 1.010184836859765\n",
      "[학습 5091] Loss: 0.8805087802666881\n",
      "[학습 5092] Loss: 1.1916386691833498\n",
      "[학습 5093] Loss: 1.0190610644352436\n",
      "[학습 5094] Loss: 1.1823276855437783\n",
      "[학습 5095] Loss: 1.0516057747934047\n",
      "[학습 5096] Loss: 0.8977435857325545\n",
      "[학습 5097] Loss: 1.1503982968308784\n",
      "[학습 5098] Loss: 1.1309522418360034\n",
      "[학습 5099] Loss: 1.1749113380377592\n",
      "[학습 5100] Loss: 1.148059337638029\n",
      "[학습 5101] Loss: 1.0323006806505395\n",
      "[학습 5102] Loss: 0.9871034027628113\n",
      "[학습 5103] Loss: 0.9755659588894678\n",
      "[학습 5104] Loss: 1.1321387533523704\n",
      "[학습 5105] Loss: 1.329315318199879\n",
      "[학습 5106] Loss: 1.2741290391776188\n",
      "[학습 5107] Loss: 1.1018326043252176\n",
      "[학습 5108] Loss: 1.1083820085637144\n",
      "[학습 5109] Loss: 1.1614731418013142\n",
      "[학습 5110] Loss: 0.9210362263853359\n",
      "[학습 5111] Loss: 1.007804520278908\n",
      "[학습 5112] Loss: 1.0820438777958616\n",
      "[학습 5113] Loss: 1.1414196099608578\n",
      "[학습 5114] Loss: 0.9708450664252503\n",
      "[학습 5115] Loss: 0.9112886735951667\n",
      "[학습 5116] Loss: 1.019900053809621\n",
      "[학습 5117] Loss: 0.8592053575323123\n",
      "[학습 5118] Loss: 0.9656436449285806\n",
      "[학습 5119] Loss: 1.1272691330740279\n",
      "[학습 5120] Loss: 0.909309470452747\n",
      "[학습 5121] Loss: 1.2145071536789067\n",
      "[학습 5122] Loss: 1.0703062605340026\n",
      "[학습 5123] Loss: 1.0469917082748537\n",
      "[학습 5124] Loss: 1.0078958151089337\n",
      "[학습 5125] Loss: 0.8862377131636263\n",
      "[학습 5126] Loss: 0.9002795361048692\n",
      "[학습 5127] Loss: 1.034642084556517\n",
      "[학습 5128] Loss: 0.7417509941935297\n",
      "[학습 5129] Loss: 1.1422203814406289\n",
      "[학습 5130] Loss: 1.0488266744382364\n",
      "[학습 5131] Loss: 0.9139333198944039\n",
      "[학습 5132] Loss: 1.13473963412909\n",
      "[학습 5133] Loss: 0.9832665790365891\n",
      "[학습 5134] Loss: 1.0423339243869276\n",
      "[학습 5135] Loss: 1.0566300274692935\n",
      "[학습 5136] Loss: 1.0260431119083209\n",
      "[학습 5137] Loss: 0.9515251130268848\n",
      "[학습 5138] Loss: 0.9965241786376068\n",
      "[학습 5139] Loss: 0.9325461218813712\n",
      "[학습 5140] Loss: 0.9518263669552763\n",
      "[학습 5141] Loss: 1.273105602140285\n",
      "[학습 5142] Loss: 0.8096408261214705\n",
      "[학습 5143] Loss: 0.9525096012845764\n",
      "[학습 5144] Loss: 0.9103414197380154\n",
      "[학습 5145] Loss: 1.1621916421756193\n",
      "[학습 5146] Loss: 0.7973343869902476\n",
      "[학습 5147] Loss: 0.9464156457087133\n",
      "[학습 5148] Loss: 0.8925172090326456\n",
      "[학습 5149] Loss: 0.6596787706793102\n",
      "[학습 5150] Loss: 1.0967646260756772\n",
      "[학습 5151] Loss: 0.9595859690146458\n",
      "[학습 5152] Loss: 0.9072748370000958\n",
      "[학습 5153] Loss: 0.951583161849737\n",
      "[학습 5154] Loss: 1.0700736020158348\n",
      "[학습 5155] Loss: 1.1992158070387797\n",
      "[학습 5156] Loss: 1.0546465804440621\n",
      "[학습 5157] Loss: 0.8936545162807663\n",
      "[학습 5158] Loss: 1.1467302790790137\n",
      "[학습 5159] Loss: 1.0312864327867803\n",
      "[학습 5160] Loss: 1.0266264396426956\n",
      "[학습 5161] Loss: 1.0494645580112874\n",
      "[학습 5162] Loss: 0.995626411071251\n",
      "[학습 5163] Loss: 0.9375553563420386\n",
      "[학습 5164] Loss: 0.9755529876042047\n",
      "[학습 5165] Loss: 0.9522618107231898\n",
      "[학습 5166] Loss: 0.9909781796735123\n",
      "[학습 5167] Loss: 1.1499494391170508\n",
      "[학습 5168] Loss: 0.9456740235180431\n",
      "[학습 5169] Loss: 1.146228811339774\n",
      "[학습 5170] Loss: 0.969220114073075\n",
      "[학습 5171] Loss: 0.8749726225844328\n",
      "[학습 5172] Loss: 0.958297918053378\n",
      "[학습 5173] Loss: 0.760276406502265\n",
      "[학습 5174] Loss: 0.9981524586527624\n",
      "[학습 5175] Loss: 0.9266091027466028\n",
      "[학습 5176] Loss: 0.8437244033674329\n",
      "[학습 5177] Loss: 1.055726160638138\n",
      "[학습 5178] Loss: 0.975480280904724\n",
      "[학습 5179] Loss: 0.9026314426957538\n",
      "[학습 5180] Loss: 1.1545355720706645\n",
      "[학습 5181] Loss: 0.9882724149408777\n",
      "[학습 5182] Loss: 0.964460679334756\n",
      "[학습 5183] Loss: 0.9366449327826867\n",
      "[학습 5184] Loss: 0.920843945327816\n",
      "[학습 5185] Loss: 1.2824473059656805\n",
      "[학습 5186] Loss: 1.2575616991191167\n",
      "[학습 5187] Loss: 1.031043779979849\n",
      "[학습 5188] Loss: 1.0959649654559918\n",
      "[학습 5189] Loss: 0.9846947199687068\n",
      "[학습 5190] Loss: 1.0433668718728695\n",
      "[학습 5191] Loss: 0.9641368622688552\n",
      "[학습 5192] Loss: 1.1276954861563082\n",
      "[학습 5193] Loss: 0.9920181397833175\n",
      "[학습 5194] Loss: 1.041120491818254\n",
      "[학습 5195] Loss: 0.8224683636440869\n",
      "[학습 5196] Loss: 0.9958174172772454\n",
      "[학습 5197] Loss: 1.036369474010144\n",
      "[학습 5198] Loss: 1.2696883211377983\n",
      "[학습 5199] Loss: 1.1136563851225763\n",
      "[학습 5200] Loss: 1.190369193122773\n",
      "[학습 5201] Loss: 0.8930320244046734\n",
      "[학습 5202] Loss: 1.0642810916133734\n",
      "[학습 5203] Loss: 0.9089200976126639\n",
      "[학습 5204] Loss: 1.1792944800984317\n",
      "[학습 5205] Loss: 0.7466380288938046\n",
      "[학습 5206] Loss: 1.1266437157007956\n",
      "[학습 5207] Loss: 0.8316746232108677\n",
      "[학습 5208] Loss: 1.0675440306843629\n",
      "[학습 5209] Loss: 1.2425313703899252\n",
      "[학습 5210] Loss: 1.0032541975869755\n",
      "[학습 5211] Loss: 0.944242898023733\n",
      "[학습 5212] Loss: 1.1422494497735312\n",
      "[학습 5213] Loss: 0.8201888591381552\n",
      "[학습 5214] Loss: 1.031719119777378\n",
      "[학습 5215] Loss: 0.9375113158713703\n",
      "[학습 5216] Loss: 0.9062880115068029\n",
      "[학습 5217] Loss: 0.8342317981707095\n",
      "[학습 5218] Loss: 0.9565005126972178\n",
      "[학습 5219] Loss: 1.0209190488910727\n",
      "[학습 5220] Loss: 1.2716128814189644\n",
      "[학습 5221] Loss: 0.7954607079114655\n",
      "[학습 5222] Loss: 1.0803499600346762\n",
      "[학습 5223] Loss: 1.1000232160150711\n",
      "[학습 5224] Loss: 1.3560537505952497\n",
      "[학습 5225] Loss: 0.8764473760675887\n",
      "[학습 5226] Loss: 0.9814720539622479\n",
      "[학습 5227] Loss: 0.8662999091515655\n",
      "[학습 5228] Loss: 1.0846961081320756\n",
      "[학습 5229] Loss: 0.9702969404654973\n",
      "[학습 5230] Loss: 1.0576952795270018\n",
      "[학습 5231] Loss: 1.1942551918311493\n",
      "[학습 5232] Loss: 1.1562320261757086\n",
      "[학습 5233] Loss: 0.9893372147395412\n",
      "[학습 5234] Loss: 1.0843191162571828\n",
      "[학습 5235] Loss: 0.9512833253588446\n",
      "[학습 5236] Loss: 1.0557632656359588\n",
      "[학습 5237] Loss: 1.0877696170526883\n",
      "[학습 5238] Loss: 0.8982267486209704\n",
      "[학습 5239] Loss: 0.8258288972789828\n",
      "[학습 5240] Loss: 0.8430359320478769\n",
      "[학습 5241] Loss: 1.0896504045623336\n",
      "[학습 5242] Loss: 1.252370045413061\n",
      "[학습 5243] Loss: 0.909073667993107\n",
      "[학습 5244] Loss: 0.9861258293853852\n",
      "[학습 5245] Loss: 0.9484508641339454\n",
      "[학습 5246] Loss: 0.9477781878675151\n",
      "[학습 5247] Loss: 1.2844943319062219\n",
      "[학습 5248] Loss: 1.317830175458779\n",
      "[학습 5249] Loss: 0.883082102187887\n",
      "[학습 5250] Loss: 0.9918642024792974\n",
      "[학습 5251] Loss: 0.9664535253304433\n",
      "[학습 5252] Loss: 1.1191355012773527\n",
      "[학습 5253] Loss: 0.8525642702604628\n",
      "[학습 5254] Loss: 0.7722245425290306\n",
      "[학습 5255] Loss: 1.2283054520714025\n",
      "[학습 5256] Loss: 1.0894152361581226\n",
      "[학습 5257] Loss: 0.9954725984668772\n",
      "[학습 5258] Loss: 1.1109529403556107\n",
      "[학습 5259] Loss: 1.0339884158431403\n",
      "[학습 5260] Loss: 1.0295995343900217\n",
      "[학습 5261] Loss: 0.9800781388244385\n",
      "[학습 5262] Loss: 0.8841608983620628\n",
      "[학습 5263] Loss: 1.1285428404017563\n",
      "[학습 5264] Loss: 0.8517668409110296\n",
      "[학습 5265] Loss: 1.0512647354436209\n",
      "[학습 5266] Loss: 1.0569610785796864\n",
      "[학습 5267] Loss: 0.9869801615608905\n",
      "[학습 5268] Loss: 1.0372155592158037\n",
      "[학습 5269] Loss: 0.8138808649126676\n",
      "[학습 5270] Loss: 1.0094267882080143\n",
      "[학습 5271] Loss: 1.1063453169038022\n",
      "[학습 5272] Loss: 1.1376321037396475\n",
      "[학습 5273] Loss: 1.064495066438199\n",
      "[학습 5274] Loss: 0.9485324809023703\n",
      "[학습 5275] Loss: 1.0789564814279928\n",
      "[학습 5276] Loss: 0.8482481600144063\n",
      "[학습 5277] Loss: 1.0529345798975576\n",
      "[학습 5278] Loss: 0.8573643556706346\n",
      "[학습 5279] Loss: 1.137175537024067\n",
      "[학습 5280] Loss: 0.9406297305507306\n",
      "[학습 5281] Loss: 0.8622297537925182\n",
      "[학습 5282] Loss: 1.004246162978432\n",
      "[학습 5283] Loss: 1.0454405944338359\n",
      "[학습 5284] Loss: 0.9327692815590268\n",
      "[학습 5285] Loss: 0.9704499787487599\n",
      "[학습 5286] Loss: 0.8907507110848275\n",
      "[학습 5287] Loss: 0.9906787260449107\n",
      "[학습 5288] Loss: 0.9710556069421017\n",
      "[학습 5289] Loss: 1.0200543105820916\n",
      "[학습 5290] Loss: 0.8638054523738466\n",
      "[학습 5291] Loss: 0.8392258815639084\n",
      "[학습 5292] Loss: 0.8647343684736708\n",
      "[학습 5293] Loss: 1.0263390968347146\n",
      "[학습 5294] Loss: 1.0665241241606969\n",
      "[학습 5295] Loss: 0.9693717979438236\n",
      "[학습 5296] Loss: 1.1317281000574655\n",
      "[학습 5297] Loss: 1.103703727450593\n",
      "[학습 5298] Loss: 1.2299192300337516\n",
      "[학습 5299] Loss: 1.0158273261329904\n",
      "[학습 5300] Loss: 1.0347400138482754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습 5301] Loss: 0.8811353885568102\n",
      "[학습 5302] Loss: 1.1670249083944426\n",
      "[학습 5303] Loss: 1.1631549017488334\n",
      "[학습 5304] Loss: 0.9470734919270575\n",
      "[학습 5305] Loss: 0.9606887061122835\n",
      "[학습 5306] Loss: 0.8805168187909302\n",
      "[학습 5307] Loss: 1.0174554856945357\n",
      "[학습 5308] Loss: 1.116924863800121\n",
      "[학습 5309] Loss: 1.1957913309079098\n",
      "[학습 5310] Loss: 0.8839390838251557\n",
      "[학습 5311] Loss: 0.9116855031650912\n",
      "[학습 5312] Loss: 0.897679037484293\n",
      "[학습 5313] Loss: 0.7720233878487335\n",
      "[학습 5314] Loss: 1.0803780080784142\n",
      "[학습 5315] Loss: 1.0165880790722495\n",
      "[학습 5316] Loss: 0.8585033937532761\n",
      "[학습 5317] Loss: 0.9762081899792966\n",
      "[학습 5318] Loss: 1.0392209799204928\n",
      "[학습 5319] Loss: 0.7334023951051338\n",
      "[학습 5320] Loss: 0.895249805693594\n",
      "[학습 5321] Loss: 0.9934849575090945\n",
      "[학습 5322] Loss: 1.0362690751147592\n",
      "[학습 5323] Loss: 1.0449263474428412\n",
      "[학습 5324] Loss: 1.073342286100294\n",
      "[학습 5325] Loss: 1.0803202640301652\n",
      "[학습 5326] Loss: 0.8440084770546423\n",
      "[학습 5327] Loss: 0.9178541619233226\n",
      "[학습 5328] Loss: 1.0171428219516725\n",
      "[학습 5329] Loss: 0.9383086536405565\n",
      "[학습 5330] Loss: 0.8896057340260569\n",
      "[학습 5331] Loss: 0.918328200617472\n",
      "[학습 5332] Loss: 1.09400134597213\n",
      "[학습 5333] Loss: 0.9725989279957468\n",
      "[학습 5334] Loss: 1.0547468956384203\n",
      "[학습 5335] Loss: 1.1229893797946284\n",
      "[학습 5336] Loss: 1.0945733860835973\n",
      "[학습 5337] Loss: 1.1490493645672608\n",
      "[학습 5338] Loss: 1.053288046501246\n",
      "[학습 5339] Loss: 0.9095291034942623\n",
      "[학습 5340] Loss: 1.0094075555954047\n",
      "[학습 5341] Loss: 1.0448490253676597\n",
      "[학습 5342] Loss: 0.9097461469068994\n",
      "[학습 5343] Loss: 0.9656058443791324\n",
      "[학습 5344] Loss: 1.0956611474531697\n",
      "[학습 5345] Loss: 1.080904510098256\n",
      "[학습 5346] Loss: 0.8679247620316812\n",
      "[학습 5347] Loss: 0.7371631633082976\n",
      "[학습 5348] Loss: 0.877281885615098\n",
      "[학습 5349] Loss: 1.0568053196962228\n",
      "[학습 5350] Loss: 1.045537197118185\n",
      "[학습 5351] Loss: 1.0905112710629266\n",
      "[학습 5352] Loss: 0.9336772847293139\n",
      "[학습 5353] Loss: 0.7982541223810731\n",
      "[학습 5354] Loss: 0.9313233066748242\n",
      "[학습 5355] Loss: 1.1200202174072202\n",
      "[학습 5356] Loss: 0.999990746219568\n",
      "[학습 5357] Loss: 1.0387876022927391\n",
      "[학습 5358] Loss: 0.8006755078306084\n",
      "[학습 5359] Loss: 0.8516265039145359\n",
      "[학습 5360] Loss: 0.8076047623510921\n",
      "[학습 5361] Loss: 1.1524753638239247\n",
      "[학습 5362] Loss: 1.1548698028569806\n",
      "[학습 5363] Loss: 0.7630372240364273\n",
      "[학습 5364] Loss: 1.0350918129413824\n",
      "[학습 5365] Loss: 0.985346126995517\n",
      "[학습 5366] Loss: 1.040257063959943\n",
      "[학습 5367] Loss: 1.008289630466215\n",
      "[학습 5368] Loss: 1.052424715026836\n",
      "[학습 5369] Loss: 0.7985762663247665\n",
      "[학습 5370] Loss: 0.8238409788691321\n",
      "[학습 5371] Loss: 0.822384076227616\n",
      "[학습 5372] Loss: 0.8993043874344971\n",
      "[학습 5373] Loss: 1.1563125125975349\n",
      "[학습 5374] Loss: 0.8420050602105215\n",
      "[학습 5375] Loss: 0.8949503496928102\n",
      "[학습 5376] Loss: 0.8798530115794795\n",
      "[학습 5377] Loss: 1.1713772114412158\n",
      "[학습 5378] Loss: 1.1686468923732138\n",
      "[학습 5379] Loss: 0.8095941309867883\n",
      "[학습 5380] Loss: 0.889356815515213\n",
      "[학습 5381] Loss: 1.2276153962008198\n",
      "[학습 5382] Loss: 0.9378022070806016\n",
      "[학습 5383] Loss: 1.0051566845903996\n",
      "[학습 5384] Loss: 0.7674915299581508\n",
      "[학습 5385] Loss: 1.0462598418022795\n",
      "[학습 5386] Loss: 1.0348177287267712\n",
      "[학습 5387] Loss: 1.1855514824389393\n",
      "[학습 5388] Loss: 1.102221317897474\n",
      "[학습 5389] Loss: 0.9246581840007587\n",
      "[학습 5390] Loss: 0.9912325569571729\n",
      "[학습 5391] Loss: 1.1573571680286483\n",
      "[학습 5392] Loss: 0.9743573454986789\n",
      "[학습 5393] Loss: 1.0855274601469806\n",
      "[학습 5394] Loss: 0.9383965948996316\n",
      "[학습 5395] Loss: 1.0029768134243406\n",
      "[학습 5396] Loss: 0.9594109557917022\n",
      "[학습 5397] Loss: 0.9543681815414957\n",
      "[학습 5398] Loss: 0.9960371120042174\n",
      "[학습 5399] Loss: 1.2008572741763464\n",
      "[학습 5400] Loss: 0.9279092160780225\n",
      "[학습 5401] Loss: 0.8228035266144478\n",
      "[학습 5402] Loss: 1.0234219738444525\n",
      "[학습 5403] Loss: 0.869450366207346\n",
      "[학습 5404] Loss: 0.9440281399130638\n",
      "[학습 5405] Loss: 0.7964819053300998\n",
      "[학습 5406] Loss: 0.9478237734902933\n",
      "[학습 5407] Loss: 1.0206156321183744\n",
      "[학습 5408] Loss: 1.2020979280495645\n",
      "[학습 5409] Loss: 0.9120215103686214\n",
      "[학습 5410] Loss: 0.7578780547353046\n",
      "[학습 5411] Loss: 0.8492650436781212\n",
      "[학습 5412] Loss: 0.8355396223735948\n",
      "[학습 5413] Loss: 0.919058036189163\n",
      "[학습 5414] Loss: 0.9736734778220236\n",
      "[학습 5415] Loss: 1.0471094647307668\n",
      "[학습 5416] Loss: 1.1072132586731829\n",
      "[학습 5417] Loss: 0.8506604005263569\n",
      "[학습 5418] Loss: 1.0362005561319523\n",
      "[학습 5419] Loss: 0.9236352977771919\n",
      "[학습 5420] Loss: 0.8878157493372311\n",
      "[학습 5421] Loss: 0.8020921419937566\n",
      "[학습 5422] Loss: 1.0109366020260437\n",
      "[학습 5423] Loss: 1.0307071385631752\n",
      "[학습 5424] Loss: 1.2414849580276934\n",
      "[학습 5425] Loss: 1.0243104918603154\n",
      "[학습 5426] Loss: 0.9046521952664969\n",
      "[학습 5427] Loss: 0.967481271900628\n",
      "[학습 5428] Loss: 1.0596503931240615\n",
      "[학습 5429] Loss: 0.96706961026391\n",
      "[학습 5430] Loss: 1.1455868035019907\n",
      "[학습 5431] Loss: 1.0305746742682476\n",
      "[학습 5432] Loss: 0.9387220381858509\n",
      "[학습 5433] Loss: 1.0490512584109992\n",
      "[학습 5434] Loss: 0.8358600781489725\n",
      "[학습 5435] Loss: 1.0298613982553266\n",
      "[학습 5436] Loss: 1.0811527047429443\n",
      "[학습 5437] Loss: 1.1484069277341646\n",
      "[학습 5438] Loss: 0.9762667350546204\n",
      "[학습 5439] Loss: 1.1228903974010656\n",
      "[학습 5440] Loss: 1.021025658487689\n",
      "[학습 5441] Loss: 1.0271852713326441\n",
      "[학습 5442] Loss: 1.0497846724333284\n",
      "[학습 5443] Loss: 1.1011167132934028\n",
      "[학습 5444] Loss: 0.875097355660437\n",
      "[학습 5445] Loss: 0.8639723983133897\n",
      "[학습 5446] Loss: 0.8751219196020983\n",
      "[학습 5447] Loss: 1.213603939374028\n",
      "[학습 5448] Loss: 0.9643752018748966\n",
      "[학습 5449] Loss: 1.0400339999212287\n",
      "[학습 5450] Loss: 0.9521782910168446\n",
      "[학습 5451] Loss: 1.0400867360473651\n",
      "[학습 5452] Loss: 0.9931871166460314\n",
      "[학습 5453] Loss: 0.9288758195496459\n",
      "[학습 5454] Loss: 1.0183646065054406\n",
      "[학습 5455] Loss: 1.271083555101295\n",
      "[학습 5456] Loss: 0.8247511548891994\n",
      "[학습 5457] Loss: 0.9917031001221099\n",
      "[학습 5458] Loss: 1.2486180333839345\n",
      "[학습 5459] Loss: 0.9438989343714189\n",
      "[학습 5460] Loss: 0.9765100750176058\n",
      "[학습 5461] Loss: 1.1845539705301333\n",
      "[학습 5462] Loss: 0.958180683004009\n",
      "[학습 5463] Loss: 0.7672363026930049\n",
      "[학습 5464] Loss: 0.8244188040719307\n",
      "[학습 5465] Loss: 1.1056135141461683\n",
      "[학습 5466] Loss: 0.9435253379995259\n",
      "[학습 5467] Loss: 1.113527889026696\n",
      "[학습 5468] Loss: 1.1977000275742504\n",
      "[학습 5469] Loss: 0.9523347362387335\n",
      "[학습 5470] Loss: 1.1518950099617589\n",
      "[학습 5471] Loss: 1.1105257303279643\n",
      "[학습 5472] Loss: 0.9930254729961114\n",
      "[학습 5473] Loss: 1.1833255422742555\n",
      "[학습 5474] Loss: 1.1509160674154035\n",
      "[학습 5475] Loss: 0.8784116808826664\n",
      "[학습 5476] Loss: 1.0016666146590767\n",
      "[학습 5477] Loss: 0.9617749291362486\n",
      "[학습 5478] Loss: 0.8019211094897701\n",
      "[학습 5479] Loss: 1.064268935190565\n",
      "[학습 5480] Loss: 1.012024193198089\n",
      "[학습 5481] Loss: 0.7590227356923762\n",
      "[학습 5482] Loss: 0.9809034078917659\n",
      "[학습 5483] Loss: 0.8944548908994157\n",
      "[학습 5484] Loss: 1.0261246084963762\n",
      "[학습 5485] Loss: 1.0952308652920903\n",
      "[학습 5486] Loss: 0.9006446774886946\n",
      "[학습 5487] Loss: 0.8718971333721709\n",
      "[학습 5488] Loss: 0.9598009732961319\n",
      "[학습 5489] Loss: 0.9307699718646905\n",
      "[학습 5490] Loss: 1.2184573604474094\n",
      "[학습 5491] Loss: 1.1753937034685986\n",
      "[학습 5492] Loss: 0.9007969552168126\n",
      "[학습 5493] Loss: 0.971644918464518\n",
      "[학습 5494] Loss: 1.0796760639721408\n",
      "[학습 5495] Loss: 0.9935576021779002\n",
      "[학습 5496] Loss: 0.890451725309217\n",
      "[학습 5497] Loss: 1.0151017482682771\n",
      "[학습 5498] Loss: 1.1303316254042397\n",
      "[학습 5499] Loss: 0.9053476729582883\n",
      "[학습 5500] Loss: 0.8001842649632274\n",
      "[학습 5501] Loss: 1.0240157309668703\n",
      "[학습 5502] Loss: 0.9191133709293297\n",
      "[학습 5503] Loss: 0.9405225656052738\n",
      "[학습 5504] Loss: 0.8327259401863027\n",
      "[학습 5505] Loss: 0.9372897809922154\n",
      "[학습 5506] Loss: 0.7814493856455337\n",
      "[학습 5507] Loss: 0.898170579564901\n",
      "[학습 5508] Loss: 0.9731705759408565\n",
      "[학습 5509] Loss: 0.8715535241349568\n",
      "[학습 5510] Loss: 0.8980575394097723\n",
      "[학습 5511] Loss: 1.005025617037546\n",
      "[학습 5512] Loss: 0.6161996058376452\n",
      "[학습 5513] Loss: 0.803551454060068\n",
      "[학습 5514] Loss: 0.8961566917913877\n",
      "[학습 5515] Loss: 0.8950133738446922\n",
      "[학습 5516] Loss: 1.0710824184788381\n",
      "[학습 5517] Loss: 0.8158798109330904\n",
      "[학습 5518] Loss: 1.1182278307359501\n",
      "[학습 5519] Loss: 0.7798517824797648\n",
      "[학습 5520] Loss: 0.8579035493293738\n",
      "[학습 5521] Loss: 1.1464268644815723\n",
      "[학습 5522] Loss: 0.8623193636576105\n",
      "[학습 5523] Loss: 1.0457521361352202\n",
      "[학습 5524] Loss: 0.9598318726056391\n",
      "[학습 5525] Loss: 0.865508150794635\n",
      "[학습 5526] Loss: 0.8916679446174376\n",
      "[학습 5527] Loss: 1.2014912623015048\n",
      "[학습 5528] Loss: 0.9980071724127052\n",
      "[학습 5529] Loss: 0.8518156054544369\n",
      "[학습 5530] Loss: 0.9631483340012008\n",
      "[학습 5531] Loss: 0.8007082486195296\n",
      "[학습 5532] Loss: 0.9321617975623768\n",
      "[학습 5533] Loss: 1.015358714225945\n",
      "[학습 5534] Loss: 1.157251930848069\n",
      "[학습 5535] Loss: 0.8885601246892101\n",
      "[학습 5536] Loss: 1.0516242946361662\n",
      "[학습 5537] Loss: 0.8076249893687916\n",
      "[학습 5538] Loss: 1.1000798559150566\n",
      "[학습 5539] Loss: 0.9632598092531923\n",
      "[학습 5540] Loss: 0.8578776969991054\n",
      "[학습 5541] Loss: 1.1755646116451413\n",
      "[학습 5542] Loss: 0.862437160694602\n",
      "[학습 5543] Loss: 1.0609683386912283\n",
      "[학습 5544] Loss: 0.9529194595305077\n",
      "[학습 5545] Loss: 0.844146106749701\n",
      "[학습 5546] Loss: 1.1504742038435678\n",
      "[학습 5547] Loss: 1.0145625599324568\n",
      "[학습 5548] Loss: 0.9109583456395857\n",
      "[학습 5549] Loss: 0.991346341607543\n",
      "[학습 5550] Loss: 1.2292659314908239\n",
      "[학습 5551] Loss: 0.8669684370037052\n",
      "[학습 5552] Loss: 0.9990831906709442\n",
      "[학습 5553] Loss: 0.9966098238145149\n",
      "[학습 5554] Loss: 0.9317404766932939\n",
      "[학습 5555] Loss: 0.9388564053016282\n",
      "[학습 5556] Loss: 1.0155322324597456\n",
      "[학습 5557] Loss: 0.9951373632343123\n",
      "[학습 5558] Loss: 0.8536667529828219\n",
      "[학습 5559] Loss: 0.8882869155434565\n",
      "[학습 5560] Loss: 0.912616829541065\n",
      "[학습 5561] Loss: 0.7891241392600261\n",
      "[학습 5562] Loss: 0.8762945228712985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습 5563] Loss: 0.8205666529070218\n",
      "[학습 5564] Loss: 1.1946209879937262\n",
      "[학습 5565] Loss: 1.1717874917031073\n",
      "[학습 5566] Loss: 0.976799295256489\n",
      "[학습 5567] Loss: 0.9070451759832088\n",
      "[학습 5568] Loss: 1.037380368353971\n",
      "[학습 5569] Loss: 0.9681794399224182\n",
      "[학습 5570] Loss: 0.8099464642858679\n",
      "[학습 5571] Loss: 1.0786314338093832\n",
      "[학습 5572] Loss: 1.0920391065656474\n",
      "[학습 5573] Loss: 0.9768961435081599\n",
      "[학습 5574] Loss: 0.9411749224241988\n",
      "[학습 5575] Loss: 1.0611998760493915\n",
      "[학습 5576] Loss: 0.8322742919106707\n",
      "[학습 5577] Loss: 0.8112697834890858\n",
      "[학습 5578] Loss: 1.066066614030349\n",
      "[학습 5579] Loss: 1.152706092598263\n",
      "[학습 5580] Loss: 1.1010178926232563\n",
      "[학습 5581] Loss: 0.8643161433358675\n",
      "[학습 5582] Loss: 0.9762380432810843\n",
      "[학습 5583] Loss: 1.095561848586518\n",
      "[학습 5584] Loss: 0.838110526947168\n",
      "[학습 5585] Loss: 0.9073047222055847\n",
      "[학습 5586] Loss: 0.9214606569984415\n",
      "[학습 5587] Loss: 0.9224776979218364\n",
      "[학습 5588] Loss: 1.0066678338267234\n",
      "[학습 5589] Loss: 0.7225943974603674\n",
      "[학습 5590] Loss: 0.9424495477381953\n",
      "[학습 5591] Loss: 1.1263366380689512\n",
      "[학습 5592] Loss: 1.0304102740380185\n",
      "[학습 5593] Loss: 0.9940588111638522\n",
      "[학습 5594] Loss: 1.0105370319086717\n",
      "[학습 5595] Loss: 0.9368175241289981\n",
      "[학습 5596] Loss: 0.7699532604427081\n",
      "[학습 5597] Loss: 1.0453681731466171\n",
      "[학습 5598] Loss: 0.885324928259531\n",
      "[학습 5599] Loss: 1.008422428291556\n",
      "[학습 5600] Loss: 0.9638512094920867\n",
      "[학습 5601] Loss: 1.203393869604438\n",
      "[학습 5602] Loss: 0.9065411084448048\n",
      "[학습 5603] Loss: 1.199310661818104\n",
      "[학습 5604] Loss: 0.9072008724990337\n",
      "[학습 5605] Loss: 1.036789481848809\n",
      "[학습 5606] Loss: 0.8840319610407852\n",
      "[학습 5607] Loss: 1.1886065645090138\n",
      "[학습 5608] Loss: 0.8972831084881352\n",
      "[학습 5609] Loss: 0.9193461571822437\n",
      "[학습 5610] Loss: 1.0269887900931138\n",
      "[학습 5611] Loss: 0.8963825022615282\n",
      "[학습 5612] Loss: 0.9486583930223875\n",
      "[학습 5613] Loss: 1.1230437798353554\n",
      "[학습 5614] Loss: 1.1260238411638286\n",
      "[학습 5615] Loss: 1.0581991395498074\n",
      "[학습 5616] Loss: 1.0078651141837238\n",
      "[학습 5617] Loss: 0.9982685809577094\n",
      "[학습 5618] Loss: 0.9155470958843753\n",
      "[학습 5619] Loss: 1.2441007386003862\n",
      "[학습 5620] Loss: 1.0006898517585503\n",
      "[학습 5621] Loss: 0.9110260029894929\n",
      "[학습 5622] Loss: 0.9030528476119124\n",
      "[학습 5623] Loss: 0.9509448411949138\n",
      "[학습 5624] Loss: 1.1094835822909828\n",
      "[학습 5625] Loss: 1.0965277256681805\n",
      "[학습 5626] Loss: 0.7989086508524654\n",
      "[학습 5627] Loss: 1.029293362791348\n",
      "[학습 5628] Loss: 0.9689273241470531\n",
      "[학습 5629] Loss: 0.8687760546153875\n",
      "[학습 5630] Loss: 0.8664722276357847\n",
      "[학습 5631] Loss: 1.0132288227472444\n",
      "[학습 5632] Loss: 0.8595761972763887\n",
      "[학습 5633] Loss: 0.9732590061280568\n",
      "[학습 5634] Loss: 1.134748536437775\n",
      "[학습 5635] Loss: 1.030011597341725\n",
      "[학습 5636] Loss: 0.9182412190469356\n",
      "[학습 5637] Loss: 1.2451481655329066\n",
      "[학습 5638] Loss: 1.038502632110459\n",
      "[학습 5639] Loss: 0.8029546386250962\n",
      "[학습 5640] Loss: 1.0881646352289351\n",
      "[학습 5641] Loss: 0.8429068439447704\n",
      "[학습 5642] Loss: 1.0680232316499014\n",
      "[학습 5643] Loss: 1.2373753448504252\n",
      "[학습 5644] Loss: 0.9584585182053948\n",
      "[학습 5645] Loss: 0.981092208846951\n",
      "[학습 5646] Loss: 0.9944648014438584\n",
      "[학습 5647] Loss: 1.0691491159614062\n",
      "[학습 5648] Loss: 1.1700095418229564\n",
      "[학습 5649] Loss: 1.011090630729\n",
      "[학습 5650] Loss: 0.9487336412902323\n",
      "[학습 5651] Loss: 1.0532353203786864\n",
      "[학습 5652] Loss: 1.2421774763205125\n",
      "[학습 5653] Loss: 1.1049354531782956\n",
      "[학습 5654] Loss: 0.8494390562252547\n",
      "[학습 5655] Loss: 0.8157886300347744\n",
      "[학습 5656] Loss: 0.8314364632260788\n",
      "[학습 5657] Loss: 1.0279423958344336\n",
      "[학습 5658] Loss: 0.9728423615608578\n",
      "[학습 5659] Loss: 0.9335087966878797\n",
      "[학습 5660] Loss: 1.014260204976206\n",
      "[학습 5661] Loss: 0.898765551397523\n",
      "[학습 5662] Loss: 1.1660538704473777\n",
      "[학습 5663] Loss: 0.8098378790492253\n",
      "[학습 5664] Loss: 1.0739865895666583\n",
      "[학습 5665] Loss: 0.9252423303297087\n",
      "[학습 5666] Loss: 1.0308625430532166\n",
      "[학습 5667] Loss: 1.0905243080564884\n",
      "[학습 5668] Loss: 0.9933284240821866\n",
      "[학습 5669] Loss: 0.8738114946510503\n",
      "[학습 5670] Loss: 1.1740187028260163\n",
      "[학습 5671] Loss: 0.7760671651561699\n",
      "[학습 5672] Loss: 0.9523499415207448\n",
      "[학습 5673] Loss: 0.8055937496418528\n",
      "[학습 5674] Loss: 1.1264330937219953\n",
      "[학습 5675] Loss: 0.9676148379117845\n",
      "[학습 5676] Loss: 0.8153895549348633\n",
      "[학습 5677] Loss: 0.9549068182674971\n",
      "[학습 5678] Loss: 0.9040177195176807\n",
      "[학습 5679] Loss: 0.9022912046775553\n",
      "[학습 5680] Loss: 1.1359327005711402\n",
      "[학습 5681] Loss: 1.1819682714087463\n",
      "[학습 5682] Loss: 1.0148389498453103\n",
      "[학습 5683] Loss: 1.091804162965165\n",
      "[학습 5684] Loss: 0.9821233369476391\n",
      "[학습 5685] Loss: 1.2979705163964845\n",
      "[학습 5686] Loss: 1.0884304101822506\n",
      "[학습 5687] Loss: 1.0062412503175657\n",
      "[학습 5688] Loss: 1.247061260394707\n",
      "[학습 5689] Loss: 0.9615396273114165\n",
      "[학습 5690] Loss: 0.7681903555955211\n",
      "[학습 5691] Loss: 0.9558390119498733\n",
      "[학습 5692] Loss: 1.0557561485618456\n",
      "[학습 5693] Loss: 1.0469078449834703\n",
      "[학습 5694] Loss: 1.0427562691131336\n",
      "[학습 5695] Loss: 1.0000849945163512\n",
      "[학습 5696] Loss: 1.0430987285329087\n",
      "[학습 5697] Loss: 1.0881118281339188\n",
      "[학습 5698] Loss: 1.1012110447710213\n",
      "[학습 5699] Loss: 0.8690878948858475\n",
      "[학습 5700] Loss: 0.95599917121573\n",
      "[학습 5701] Loss: 0.9422099167976208\n",
      "[학습 5702] Loss: 0.9664072999663423\n",
      "[학습 5703] Loss: 0.9322470373403425\n",
      "[학습 5704] Loss: 0.899157630715809\n",
      "[학습 5705] Loss: 0.9301534129991944\n",
      "[학습 5706] Loss: 1.0155224507685836\n",
      "[학습 5707] Loss: 0.8884126983796928\n",
      "[학습 5708] Loss: 1.1832097669195312\n",
      "[학습 5709] Loss: 0.8706961053962285\n",
      "[학습 5710] Loss: 0.7284238445414681\n",
      "[학습 5711] Loss: 0.8801726033648822\n",
      "[학습 5712] Loss: 1.0139630423531547\n",
      "[학습 5713] Loss: 1.0130409008640757\n",
      "[학습 5714] Loss: 0.9299722751774001\n",
      "[학습 5715] Loss: 0.9701831653526611\n",
      "[학습 5716] Loss: 0.9198507856721518\n",
      "[학습 5717] Loss: 1.0353130086962898\n",
      "[학습 5718] Loss: 1.1143623393604427\n",
      "[학습 5719] Loss: 1.1311337420710392\n",
      "[학습 5720] Loss: 0.8289660851014824\n",
      "[학습 5721] Loss: 0.8332064436800201\n",
      "[학습 5722] Loss: 1.032403362652451\n",
      "[학습 5723] Loss: 0.9813382773218643\n",
      "[학습 5724] Loss: 1.0498564244628728\n",
      "[학습 5725] Loss: 0.8552828121573407\n",
      "[학습 5726] Loss: 0.8939783375514924\n",
      "[학습 5727] Loss: 0.8441471829370896\n",
      "[학습 5728] Loss: 0.9228231929582735\n",
      "[학습 5729] Loss: 0.8991327316751173\n",
      "[학습 5730] Loss: 1.1043456078751273\n",
      "[학습 5731] Loss: 0.8605123088938644\n",
      "[학습 5732] Loss: 1.0832892229269289\n",
      "[학습 5733] Loss: 1.1390717786690292\n",
      "[학습 5734] Loss: 0.9108795528686935\n",
      "[학습 5735] Loss: 0.8939091941223269\n",
      "[학습 5736] Loss: 0.918112319080332\n",
      "[학습 5737] Loss: 0.7910542723120322\n",
      "[학습 5738] Loss: 0.8657235571469529\n",
      "[학습 5739] Loss: 0.9662148713809604\n",
      "[학습 5740] Loss: 0.8527702288155785\n",
      "[학습 5741] Loss: 1.0705670698477505\n",
      "[학습 5742] Loss: 0.9758953739238571\n",
      "[학습 5743] Loss: 1.0575804543511824\n",
      "[학습 5744] Loss: 1.0014237259034937\n",
      "[학습 5745] Loss: 1.1096046943413733\n",
      "[학습 5746] Loss: 0.9115411229436549\n",
      "[학습 5747] Loss: 0.9975429256339384\n",
      "[학습 5748] Loss: 0.7924328457415423\n",
      "[학습 5749] Loss: 1.1251203602074222\n",
      "[학습 5750] Loss: 1.1086192139907107\n",
      "[학습 5751] Loss: 0.8620297163042274\n",
      "[학습 5752] Loss: 0.9581358256245869\n",
      "[학습 5753] Loss: 0.8918695543720616\n",
      "[학습 5754] Loss: 0.9002142159510487\n",
      "[학습 5755] Loss: 0.881951908482971\n",
      "[학습 5756] Loss: 1.182449699957904\n",
      "[학습 5757] Loss: 1.1596349151867031\n",
      "[학습 5758] Loss: 1.0939821930073068\n",
      "[학습 5759] Loss: 0.8728293928608494\n",
      "[학습 5760] Loss: 0.8582729231115759\n",
      "[학습 5761] Loss: 1.2389921595068953\n",
      "[학습 5762] Loss: 0.8956860796447068\n",
      "[학습 5763] Loss: 1.1106484830631085\n",
      "[학습 5764] Loss: 1.1653459466922151\n",
      "[학습 5765] Loss: 0.749467932161836\n",
      "[학습 5766] Loss: 1.0025073083468772\n",
      "[학습 5767] Loss: 1.0519676596072225\n",
      "[학습 5768] Loss: 1.0023261028553674\n",
      "[학습 5769] Loss: 0.8242369226349987\n",
      "[학습 5770] Loss: 0.9788356749188282\n",
      "[학습 5771] Loss: 0.9187899817517394\n",
      "[학습 5772] Loss: 0.9853022561456709\n",
      "[학습 5773] Loss: 1.0265998639437393\n",
      "[학습 5774] Loss: 0.9260378016408098\n",
      "[학습 5775] Loss: 0.8120985363856983\n",
      "[학습 5776] Loss: 0.950739616174536\n",
      "[학습 5777] Loss: 1.1536113826799559\n",
      "[학습 5778] Loss: 0.9960427750610427\n",
      "[학습 5779] Loss: 1.0907164455880785\n",
      "[학습 5780] Loss: 0.9200054557324242\n",
      "[학습 5781] Loss: 1.0038315428282951\n",
      "[학습 5782] Loss: 0.867165487534137\n",
      "[학습 5783] Loss: 0.9745252782562048\n",
      "[학습 5784] Loss: 0.9189093947177728\n",
      "[학습 5785] Loss: 1.2392159570981434\n",
      "[학습 5786] Loss: 1.0663507308737867\n",
      "[학습 5787] Loss: 0.8630045968980976\n",
      "[학습 5788] Loss: 0.9087175989556218\n",
      "[학습 5789] Loss: 0.9564365522338741\n",
      "[학습 5790] Loss: 1.050957636372917\n",
      "[학습 5791] Loss: 0.8654019653513967\n",
      "[학습 5792] Loss: 0.8371162573350778\n",
      "[학습 5793] Loss: 0.8460966871692897\n",
      "[학습 5794] Loss: 0.9383471993642541\n",
      "[학습 5795] Loss: 0.9963200544571607\n",
      "[학습 5796] Loss: 0.946713122095173\n",
      "[학습 5797] Loss: 0.8489662616705063\n",
      "[학습 5798] Loss: 1.0771962886346467\n",
      "[학습 5799] Loss: 0.87158311917815\n",
      "[학습 5800] Loss: 0.9868289606747759\n",
      "[학습 5801] Loss: 1.0127113474713412\n",
      "[학습 5802] Loss: 0.84548782115764\n",
      "[학습 5803] Loss: 0.9163650679083676\n",
      "[학습 5804] Loss: 1.0331661757571822\n",
      "[학습 5805] Loss: 0.8536676809946604\n",
      "[학습 5806] Loss: 1.052325363144256\n",
      "[학습 5807] Loss: 0.8652542583998519\n",
      "[학습 5808] Loss: 0.9609405586915563\n",
      "[학습 5809] Loss: 0.9135263776620463\n",
      "[학습 5810] Loss: 0.9663641423905532\n",
      "[학습 5811] Loss: 0.9540908930432914\n",
      "[학습 5812] Loss: 0.8542787340984646\n",
      "[학습 5813] Loss: 1.034605674439268\n",
      "[학습 5814] Loss: 1.020914014034551\n",
      "[학습 5815] Loss: 0.8998671304580245\n",
      "[학습 5816] Loss: 0.7149902723558402\n",
      "[학습 5817] Loss: 1.0421628619779946\n",
      "[학습 5818] Loss: 1.045764916379347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[학습 5819] Loss: 0.828036934251823\n",
      "[학습 5820] Loss: 0.8865078984883251\n",
      "[학습 5821] Loss: 0.917489483531079\n",
      "[학습 5822] Loss: 0.88753802954792\n",
      "[학습 5823] Loss: 1.0790821870855416\n",
      "[학습 5824] Loss: 0.9169673577678488\n",
      "[학습 5825] Loss: 1.0832604701131663\n",
      "[학습 5826] Loss: 0.9178776959068671\n",
      "[학습 5827] Loss: 0.9767402812151207\n",
      "[학습 5828] Loss: 0.8945632956514493\n",
      "[학습 5829] Loss: 1.065591911560949\n",
      "[학습 5830] Loss: 0.9568556216815758\n",
      "[학습 5831] Loss: 1.1513641085975868\n",
      "[학습 5832] Loss: 1.0117154913971091\n",
      "[학습 5833] Loss: 0.8666757388449987\n",
      "[학습 5834] Loss: 0.9623526976960712\n",
      "[학습 5835] Loss: 1.029890526815838\n",
      "[학습 5836] Loss: 0.7130466930044137\n",
      "[학습 5837] Loss: 0.9420830596739717\n",
      "[학습 5838] Loss: 0.7665021749110065\n",
      "[학습 5839] Loss: 0.8146253105981256\n",
      "[학습 5840] Loss: 1.096362476117087\n",
      "[학습 5841] Loss: 0.7563315665199987\n",
      "[학습 5842] Loss: 0.8097730513833536\n",
      "[학습 5843] Loss: 0.6792037159053191\n",
      "[학습 5844] Loss: 0.9668673059593362\n",
      "[학습 5845] Loss: 0.9213709688789058\n",
      "[학습 5846] Loss: 0.9274857126796653\n",
      "[학습 5847] Loss: 0.9523914716709715\n",
      "[학습 5848] Loss: 0.987787039410505\n",
      "[학습 5849] Loss: 0.7852713544336859\n",
      "[학습 5850] Loss: 1.0425109679228068\n",
      "[학습 5851] Loss: 0.993451586977686\n",
      "[학습 5852] Loss: 0.9954558144752691\n",
      "[학습 5853] Loss: 0.9044083480151665\n",
      "[학습 5854] Loss: 0.9360121184435479\n",
      "[학습 5855] Loss: 0.9261719418305704\n",
      "[학습 5856] Loss: 0.7805192032045403\n",
      "[학습 5857] Loss: 0.8934919920653742\n",
      "[학습 5858] Loss: 1.0947760931112769\n",
      "[학습 5859] Loss: 0.9206430771236711\n",
      "[학습 5860] Loss: 1.0014644241137811\n",
      "[학습 5861] Loss: 0.9588987612731745\n",
      "[학습 5862] Loss: 0.8507386800321441\n",
      "[학습 5863] Loss: 0.8298981314192397\n",
      "[학습 5864] Loss: 1.0928793209974423\n",
      "[학습 5865] Loss: 0.9800524872384017\n",
      "[학습 5866] Loss: 0.7165331258390368\n",
      "[학습 5867] Loss: 1.073818878912909\n",
      "[학습 5868] Loss: 0.9001119347894527\n",
      "[학습 5869] Loss: 0.9820007195909346\n",
      "[학습 5870] Loss: 0.8904138320107535\n",
      "[학습 5871] Loss: 0.9044113229748649\n",
      "[학습 5872] Loss: 1.2190670439510403\n",
      "[학습 5873] Loss: 1.1588224543223318\n",
      "[학습 5874] Loss: 0.9028045078954432\n",
      "[학습 5875] Loss: 0.7888459333106081\n",
      "[학습 5876] Loss: 1.122948104256775\n",
      "[학습 5877] Loss: 0.8031710638525823\n",
      "[학습 5878] Loss: 1.0475375111691867\n",
      "[학습 5879] Loss: 1.2883250011696104\n",
      "[학습 5880] Loss: 1.035016222272846\n",
      "[학습 5881] Loss: 0.9100973058903093\n",
      "[학습 5882] Loss: 1.1018892388860233\n",
      "[학습 5883] Loss: 0.9244384810466475\n",
      "[학습 5884] Loss: 0.9821158061248053\n",
      "[학습 5885] Loss: 1.0598499545368762\n",
      "[학습 5886] Loss: 0.8373157471443042\n",
      "[학습 5887] Loss: 0.8413964618804032\n",
      "[학습 5888] Loss: 1.0605463804123005\n",
      "[학습 5889] Loss: 1.1443976150373267\n",
      "[학습 5890] Loss: 0.8797113327551915\n",
      "[학습 5891] Loss: 0.9811051826607027\n",
      "[학습 5892] Loss: 0.9126633409747636\n",
      "[학습 5893] Loss: 1.108061057762689\n",
      "[학습 5894] Loss: 1.2326252584119066\n",
      "[학습 5895] Loss: 0.9298112748547689\n",
      "[학습 5896] Loss: 0.8805508886887821\n",
      "[학습 5897] Loss: 1.0851664959622453\n",
      "[학습 5898] Loss: 0.9974946143336963\n",
      "[학습 5899] Loss: 0.8527433822863215\n",
      "[학습 5900] Loss: 1.0121124311288554\n",
      "[학습 5901] Loss: 1.19433705299169\n",
      "[학습 5902] Loss: 0.9469465118665326\n",
      "[학습 5903] Loss: 0.9370341579039855\n",
      "[학습 5904] Loss: 1.0640174679058507\n",
      "[학습 5905] Loss: 1.0321290846588125\n",
      "[학습 5906] Loss: 0.9681380747995257\n",
      "[학습 5907] Loss: 0.9095826935804613\n",
      "[학습 5908] Loss: 0.9265607770532679\n",
      "[학습 5909] Loss: 0.9632818873691664\n",
      "[학습 5910] Loss: 0.9031366015552896\n",
      "[학습 5911] Loss: 0.8759703942556354\n",
      "[학습 5912] Loss: 0.851831432055011\n",
      "[학습 5913] Loss: 0.9887826895405716\n",
      "[학습 5914] Loss: 0.7692503263947182\n",
      "[학습 5915] Loss: 0.8284917584072076\n",
      "[학습 5916] Loss: 1.1649025260636368\n",
      "[학습 5917] Loss: 0.7338523855943181\n",
      "[학습 5918] Loss: 0.9759899187831779\n",
      "[학습 5919] Loss: 1.1280036006389\n",
      "[학습 5920] Loss: 0.9792479776119758\n",
      "[학습 5921] Loss: 0.8145247807818674\n",
      "[학습 5922] Loss: 0.9708460646588946\n",
      "[학습 5923] Loss: 0.9718097032198978\n",
      "[학습 5924] Loss: 0.9456015578422889\n",
      "[학습 5925] Loss: 1.0123053796173864\n",
      "[학습 5926] Loss: 0.8433602102545801\n",
      "[학습 5927] Loss: 1.2205982029941742\n",
      "[학습 5928] Loss: 0.8737062201072004\n",
      "[학습 5929] Loss: 0.9782923693774939\n",
      "[학습 5930] Loss: 0.8566215901146689\n",
      "[학습 5931] Loss: 0.9871373653331992\n",
      "[학습 5932] Loss: 0.8845775896705067\n",
      "[학습 5933] Loss: 0.9172950799439118\n",
      "[학습 5934] Loss: 0.9959544878709558\n",
      "[학습 5935] Loss: 0.8451534115760346\n",
      "[학습 5936] Loss: 0.9516750224619662\n",
      "[학습 5937] Loss: 0.7976698635805982\n",
      "[학습 5938] Loss: 1.0594628331024694\n",
      "[학습 5939] Loss: 0.9042430751799236\n",
      "[학습 5940] Loss: 1.1196016133094582\n",
      "[학습 5941] Loss: 0.92046819285748\n",
      "[학습 5942] Loss: 0.7479840112499747\n",
      "[학습 5943] Loss: 0.9548557865509991\n",
      "[학습 5944] Loss: 0.8512837213353487\n",
      "[학습 5945] Loss: 1.0028119228515338\n",
      "[학습 5946] Loss: 0.9511697717163995\n",
      "[학습 5947] Loss: 0.9140848357860167\n",
      "[학습 5948] Loss: 0.9760978424884382\n",
      "[학습 5949] Loss: 1.078371654597499\n",
      "[학습 5950] Loss: 0.7135619617996085\n",
      "[학습 5951] Loss: 1.0440952763838518\n",
      "[학습 5952] Loss: 0.8182473467172164\n",
      "[학습 5953] Loss: 1.0450953824765348\n",
      "[학습 5954] Loss: 1.040012754302741\n",
      "[학습 5955] Loss: 0.8717175323649416\n",
      "[학습 5956] Loss: 0.8617053263581778\n",
      "[학습 5957] Loss: 1.0694003569555892\n",
      "[학습 5958] Loss: 0.9349631588152687\n",
      "[학습 5959] Loss: 0.9031185737329003\n",
      "[학습 5960] Loss: 1.0122248046696511\n",
      "[학습 5961] Loss: 0.9203083407201532\n",
      "[학습 5962] Loss: 0.9613191349853701\n",
      "[학습 5963] Loss: 0.8789135981166555\n",
      "[학습 5964] Loss: 0.7676056038685833\n",
      "[학습 5965] Loss: 1.1044124555630261\n",
      "[학습 5966] Loss: 1.0106629081080434\n",
      "[학습 5967] Loss: 0.9547632119090673\n",
      "[학습 5968] Loss: 1.0710307627993965\n",
      "[학습 5969] Loss: 1.2263844639807708\n",
      "[학습 5970] Loss: 0.8220601300943372\n",
      "[학습 5971] Loss: 0.9609895548180838\n",
      "[학습 5972] Loss: 0.9380894134700571\n",
      "[학습 5973] Loss: 0.967970367634623\n",
      "[학습 5974] Loss: 1.0180167844995571\n",
      "[학습 5975] Loss: 0.9144286998966134\n",
      "[학습 5976] Loss: 0.9439401444754303\n",
      "[학습 5977] Loss: 0.9405168389210725\n",
      "[학습 5978] Loss: 0.9895554000404999\n",
      "[학습 5979] Loss: 1.01888394051557\n",
      "[학습 5980] Loss: 1.1196935428704986\n",
      "[학습 5981] Loss: 0.9411262206007709\n",
      "[학습 5982] Loss: 1.009411402345724\n",
      "[학습 5983] Loss: 1.0430515036459187\n",
      "[학습 5984] Loss: 0.93770354268973\n",
      "[학습 5985] Loss: 1.0747042355165037\n",
      "[학습 5986] Loss: 1.0567593912058704\n",
      "[학습 5987] Loss: 1.0184122023578401\n",
      "[학습 5988] Loss: 0.9057950693576431\n",
      "[학습 5989] Loss: 0.827578418030516\n",
      "[학습 5990] Loss: 0.9927375668108306\n",
      "[학습 5991] Loss: 0.8797004811472315\n",
      "[학습 5992] Loss: 0.765169194784844\n",
      "[학습 5993] Loss: 1.147292775733149\n",
      "[학습 5994] Loss: 1.008160069977349\n",
      "[학습 5995] Loss: 0.8533159061833637\n",
      "[학습 5996] Loss: 0.9179079478295472\n",
      "[학습 5997] Loss: 0.9778537197212145\n",
      "[학습 5998] Loss: 0.7559057163645738\n",
      "[학습 5999] Loss: 1.048448261243167\n",
      "[학습 6000] Loss: 0.8374250892681949\n"
     ]
    }
   ],
   "source": [
    "loss_history = model.fit(X_train, Y_train, \n",
    "                         배치크기=100, 학습횟수=600*10,\n",
    "                         학습률=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xac50c88>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa5b4518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "y_test = np.argmax(Y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69220000000000004"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝 라이브러리\n",
    "\n",
    "tensorflow + keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(50, input_shape=(784,), activation='sigmoid'))\n",
    "model.add(Dense(100, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.losses import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=categorical_crossentropy, optimizer='sgd', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 45,360\n",
      "Trainable params: 45,360\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 2.3025 - acc: 0.1330 - val_loss: 2.2742 - val_acc: 0.1382\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 2.2594 - acc: 0.2048 - val_loss: 2.2434 - val_acc: 0.2838\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 2.2237 - acc: 0.3177 - val_loss: 2.2010 - val_acc: 0.3686\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 2.1735 - acc: 0.3987 - val_loss: 2.1409 - val_acc: 0.4662\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 2.1005 - acc: 0.4701 - val_loss: 2.0528 - val_acc: 0.5069\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 1.9976 - acc: 0.5127 - val_loss: 1.9338 - val_acc: 0.5868\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 1.8651 - acc: 0.5563 - val_loss: 1.7875 - val_acc: 0.5683\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 1.7133 - acc: 0.5887 - val_loss: 1.6288 - val_acc: 0.6312\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 1.5559 - acc: 0.6311 - val_loss: 1.4689 - val_acc: 0.6770\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 1.4055 - acc: 0.6676 - val_loss: 1.3219 - val_acc: 0.7046\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 1.2695 - acc: 0.6974 - val_loss: 1.1907 - val_acc: 0.7210\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 1.1499 - acc: 0.7256 - val_loss: 1.0756 - val_acc: 0.7507\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 1.0458 - acc: 0.7495 - val_loss: 0.9761 - val_acc: 0.7738\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.9558 - acc: 0.7698 - val_loss: 0.8908 - val_acc: 0.7928\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.8792 - acc: 0.7872 - val_loss: 0.8186 - val_acc: 0.8091\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.8141 - acc: 0.8010 - val_loss: 0.7580 - val_acc: 0.8235\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.7593 - acc: 0.8135 - val_loss: 0.7072 - val_acc: 0.8339\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.7129 - acc: 0.8222 - val_loss: 0.6641 - val_acc: 0.8422\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.6736 - acc: 0.8316 - val_loss: 0.6278 - val_acc: 0.8480\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.6401 - acc: 0.8377 - val_loss: 0.5970 - val_acc: 0.8535\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.6112 - acc: 0.8439 - val_loss: 0.5705 - val_acc: 0.8569\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.5860 - acc: 0.8490 - val_loss: 0.5476 - val_acc: 0.8616\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.5640 - acc: 0.8537 - val_loss: 0.5276 - val_acc: 0.8642\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.5446 - acc: 0.8584 - val_loss: 0.5095 - val_acc: 0.8684\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.5273 - acc: 0.8616 - val_loss: 0.4937 - val_acc: 0.8731\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.5118 - acc: 0.8651 - val_loss: 0.4794 - val_acc: 0.8757\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.4977 - acc: 0.8685 - val_loss: 0.4665 - val_acc: 0.8793\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.4849 - acc: 0.8713 - val_loss: 0.4549 - val_acc: 0.8816\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.4734 - acc: 0.8736 - val_loss: 0.4443 - val_acc: 0.8833\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.4627 - acc: 0.8771 - val_loss: 0.4351 - val_acc: 0.8864\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.4531 - acc: 0.8790 - val_loss: 0.4259 - val_acc: 0.8869\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.4441 - acc: 0.8809 - val_loss: 0.4177 - val_acc: 0.8898\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.4358 - acc: 0.8825 - val_loss: 0.4103 - val_acc: 0.8907\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.4282 - acc: 0.8849 - val_loss: 0.4037 - val_acc: 0.8925\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 0.4210 - acc: 0.8865 - val_loss: 0.3969 - val_acc: 0.8923\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.4144 - acc: 0.8878 - val_loss: 0.3909 - val_acc: 0.8939\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.4082 - acc: 0.8888 - val_loss: 0.3853 - val_acc: 0.8947\n",
      "Epoch 38/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.4024 - acc: 0.8899 - val_loss: 0.3800 - val_acc: 0.8968\n",
      "Epoch 39/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.3970 - acc: 0.8912 - val_loss: 0.3754 - val_acc: 0.8973\n",
      "Epoch 40/100\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.3918 - acc: 0.8925 - val_loss: 0.3705 - val_acc: 0.8978\n",
      "Epoch 41/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.3870 - acc: 0.8936 - val_loss: 0.3663 - val_acc: 0.8990\n",
      "Epoch 42/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.3824 - acc: 0.8948 - val_loss: 0.3620 - val_acc: 0.8998\n",
      "Epoch 43/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.3780 - acc: 0.8957 - val_loss: 0.3581 - val_acc: 0.8999\n",
      "Epoch 44/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.3739 - acc: 0.8964 - val_loss: 0.3540 - val_acc: 0.9012\n",
      "Epoch 45/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.3699 - acc: 0.8973 - val_loss: 0.3507 - val_acc: 0.9019\n",
      "Epoch 46/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.3662 - acc: 0.8982 - val_loss: 0.3472 - val_acc: 0.9028\n",
      "Epoch 47/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.3626 - acc: 0.8991 - val_loss: 0.3438 - val_acc: 0.9038\n",
      "Epoch 48/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.3591 - acc: 0.8998 - val_loss: 0.3406 - val_acc: 0.9043\n",
      "Epoch 49/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.3558 - acc: 0.9003 - val_loss: 0.3376 - val_acc: 0.9047\n",
      "Epoch 50/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.3526 - acc: 0.9010 - val_loss: 0.3346 - val_acc: 0.9062\n",
      "Epoch 51/100\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 0.3495 - acc: 0.9019 - val_loss: 0.3318 - val_acc: 0.9067\n",
      "Epoch 52/100\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.3465 - acc: 0.9026 - val_loss: 0.3290 - val_acc: 0.9077\n",
      "Epoch 53/100\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.3436 - acc: 0.9034 - val_loss: 0.3263 - val_acc: 0.9079\n",
      "Epoch 54/100\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.3408 - acc: 0.9043 - val_loss: 0.3240 - val_acc: 0.9080\n",
      "Epoch 55/100\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.3381 - acc: 0.9049 - val_loss: 0.3213 - val_acc: 0.9085\n",
      "Epoch 56/100\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.3354 - acc: 0.9056 - val_loss: 0.3190 - val_acc: 0.9089\n",
      "Epoch 57/100\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.3329 - acc: 0.9061 - val_loss: 0.3164 - val_acc: 0.9097\n",
      "Epoch 58/100\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.3303 - acc: 0.9063 - val_loss: 0.3143 - val_acc: 0.9109\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.3279 - acc: 0.9068 - val_loss: 0.3120 - val_acc: 0.9105\n",
      "Epoch 60/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.3255 - acc: 0.9075 - val_loss: 0.3099 - val_acc: 0.9113\n",
      "Epoch 61/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.3231 - acc: 0.9080 - val_loss: 0.3076 - val_acc: 0.9123\n",
      "Epoch 62/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.3208 - acc: 0.9085 - val_loss: 0.3059 - val_acc: 0.9123\n",
      "Epoch 63/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.3185 - acc: 0.9094 - val_loss: 0.3037 - val_acc: 0.9140\n",
      "Epoch 64/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.3164 - acc: 0.9097 - val_loss: 0.3017 - val_acc: 0.9134\n",
      "Epoch 65/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.3142 - acc: 0.9101 - val_loss: 0.2995 - val_acc: 0.9148\n",
      "Epoch 66/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.3121 - acc: 0.9108 - val_loss: 0.2975 - val_acc: 0.9153\n",
      "Epoch 67/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.3100 - acc: 0.9111 - val_loss: 0.2956 - val_acc: 0.9155\n",
      "Epoch 68/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.3080 - acc: 0.9120 - val_loss: 0.2938 - val_acc: 0.9165\n",
      "Epoch 69/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.3059 - acc: 0.9124 - val_loss: 0.2921 - val_acc: 0.9170\n",
      "Epoch 70/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.3039 - acc: 0.9127 - val_loss: 0.2905 - val_acc: 0.9177\n",
      "Epoch 71/100\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.3020 - acc: 0.9132 - val_loss: 0.2886 - val_acc: 0.9183\n",
      "Epoch 72/100\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3000 - acc: 0.9141 - val_loss: 0.2871 - val_acc: 0.9175\n",
      "Epoch 73/100\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.2981 - acc: 0.9144 - val_loss: 0.2851 - val_acc: 0.9193\n",
      "Epoch 74/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.2963 - acc: 0.9151 - val_loss: 0.2835 - val_acc: 0.9191\n",
      "Epoch 75/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2945 - acc: 0.9154 - val_loss: 0.2819 - val_acc: 0.9195\n",
      "Epoch 76/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2926 - acc: 0.9159 - val_loss: 0.2801 - val_acc: 0.9200\n",
      "Epoch 77/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2909 - acc: 0.9165 - val_loss: 0.2785 - val_acc: 0.9209\n",
      "Epoch 78/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2892 - acc: 0.9168 - val_loss: 0.2769 - val_acc: 0.9212\n",
      "Epoch 79/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2873 - acc: 0.9175 - val_loss: 0.2755 - val_acc: 0.9211\n",
      "Epoch 80/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.2857 - acc: 0.9179 - val_loss: 0.2737 - val_acc: 0.9220\n",
      "Epoch 81/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.2839 - acc: 0.9188 - val_loss: 0.2724 - val_acc: 0.9227\n",
      "Epoch 82/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2823 - acc: 0.9187 - val_loss: 0.2709 - val_acc: 0.9233\n",
      "Epoch 83/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2806 - acc: 0.9195 - val_loss: 0.2692 - val_acc: 0.9234\n",
      "Epoch 84/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2790 - acc: 0.9201 - val_loss: 0.2679 - val_acc: 0.9234\n",
      "Epoch 85/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.2773 - acc: 0.9206 - val_loss: 0.2665 - val_acc: 0.9237\n",
      "Epoch 86/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2758 - acc: 0.9210 - val_loss: 0.2651 - val_acc: 0.9247\n",
      "Epoch 87/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2742 - acc: 0.9211 - val_loss: 0.2635 - val_acc: 0.9255\n",
      "Epoch 88/100\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.2726 - acc: 0.9216 - val_loss: 0.2623 - val_acc: 0.9257\n",
      "Epoch 89/100\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 0.2710 - acc: 0.9224 - val_loss: 0.2613 - val_acc: 0.9255\n",
      "Epoch 90/100\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 0.2696 - acc: 0.9224 - val_loss: 0.2595 - val_acc: 0.9260\n",
      "Epoch 91/100\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.2680 - acc: 0.9227 - val_loss: 0.2582 - val_acc: 0.9271\n",
      "Epoch 92/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2665 - acc: 0.9234 - val_loss: 0.2568 - val_acc: 0.9268\n",
      "Epoch 93/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2650 - acc: 0.9236 - val_loss: 0.2556 - val_acc: 0.9278\n",
      "Epoch 94/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2636 - acc: 0.9240 - val_loss: 0.2543 - val_acc: 0.9281\n",
      "Epoch 95/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2622 - acc: 0.9245 - val_loss: 0.2528 - val_acc: 0.9279\n",
      "Epoch 96/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2607 - acc: 0.9248 - val_loss: 0.2515 - val_acc: 0.9292\n",
      "Epoch 97/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2593 - acc: 0.9251 - val_loss: 0.2503 - val_acc: 0.9291\n",
      "Epoch 98/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.2579 - acc: 0.9257 - val_loss: 0.2491 - val_acc: 0.9294\n",
      "Epoch 99/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.2565 - acc: 0.9265 - val_loss: 0.2479 - val_acc: 0.9297\n",
      "Epoch 100/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2551 - acc: 0.9266 - val_loss: 0.2466 - val_acc: 0.9297\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=100, epochs=100, \n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.132979</td>\n",
       "      <td>2.302540</td>\n",
       "      <td>0.138167</td>\n",
       "      <td>2.274208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.204771</td>\n",
       "      <td>2.259377</td>\n",
       "      <td>0.283833</td>\n",
       "      <td>2.243353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.317729</td>\n",
       "      <td>2.223705</td>\n",
       "      <td>0.368583</td>\n",
       "      <td>2.200959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.398708</td>\n",
       "      <td>2.173523</td>\n",
       "      <td>0.466250</td>\n",
       "      <td>2.140861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.470125</td>\n",
       "      <td>2.100495</td>\n",
       "      <td>0.506917</td>\n",
       "      <td>2.052832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.512708</td>\n",
       "      <td>1.997590</td>\n",
       "      <td>0.586750</td>\n",
       "      <td>1.933791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.556312</td>\n",
       "      <td>1.865116</td>\n",
       "      <td>0.568333</td>\n",
       "      <td>1.787469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.588750</td>\n",
       "      <td>1.713308</td>\n",
       "      <td>0.631167</td>\n",
       "      <td>1.628837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.631125</td>\n",
       "      <td>1.555939</td>\n",
       "      <td>0.677000</td>\n",
       "      <td>1.468916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.667583</td>\n",
       "      <td>1.405492</td>\n",
       "      <td>0.704583</td>\n",
       "      <td>1.321916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.697375</td>\n",
       "      <td>1.269491</td>\n",
       "      <td>0.721000</td>\n",
       "      <td>1.190710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.725583</td>\n",
       "      <td>1.149871</td>\n",
       "      <td>0.750667</td>\n",
       "      <td>1.075618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.749458</td>\n",
       "      <td>1.045761</td>\n",
       "      <td>0.773833</td>\n",
       "      <td>0.976090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.769813</td>\n",
       "      <td>0.955797</td>\n",
       "      <td>0.792833</td>\n",
       "      <td>0.890844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.787229</td>\n",
       "      <td>0.879201</td>\n",
       "      <td>0.809083</td>\n",
       "      <td>0.818622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.801042</td>\n",
       "      <td>0.814092</td>\n",
       "      <td>0.823500</td>\n",
       "      <td>0.758024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.813479</td>\n",
       "      <td>0.759267</td>\n",
       "      <td>0.833917</td>\n",
       "      <td>0.707217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.822250</td>\n",
       "      <td>0.712929</td>\n",
       "      <td>0.842167</td>\n",
       "      <td>0.664075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.831646</td>\n",
       "      <td>0.673625</td>\n",
       "      <td>0.848000</td>\n",
       "      <td>0.627811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.837708</td>\n",
       "      <td>0.640055</td>\n",
       "      <td>0.853500</td>\n",
       "      <td>0.597042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.843937</td>\n",
       "      <td>0.611157</td>\n",
       "      <td>0.856917</td>\n",
       "      <td>0.570505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.849000</td>\n",
       "      <td>0.586043</td>\n",
       "      <td>0.861583</td>\n",
       "      <td>0.547630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.853667</td>\n",
       "      <td>0.564016</td>\n",
       "      <td>0.864250</td>\n",
       "      <td>0.527561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.858396</td>\n",
       "      <td>0.544593</td>\n",
       "      <td>0.868417</td>\n",
       "      <td>0.509515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.861646</td>\n",
       "      <td>0.527256</td>\n",
       "      <td>0.873083</td>\n",
       "      <td>0.493739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.865104</td>\n",
       "      <td>0.511753</td>\n",
       "      <td>0.875667</td>\n",
       "      <td>0.479430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.868542</td>\n",
       "      <td>0.497666</td>\n",
       "      <td>0.879250</td>\n",
       "      <td>0.466546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.871271</td>\n",
       "      <td>0.484929</td>\n",
       "      <td>0.881583</td>\n",
       "      <td>0.454868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.873625</td>\n",
       "      <td>0.473383</td>\n",
       "      <td>0.883250</td>\n",
       "      <td>0.444270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.877104</td>\n",
       "      <td>0.462740</td>\n",
       "      <td>0.886417</td>\n",
       "      <td>0.435092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.913167</td>\n",
       "      <td>0.301985</td>\n",
       "      <td>0.918250</td>\n",
       "      <td>0.288552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.914083</td>\n",
       "      <td>0.300041</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>0.287066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.914375</td>\n",
       "      <td>0.298145</td>\n",
       "      <td>0.919250</td>\n",
       "      <td>0.285128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.915146</td>\n",
       "      <td>0.296306</td>\n",
       "      <td>0.919083</td>\n",
       "      <td>0.283544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.915375</td>\n",
       "      <td>0.294486</td>\n",
       "      <td>0.919500</td>\n",
       "      <td>0.281910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.915854</td>\n",
       "      <td>0.292649</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.280127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.916521</td>\n",
       "      <td>0.290914</td>\n",
       "      <td>0.920917</td>\n",
       "      <td>0.278486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.916833</td>\n",
       "      <td>0.289157</td>\n",
       "      <td>0.921167</td>\n",
       "      <td>0.276920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.917479</td>\n",
       "      <td>0.287346</td>\n",
       "      <td>0.921083</td>\n",
       "      <td>0.275483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.917875</td>\n",
       "      <td>0.285672</td>\n",
       "      <td>0.922000</td>\n",
       "      <td>0.273747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.918771</td>\n",
       "      <td>0.283942</td>\n",
       "      <td>0.922667</td>\n",
       "      <td>0.272420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.918667</td>\n",
       "      <td>0.282273</td>\n",
       "      <td>0.923333</td>\n",
       "      <td>0.270866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.919500</td>\n",
       "      <td>0.280629</td>\n",
       "      <td>0.923417</td>\n",
       "      <td>0.269192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.920125</td>\n",
       "      <td>0.278992</td>\n",
       "      <td>0.923417</td>\n",
       "      <td>0.267869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.920625</td>\n",
       "      <td>0.277322</td>\n",
       "      <td>0.923750</td>\n",
       "      <td>0.266474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.921021</td>\n",
       "      <td>0.275778</td>\n",
       "      <td>0.924750</td>\n",
       "      <td>0.265063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.921146</td>\n",
       "      <td>0.274171</td>\n",
       "      <td>0.925500</td>\n",
       "      <td>0.263550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.921625</td>\n",
       "      <td>0.272612</td>\n",
       "      <td>0.925667</td>\n",
       "      <td>0.262307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.922396</td>\n",
       "      <td>0.271011</td>\n",
       "      <td>0.925500</td>\n",
       "      <td>0.261317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.922417</td>\n",
       "      <td>0.269560</td>\n",
       "      <td>0.926000</td>\n",
       "      <td>0.259466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.922729</td>\n",
       "      <td>0.268006</td>\n",
       "      <td>0.927083</td>\n",
       "      <td>0.258234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.923417</td>\n",
       "      <td>0.266512</td>\n",
       "      <td>0.926833</td>\n",
       "      <td>0.256756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.923583</td>\n",
       "      <td>0.265037</td>\n",
       "      <td>0.927750</td>\n",
       "      <td>0.255573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.924042</td>\n",
       "      <td>0.263582</td>\n",
       "      <td>0.928083</td>\n",
       "      <td>0.254272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.924521</td>\n",
       "      <td>0.262177</td>\n",
       "      <td>0.927917</td>\n",
       "      <td>0.252786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.924833</td>\n",
       "      <td>0.260721</td>\n",
       "      <td>0.929167</td>\n",
       "      <td>0.251542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.925063</td>\n",
       "      <td>0.259270</td>\n",
       "      <td>0.929083</td>\n",
       "      <td>0.250314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.925708</td>\n",
       "      <td>0.257947</td>\n",
       "      <td>0.929417</td>\n",
       "      <td>0.249090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.926458</td>\n",
       "      <td>0.256465</td>\n",
       "      <td>0.929750</td>\n",
       "      <td>0.247943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.926583</td>\n",
       "      <td>0.255149</td>\n",
       "      <td>0.929667</td>\n",
       "      <td>0.246623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         acc      loss   val_acc  val_loss\n",
       "0   0.132979  2.302540  0.138167  2.274208\n",
       "1   0.204771  2.259377  0.283833  2.243353\n",
       "2   0.317729  2.223705  0.368583  2.200959\n",
       "3   0.398708  2.173523  0.466250  2.140861\n",
       "4   0.470125  2.100495  0.506917  2.052832\n",
       "5   0.512708  1.997590  0.586750  1.933791\n",
       "6   0.556312  1.865116  0.568333  1.787469\n",
       "7   0.588750  1.713308  0.631167  1.628837\n",
       "8   0.631125  1.555939  0.677000  1.468916\n",
       "9   0.667583  1.405492  0.704583  1.321916\n",
       "10  0.697375  1.269491  0.721000  1.190710\n",
       "11  0.725583  1.149871  0.750667  1.075618\n",
       "12  0.749458  1.045761  0.773833  0.976090\n",
       "13  0.769813  0.955797  0.792833  0.890844\n",
       "14  0.787229  0.879201  0.809083  0.818622\n",
       "15  0.801042  0.814092  0.823500  0.758024\n",
       "16  0.813479  0.759267  0.833917  0.707217\n",
       "17  0.822250  0.712929  0.842167  0.664075\n",
       "18  0.831646  0.673625  0.848000  0.627811\n",
       "19  0.837708  0.640055  0.853500  0.597042\n",
       "20  0.843937  0.611157  0.856917  0.570505\n",
       "21  0.849000  0.586043  0.861583  0.547630\n",
       "22  0.853667  0.564016  0.864250  0.527561\n",
       "23  0.858396  0.544593  0.868417  0.509515\n",
       "24  0.861646  0.527256  0.873083  0.493739\n",
       "25  0.865104  0.511753  0.875667  0.479430\n",
       "26  0.868542  0.497666  0.879250  0.466546\n",
       "27  0.871271  0.484929  0.881583  0.454868\n",
       "28  0.873625  0.473383  0.883250  0.444270\n",
       "29  0.877104  0.462740  0.886417  0.435092\n",
       "..       ...       ...       ...       ...\n",
       "70  0.913167  0.301985  0.918250  0.288552\n",
       "71  0.914083  0.300041  0.917500  0.287066\n",
       "72  0.914375  0.298145  0.919250  0.285128\n",
       "73  0.915146  0.296306  0.919083  0.283544\n",
       "74  0.915375  0.294486  0.919500  0.281910\n",
       "75  0.915854  0.292649  0.920000  0.280127\n",
       "76  0.916521  0.290914  0.920917  0.278486\n",
       "77  0.916833  0.289157  0.921167  0.276920\n",
       "78  0.917479  0.287346  0.921083  0.275483\n",
       "79  0.917875  0.285672  0.922000  0.273747\n",
       "80  0.918771  0.283942  0.922667  0.272420\n",
       "81  0.918667  0.282273  0.923333  0.270866\n",
       "82  0.919500  0.280629  0.923417  0.269192\n",
       "83  0.920125  0.278992  0.923417  0.267869\n",
       "84  0.920625  0.277322  0.923750  0.266474\n",
       "85  0.921021  0.275778  0.924750  0.265063\n",
       "86  0.921146  0.274171  0.925500  0.263550\n",
       "87  0.921625  0.272612  0.925667  0.262307\n",
       "88  0.922396  0.271011  0.925500  0.261317\n",
       "89  0.922417  0.269560  0.926000  0.259466\n",
       "90  0.922729  0.268006  0.927083  0.258234\n",
       "91  0.923417  0.266512  0.926833  0.256756\n",
       "92  0.923583  0.265037  0.927750  0.255573\n",
       "93  0.924042  0.263582  0.928083  0.254272\n",
       "94  0.924521  0.262177  0.927917  0.252786\n",
       "95  0.924833  0.260721  0.929167  0.251542\n",
       "96  0.925063  0.259270  0.929083  0.250314\n",
       "97  0.925708  0.257947  0.929417  0.249090\n",
       "98  0.926458  0.256465  0.929750  0.247943\n",
       "99  0.926583  0.255149  0.929667  0.246623\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "훈련결과 = pd.DataFrame(history.history)\n",
    "훈련결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20a1b048>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl8XHXd//3Xd/bJ3qTZ07QNLS3Q\nQAtdaIWwiLSigiiryNKLRZBL1EsUudT7x6Vye13qpV7ecumFCKJXhXKDXKJsivSirRRo0r1039Ik\nbZOm2ZOZLPP9/XGm0GKXtE1ykpn38/E4j5kzc+acT+YxeZ9zvuec7zHWWkREJLF43C5AREQGnsJd\nRCQBKdxFRBKQwl1EJAEp3EVEEpDCXUQkASncRUQSkMJdRCQBKdxFRBKQz60Fjx492o4bN86txYuI\njEhVVVX7rbW5x5vOtXAfN24clZWVbi1eRGREMsbs6s90apYREUlACncRkQSkcBcRSUCutbmLSHLq\n6emhpqaGSCTidinDWigUoqSkBL/ff1KfV7iLyJCqqakhPT2dcePGYYxxu5xhyVpLY2MjNTU1jB8/\n/qTmoWYZERlSkUiEnJwcBfsxGGPIyck5pb0bhbuIDDkF+/Gd6nfkWrjXt0XdWrSISMJzLdz3tUZ4\npnK3W4sXkSSWlpbmdgmDzrVwTwt6+ebz66ja1eRWCSIiCcu1cB/HHiZk9HL3f1ext0WnRInI0LPW\n8tWvfpUpU6ZQXl7OwoULAdizZw8VFRVMnTqVKVOmsGTJEvr6+rjtttvem/bHP/6xy9Ufm2unQpqe\nDp4PfYePNd7HXb+t5JnPzSbk97pVjoi44F/+uJ5361oHdJ5nFmXwfz5xVr+m/f3vf8+qVatYvXo1\n+/fvZ8aMGVRUVPC73/2OuXPn8o1vfIO+vj46OztZtWoVtbW1rFu3DoDm5uYBrXuguXe2TPYEgp17\n+VPKQ/TWreZzv60i2tvnWjkiknyWLl3KjTfeiNfrJT8/n4suuojly5czY8YMnnjiCR566CHWrl1L\neno6ZWVlbN++nS984Qu88sorZGRkuF3+Mbl3EVMwDf7h94QWXMsfwt/l+i1f457/Nvz8s+cS9GkL\nXiQZ9HcLe7BYa4/4ekVFBYsXL+bFF1/k5ptv5qtf/Sq33HILq1ev5tVXX+WRRx7hmWee4fHHHx/i\nivvP3fPc88+EO/6CPyOfBek/ZePGd7l3wUq6e2OuliUiyaGiooKFCxfS19dHQ0MDixcvZubMmeza\ntYu8vDzuvPNObr/9dlasWMH+/fuJxWJ8+tOf5jvf+Q4rVqxwu/xjcr/7gYwi+MxCwo9dxp9yH+GC\nDQ/wwHM+fnz9VLcrE5EEd/XVV7Ns2TLOOeccjDF8//vfp6CggCeffJIf/OAH+P1+0tLS+M1vfkNt\nbS3z588nFnM2Pr/3ve+5XP2xmaPtlgy26dOn28Nu1rHlNfjdtWzNvoiP1N7OT244l6umFrtSm4gM\nng0bNnDGGWe4XcaIcKTvyhhTZa2dfrzPDp/uByZeBpc/zITGRXwv5xW++T/rqG3ucrsqEZERafiE\nO8D590D5tVzftZCSWB3/tHAVfTF39ixEREay4RXuxsDlD2N8IX5V8Dxv7zjAY0u2u12ViMiIM7zC\nHSA9Hy76GkX1b3D/uJ388M+b2H2g0+2qRERGlOEX7gCz7oacCdwdeQw/vfzX4m1uVyQiMqIMz3D3\nBWDev+Fr3s4Pxyzjmcoa9rWq/xkRkf4anuEOztkzp89jXuOTpMXa+OVitb2LiPTX8A13gEu/iaen\ng2+NWc2Ct6s50NHtdkUikmSO1ff7zp07mTJlyhBW03/DO9wLyqF4Old0v0qkt5cn/rbD7YpEREYE\n97sfOJ7zbiP4wj9yb9l+fv2mnzsrysgI+d2uSkQGwstfh71rB3aeBeXw0X896tsPPPAAY8eO5fOf\n/zwADz30EMYYFi9eTFNTEz09PXz3u9/lqquuOqHFRiIR7rnnHiorK/H5fPzoRz/ikksuYf369cyf\nP5/u7m5isRjPPfccRUVFXHfdddTU1NDX18e3vvUtrr/++lP6sz9oeG+5A0z5FAQzuD38Bm2RXp5+\np9rtikRkBLvhhhveuykHwDPPPMP8+fN5/vnnWbFiBYsWLeIrX/nKUXuMPJpHHnkEgLVr1/LUU09x\n6623EolE+MUvfsEXv/hFVq1aRWVlJSUlJbzyyisUFRWxevVq1q1bx7x58wb0b4SRsOUeSIWzr2PU\nit/yoeLr+cOqOu6qOM3tqkRkIBxjC3uwTJs2jfr6eurq6mhoaGDUqFEUFhby5S9/mcWLF+PxeKit\nrWXfvn0UFBT0e75Lly7lC1/4AgCTJ09m7NixbN68mdmzZ/Pwww9TU1PDpz71KSZOnEh5eTn3338/\nDzzwAB//+Me58MILB/zvHP5b7gDn3QZ9Uf4xu5L1da1sb2h3uyIRGcGuueYann32WRYuXMgNN9zA\nggULaGhooKqqilWrVpGfn08kcmKnXx9tS/8zn/kML7zwAuFwmLlz5/L6669z+umnU1VVRXl5OQ8+\n+CDf/va3B+LPOszICPf4gdXpjX8ALH9as8ftikRkBLvhhht4+umnefbZZ7nmmmtoaWkhLy8Pv9/P\nokWL2LVr1wnPs6KiggULFgCwefNmqqurmTRpEtu3b6esrIz77ruPK6+8kjVr1lBXV0dKSgqf/exn\nuf/++welb/iREe4A592G/8Bmbi7aw5/W1LldjYiMYGeddRZtbW0UFxdTWFjITTfdRGVlJdOnT2fB\nggVMnjz5hOf5+c9/nr6+PsrLy7n++uv59a9/TTAYZOHChUyZMoWpU6eyceNGbrnlFtauXcvMmTOZ\nOnUqDz/8MN/85jcH/G8cPv25H093B/xgAhsLPsG8LVfx6pcqmFSQPngFisigUH/u/ZcY/bkfTyAV\nyi5hYsubeIzV1ruIyDEcN9yNMWOMMYuMMRuMMeuNMV88wjTGGPNTY8xWY8waY8y5g1Lt6Zfjbd3N\ntaXt/HF13QmfqiQicjLWrl3L1KlTDxtmzZrldlnH1J9TIXuBr1hrVxhj0oEqY8xfrLXvHjLNR4GJ\n8WEW8PP448CaeDkAN2ZtYOGudNbXtTKlOHPAFyMig8taizHG7TL6rby8nFWrVg3pMk914/W4W+7W\n2j3W2hXx523ABuCDNze9CviNdbwFZBljCk+psiPJKILCc5jSsQyfx/DH1WqaERlpQqEQjY2N2vM+\nBmstjY2NhEKhk57HCV3EZIwZB0wD3v7AW8XA7kPGa+KvHXbOojHmLuAugNLS0hOr9KCJc/Et+SFz\nywK8vG4vD16hAzMiI0lJSQk1NTU0NDS4XcqwFgqFKCkpOenP9zvcjTFpwHPAl6y1rR98+wgf+bvV\nsrX2UeBRcM6WOYE633f6PFj8fa4ftZkXt46lurGT0pyUk5qViAw9v9/P+PHj3S4j4fXrbBljjB8n\n2BdYa39/hElqgDGHjJcAg9NmUjQNUnOZFnF2HpZu3T8oixERGcn6c7aMAX4FbLDW/ugok70A3BI/\na+Z8oMVaOziXkXo8MPFy0nYvojjdz98U7iIif6c/zTIfAm4G1hpjDh4u/megFMBa+wvgJeAKYCvQ\nCcwf+FIPcfpczKoF3Dh2L49tg1jM4vGMnCPvIiKD7bjhbq1dypHb1A+dxgL3DlRRx1V2CXj8XO5f\nyQ87L+PdPTolUkTkUCPnCtVDhTJg7BzKmt4E1O4uIvJBIzPcAcouwte4kem5MbW7i4h8wMgN99I5\nAFyTt5t3dhwg0tPnckEiIsPHyA334nPBG2S2dzPR3hgrqpvcrkhEZNgYueHuC0LJDEpaV+L1GDXN\niIgcYuSGO8DY2Xj3rWV2cYClWxvdrkZEZNgY4eE+B2wfV+fWsbammZbOHrcrEhEZFkZ2uJfMBOPl\nfO9GYhbe3qGtdxERGOnhHkyDwnMobFlJwOuhapcOqoqIwEgPd4Cxc/DUVjG1KEylwl1EBEiQcKcv\nysdH72FtTYvOdxcRIRHCvXQ2ALO9m+jui7G2tsXlgkRE3Dfywz0lG3LPYGy702Fl5U41zYiIjPxw\nBxg7h0BdJRNyQlTtOuB2NSIirkuYcKe7jU8UNFK5q4lYTDfeFZHklhjhPmYWABeGdtDc2cP2/e0u\nFyQi4q7ECPfMEkgvZGL3BkDt7iIiiRHuxkDJDNIaVpCdGmC5wl1EklxihDtAyQxM8y4uLrY6qCoi\nSS9xwn3MTAAuz9jNzsZOGtqiLhckIuKexAn3wqng8TPVbAbQ1ruIJLXECXd/CArPJq9lDQGfRwdV\nRSSpJU64A5TMwFO3kqlFaVTptnsiksQSLtzp7WJu7n7W17YS7VUnYiKSnBIr3OMHVWf7t9PdF2Nd\nbavLBYmIuCOxwj1zDKQVUBZdD8BKNc2ISJJKrHA3BkqmE9q7gpJRYd2ZSUSSVmKFOzhNM007qCiC\nFdVNWKtOxEQk+SReuJc47e6XpVezrzVKXUvE5YJERIZe4oV70VTw+JhiNwGwQk0zIpKEEi/c/WEo\nKGd08xpCfg8rdFBVRJJQ4oU7QMlM52Km4nRWVDe7XY2IyJBL0HCfAT0dXD76AOtrW4j06GImEUku\nCRru0wGYHdhOb8yytrbF5YJERIZWYob7qHGQmsv46LuADqqKSPJJzHCP35kptHcFY3NSdFBVRJJO\nYoY7OE0zjVu4oNjLiupmXcwkIkklgcN9BgAfTq+moS3K7gNdLhckIjJ0Ejfci84F4+FstgCwfKfu\nzCQiyeO44W6MedwYU2+MWXeU9y82xrQYY1bFh/9n4Ms8CcE0yDuLnKY1pId8VOq2eyKSRPqz5f5r\nYN5xpllirZ0aH7596mUNkJLpmNoqZpRm6rZ7IpJUjhvu1trFwMjc7B0zE6ItXJbXypb6dpo6ut2u\nSERkSAxUm/tsY8xqY8zLxpizjjaRMeYuY0ylMaayoaFhgBZ9DPGDqucHtgOof3cRSRoDEe4rgLHW\n2nOA/w/4n6NNaK191Fo73Vo7PTc3dwAWfRzZp0Eoi9LO9QS8Hpar3V1EksQph7u1ttVa2x5//hLg\nN8aMPuXKBoLHAyUz8NVVMqU4Q+3uIpI0TjncjTEFxhgTfz4zPs/GU53vgCmZAfUbuKDEz5qaZnUi\nJiJJoT+nQj4FLAMmGWNqjDG3G2PuNsbcHZ/kGmCdMWY18FPgBjucLgcdOxuwXJK6g54+y5oadSIm\nIonPd7wJrLU3Huf9nwE/G7CKBlrxdPD4mdy9Djifyl0HmDk+2+2qREQGVeJeoXpQIAWKphKue4cJ\neWlqdxeRpJD44Q5QOhtqqzh/TAqVOw8Qiw2fViMRkcGQHOE+9kMQ6+GyjBpaI71sqW93uyIRkUGV\nHOFeOgswTLXOzTve2TF8TuYRERkMyRHu4VGQdyaZ9cspygyxbLvCXUQSW3KEO8DY2Zia5VxwWhbL\ntjWq3V1EEloShfsc6G5n3ugGmjp72LC31e2KREQGTfKEe+kcAKabjQAs26amGRFJXMkT7hmFMGoc\nGfuWUzY6lTcV7iKSwJIn3MHZeq9expzTsnl7eyM9fTG3KxIRGRTJFe5j50BnIx/Ja6Wju0/9zIhI\nwkq+cAem45zvvmzbfjerEREZNMkV7tllkFFCas0SzijMULu7iCSs5Ap3Y+C0i2HHYi4oy6JyV5P6\ndxeRhJRc4Q5QdglEmrk8ey/dvTFW6L6qIpKAki/cx18EwNnRlXg9Rk0zIpKQki/c03KhoJxg9WLO\nKclkyVYdVBWRxJN84Q5QdjHsfpsPn5bOmppmGtujblckIjKgkjTcL4G+bq7I2IG18L+bGtyuSERk\nQCVnuJfOBm+AcS3vkJse5PVN9W5XJCIyoJIz3AMpUHo+Zsf/cumkPBZvalBXBCKSUJIz3MFpmtm3\njrnjPLRFe3XjbBFJKEkc7hcDMMezDr/XsEhNMyKSQJI33AvPgfAoQtWLmTU+h79u2Od2RSIiAyZ5\nw93jdbbet77GpZNGs62hg+rGTrerEhEZEMkb7gCTroD2fcwbVQfA6xu19S4iiSG5w33CZWC8FO1b\nRNnoVP66Ue3uIpIYkjvcU7KdPt43vcwlk/N4e/sBOqK9blclInLKkjvcwWmaqX+Xj5VE6e6L6awZ\nEUkICvdJ8wCY2vkmuelB/rR6j8sFiYicOoV7dhnknoFn88tcMaWARZvqaVfTjIiMcAp3gMlXwK43\nuXJSCtHemM55F5ERT+EOTru77WNadDkFGSH+tEZNMyIysincAYrOhdQ8p2mmvJA3NjXQGulxuyoR\nkZOmcAfweJwDq1te4+NnZdPdF+O1d9U0IyIjl8L9oMmfgO42pvWspDgrrKYZERnRFO4HnXYJhLMx\na/9/rigvYMmWBlo61TQjIiOTwv0grx/O+iRsfIkrz8ikp8/y6rt73a5KROSkKNwPVX4d9HYxpW0J\n43JSeK6qxu2KREROynHD3RjzuDGm3hiz7ijvG2PMT40xW40xa4wx5w58mUNkzCzIHINZ+yzXzRjD\n2zsOsL2h3e2qREROWH+23H8NzDvG+x8FJsaHu4Cfn3pZLvF4oPwa2PY6104O4vUYnqnU1ruIjDzH\nDXdr7WLgwDEmuQr4jXW8BWQZYwoHqsAhV34t2D5yq1/h0sl5PFtVo5tni8iIMxBt7sXA7kPGa+Kv\n/R1jzF3GmEpjTGVDQ8MALHoQ5J8FeWfBmme4YcYY9rdHeV39vIvICDMQ4W6O8Jo90oTW2kettdOt\ntdNzc3MHYNGDpPwaqHmHi3I7yM8I8vQ71W5XJCJyQgYi3GuAMYeMlwB1AzBf95RfA4Bv/bNce94Y\n3tjcQF1zl8tFiYj030CE+wvALfGzZs4HWqy1I/vyzqxSGH8RVD3JdecWEbPwrE6LFJERpD+nQj4F\nLAMmGWNqjDG3G2PuNsbcHZ/kJWA7sBX4JfD5Qat2KM24A1prKG1cwocm5LBw+W56dWBVREYI3/Em\nsNbeeJz3LXDvgFU0XEy6AtILofJX3DL7ET732ypeWreXK88pcrsyEZHj0hWqR+P1wXm3wdbX+Eh+\nJ2WjU3l08TacdZmIyPCmcD+Wc28F48Wz4gnuqihjXW0rb25rdLsqEZHjUrgfS0YhTP4YrPxvPlme\nw+i0IP+1eLvbVYmIHJfC/Xhm3AFdBwht/iPzPzSOxZsbeLeu1e2qRESOSeF+POMrIGcivPMon51Z\nSkrAyy+XaOtdRIY3hfvxGAOzPge1VWTWv8WNM0t5YXUdNU2dblcmInJUCvf+mHYzpBXA4h9w+wXj\n8RrDf7y2xe2qRESOSuHeH/4QfOg+2LGYotY13DJ7LM+tqGHT3ja3KxMROSKFe3+ddxuk5MDi73Pv\nJRNIDfr4wasb3a5KROSIFO79FUiF2f8IW19jVPM67rn4NF7bUM87O47V1b2IiDsU7idixh0QyoLF\nP2T+nPHkZwT53ssbdNWqiAw7CvcTEcqA8++BTS8SblzHly87nZXVzby6fq/blYmIHEbhfqJmfQ7C\n2fDKP3PNucVMzEvjuy9uoLO71+3KRETeo3A/UeFRcOk3YNdSfJv+yMNXl1PT1MW//3mz25WJiLxH\n4X4yzpsP+VPgz99iZkmYm2aV8sTfdrBqd7PblYmIAAr3k+Pxwrx/hZZq+NtPeeCjk8lND/L159bQ\noxt6iMgwoHA/WeMvhDOvgqU/JiO6j+9+spyNe9t4VL1GisgwoHA/FR/5DmDh5Qf4yBl5fKy8kP94\nbQvralvcrkxEkpzC/VSMGgsXfx02/gnWLOTbV51FdmqAe3+3gtZIj9vViUgSU7ifqjn3QelseOmr\n5PTW87PPTKOmqYsHnl2ji5tExDUK91Pl8cInfw42Bv9zD9NLs3hg3iReXreXX7+50+3qRCRJKdwH\nQvZ4mPc92LkE3v4Fd15YxkfOzOf/fWkDVbvU94yIDD2F+0CZdjOc/lF47SFM3Qp+eM05FGeFuePJ\nSrY1tLtdnYgkGYX7QDEGrnoE0vLh6ZvI7GvkyX+YiddjuPXxd6hvjbhdoYgkEYX7QErNgRt/B5EW\nWHgzYzN9PH7bDA50dHPbE8tp0xk0IjJEFO4DraAcrv4F1LwDL/4TZxdn8vPPnsfmfW3c+ZtKOqLq\nYExEBp/CfTCceRVUfA1W/jcs/TEXnZ7Lv193Dst3NnHL4+/Q0qUteBEZXAr3wXLxg1B+Lfz1X2DZ\nf3LV1GJ+duM01tQ0c9Njb3Ggo9vtCkUkgSncB4vHA5/8BZxxJbz6ICx/jI+WF/LozdPZsq+d6/9r\nGbXNXW5XKSIJSuE+mLw++PSvnFMkX/wKVD3JJZPzeGL+DPa2RLjqZ0up3Knz4EVk4CncB5svANc9\nCRMugz/eB2/8gDllOTx/7xzSgj5u/OVbPLN8t9tVikiCUbgPBV8QbngKzr4BFn0XXvgCE3JC/OHe\nCzi/LIevPbeGbzy/lq7uPrcrFZEEoXAfKr6Ac4pkxVdh5W/hd9eTaTp44rYZfK6ijAVvV/OJny3l\n3bpWtysVkQSgcB9KxsCl34RP/BR2vAH/VYFv7yoevOIMfnv7TFq7evjkI3/jsSXb6YupR0kROXkK\ndzecdyvMfwViffD4XHjnl1w4YTSvfKmCitNz+e6LG/jUf/6N9XW66YeInByFu1vGzIC7l0DZJfDS\n/fD0Z8jua+SXt5zHT2+cRm1zhCt/9jcefvFd2nVVq4icIIW7m1Ky4can4fKHYdvr8MgszMrfcuXZ\nhfz1ny7iuulj+OWSHVz8g0X89q1d9Orm2yLST8atuwVNnz7dVlZWurLsYalxG7xwH+xaCuMrYN6/\nQf6ZrN7dzMMvbeCdHQcoy03la3MncfmZBXg8xu2KRcQFxpgqa+30406ncB9GYjGoegL++m2ItsJ5\n8+GSf8am5PCXd/fxr69sZHtDB5ML0rnvwxOZd5ZCXiTZ9Dfc+9UsY4yZZ4zZZIzZaoz5+hHev80Y\n02CMWRUf7jiZopOexwMzbof7VsKMO6Hq1/DTczFL/p3LJ6Tx5y9V8OPrz6G7L8bnF6xg7k8Ws3B5\nNZEenR8vIoc77pa7McYLbAY+AtQAy4EbrbXvHjLNbcB0a+0/9nfB2nLvh4ZN8Jf/A5tfhpQc+NCX\nYMYd9PnCvLh2Dz//321s2NNKTmqAm84fy02zSsnPCLldtYgMogFrljHGzAYestbOjY8/CGCt/d4h\n09yGwn3w1FQ5V7Zue90J+Rl3wIw7samjWba9kceX7uCvG+vxGMOHJ+fxmVmlXDgxF6+abEQSTn/D\n3dePeRUDh3Z+UgPMOsJ0nzbGVOBs5X/ZWqsOUwZKyXlw8/Owaxn87T/gjX+DpT/BnH0dc2beyZxb\nZ7BzfwdPLa/m2coa/vzuPoqzwlw1tYirpxUzMT/d7b9ARIZYf7bcrwXmWmvviI/fDMy01n7hkGly\ngHZrbdQYczdwnbX20iPM6y7gLoDS0tLzdu3aNXB/STLZvwXe+k9Y9RT0dkHxeTD9H+Csq4l6Qvx5\n/T6eW1HDki376YtZphRn8LHyIq4oL2BsTqrb1YvIKRjSZpkPTO8FDlhrM481XzXLDICuZlizECof\nh4aN4E917gJ1zvUw7kIaOnr54+o6/rCqltU1ztWuZxVlMPesAi47I58zCtMxRk03IiPJQIa7D6ep\n5cNALc4B1c9Ya9cfMk2htXZP/PnVwAPW2vOPNV+F+wCyFqrfgtW/g/X/45xGmV7oBP2Zn4Qxs6hp\nifDKur28tHYPK3c3Yy0UZYb48Bn5VJyey/ll2aSH/G7/JSJyHAN6nrsx5grgJ4AXeNxa+7Ax5ttA\npbX2BWPM94ArgV7gAHCPtXbjseapcB8kPV2w8UVY93vY+hr0RZ2gP30eTPoojK+gIeJh0cZ6/rJh\nH0u37Kerpw+fx3Bu6SjmTMhhzmmjmTomi4BPFzCLDDe6iEkg0gqbX4UNf4Ctr0NPB/hTYNwFMP4i\nKLuIaM5kqqpbWLJlP0u2NLC+rhVrIeT3cN7YUcwYl82McdlMHZNFarA/x99FZDAp3OVwvVHYuQQ2\nvQLbF0HjVuf1lBwYdyGUXQTjL6I5VMLbO5tYtq2Rd3YcYMNeJ+y9HsOk/HSmlWYxdYwzlOWm6XRL\nkSGmcJdja6mFHYudfuW3vwFtdc7raQVQOgtKZ8OYmbRmTmJlXReVOw+wanczq6qbaYv3UpkS8DKl\nKJPykkzOLMzgjMIMJuSlqTlHZBAp3KX/rHU6LtvxhnNgtvotaKl23vMGoKDcOd2y4Gxi+VPYTgmr\n90ZZW9vCmppm1te1Eu11eqz0ew2n5aYxqSDdGfLTOT0/neKssPrBERkACnc5NS21ULMcaqucoW6V\n02YPYLyQOxmKpkLhOfTlTWGXt5R1TV7erWtl095WNu1to64l8t7swn4vp+Wlclpu2nvD+NGpjBud\nQkpAbfki/aVwl4EVi0HTDti7BvaudYa6VdBR//40aQWQNxlGT4Lc02lPP42tsSI2tIXYXN/O1vp2\ntjd0UNvcddisCzNDjMtJpTQ7hdKcFMbmpDjPs1PIDPt1Lr7IIRTuMjTa9jpBX7/BGRo2QMPm97fy\nAYIZkHMa5EyAUePoTh9DnclnW28uGzrS2L6/i52NHVQf6GJ/e/Sw2acHfZRkp1CcFaZkVJjirDDF\no8IUZoYozgozOi2o5h5JKgp3cY+10Frr9GrZuNXpLqFxKxzY5jT32EO6KPYGIXs8jBoHmSV0pxbR\n4M2jJpbDtu5sNneksLs5Sk1TFzVNnXR0H969sd9ryM8IUZQZpiAzRGFmiPyMEAXxx/yMIHnpIR3k\nlYShcJfhqa/HCf6mnXBgBxzY7gzNu6B5N0SaD5/e44eMIsgswWYUEU0poMmXzz6TS3Usmx2RdHZ0\nBKhr6WZPaxf7WqJ0H+F2hNmpAfLSg+SmO2HvPDrjuelBRqc5jxkhn5qBZFgbyF4hRQaO1+9spY8a\nB2UX//370TZoqXGCvqUamquhtQ5a6zA1ywm11lHY100hMPXgZ4wX0vIgMw9bUkA0lE+bL4cD3lHU\n2yxqezKp7g6yI+Kjrq2HrfXt7G+P0tP39xs2Aa+HnLSAM6QGyUkLMDotSHZqgOzUAKPTAmSnBsmJ\nj6cEvFoZyLCkcJfhJZgOeWeZax1/AAAJhUlEQVQ4w5HEYs5B3JYaaNkN7fXQvi8+1GNa6wjVriDU\n0UAuMOmDnw9lQloeNn80PaEcOn2jaPVm0WwyaYils683jbqeVGq6U9nZEXtvRXDwVM8PCvg8ZKcE\nGJUaYFSK/73H7JQAWSnOCiArxU9WivN6VkpAewcyJBTuMrJ4PJBe4Awlx9gz7euJB/9eaIuHf0eD\n81pHPaajkUDTVgLt9WR1NVHKUZonQ1nYvBxi4RyigSy6fBm0ezJoNek0xVI50BdmX28K+3pSqI2m\nsKM5xJudhuZIL0dr8fR6DFlhP1kpfjLDTuBnhv1HHlIOHw/5vaf+HUpSULhLYvL6IbPYGY6nrxe6\nDjjh39noDB3733tuOvbj7dxPSuceUrrWk9N5wOlH/6jLDmCzs4gFM+gJZBLxZdLpdVYKbaTQbMM0\n9YVp7A1T3xuivilI9Z4QtdEA+yI+7DFubRzwecgM+8kI+cgM+0kP+cmIjzuPfjLCPuf1kPOYGR9P\nD/kI+9WMlCwU7iJen9Nmn5bX/8/0dDn96UeaoasJOg84K4jORuhqxkSa8XY14+1qItS1n6y2Lc50\n3e3HnK0NGWwwg75ABj2BDLp96XR50ugyIToI025DtNgwzX1hGntDNDaHaKgPsLU7SF0kSFMsTDth\n4MgB7vMY0uOhnxb0xZ+/H/7Oa/73Xj84fnDatKCPtJAPv1dnHw13CneRk+EPO0NG4Yl9rq/X6W8/\n0uz02hlpia8gmiHaiom0YiLNeCKt+CPNpERayIrUOSuFaLvz2Nd95HkHnAdrPMQC6fT60+nxpRH1\nphLxpNJpUt5bQbTZEK2xIE2dQZpbgxzo9VPTHWB/t5+2WJAOG6KTIB2EjrgnEfJ7nKCPh31qwAn/\n1KAzpAeP9NxLWvx5WtBHSsBLatBH0OfR3sQgULiLDCWvD1KyneFk9UTiK4gWZwURbXHOMoq0QKQF\nE2lx9hqibQSjbaRFD05T40wXbYPeyJHnfYT7tfT60uj1p9LtSaHbE6LbBON7Eim0E6atJ0xrNEhr\nc4DWPj/NvQHqegI09wXiK4kQ7YTosM7eRzc+Dt2z8HkMKYH3g/9g+KcGvaQGnPGUQ56nBrykxB+d\n8fffTwl6SfF78WnPQuEuMuL4Q85wIs1IH9TX6+wFdLdDd8fhewbdne+/F23HF23DF20h1N3hvNcT\nfz/aGF9ZtDqvHcobH44gZnz0+lLo8YTp9oSIekJETJgIQTr7gnR2BGhvD9AWC9HcF6SlL0BLr5/6\nPj9dBIkQoNM6exWd8ZVGJ0E6CRGL72UEfJ7Dwj8c8JIa9BL2H9xj8JIScFYQ4YDzWjgQX0EEvPHB\n+dzB8ZDfO6L2MhTuIsnI64NwljMMhFgsHvrxFcWhz6Pthz16ujsIxIfUnvgKo7vD+UxPS3wF0gG9\n7c6dxOCYK4tD9XoC9HpC9Jgg3SZINBYkEgnQFQnSaYN02gAdNkBbLEBbn7On0WiD1BIgip+IdR67\n8RPFT5cN0kaYdhumkxC9ngA+f/CQFcL7K4Ow3/vea2G/l3DAQ0rAR8j//vvvPT9s+vffG8iVh8Jd\nRE6dxwPBNGcgf+Dm29cTX1lEnDOUerreD//ujvdXIPHnvp5OfD0RQj0dzmd6uuIrjYND4yGf7wTT\nc8IlxfDQ0xekOxIkGg0SIUgXQaLWT5f102kDdMb8tMf8dNkAXQTYR3zlQui9lUiE+HDI82784A9h\nfGGMP4wvECIl6KwgDq4E+kvhLiLDl9cP4VEQHqT59/U4K4DeyPuPvVHnoPXBFUO07f0VSE8Xnt4I\nwZ4ugj2dpB9cUfRE4p+NxJ83Yd9bsXRhjnaM42hiQNQZutv8zl6IcZqkftHPWSjcRSR5ef3OQMaA\nz/qwxpVY33tB/3crk6M99kahN0Kgp4tAb5S0g59nU7+Wr3AXERlsHq/TtUYwfQBm9nj/FjkASxIR\nkWFG4S4ikoAU7iIiCUjhLiKSgBTuIiIJSOEuIpKAFO4iIglI4S4ikoCMPdq9wAZ7wca00d9LrZLD\naGC/20UME/ouDqfv43DJ/n2MtdbmHm8iN69Q3WStPcZNMJOLMaZS34dD38Xh9H0cTt9H/6hZRkQk\nASncRUQSkJvh/qiLyx6O9H28T9/F4fR9HE7fRz+4dkBVREQGj5plREQSkCvhboyZZ4zZZIzZaoz5\nuhs1uMUYM8YYs8gYs8EYs94Y88X469nGmL8YY7bEH0e5XetQMsZ4jTErjTF/io+PN8a8Hf8+Fhpj\nAm7XOFSMMVnGmGeNMRvjv5PZyfr7MMZ8Of5/ss4Y85QxJpTMv40TMeThbozxAo8AHwXOBG40xpw5\n1HW4qBf4irX2DOB84N743/914K/W2onAX+PjyeSLwIZDxv8N+HH8+2gCbnelKnf8B/CKtXYycA7O\n95J0vw9jTDFwHzDdWjsF5xbZN5Dcv41+c2PLfSaw1Vq73VrbDTwNXOVCHa6w1u6x1q6IP2/D+cct\nxvkOnoxP9iTwSXcqHHrGmBLgY8Bj8XEDXAo8G58kab4PY0wGUAH8CsBa222tbSZ5fx8+IGyM8QEp\nwB6S9LdxotwI92Jg9yHjNfHXko4xZhwwDXgbyLfW7gFnBQDkuVfZkPsJ8DWc2wID5ADN1tre+Hgy\n/UbKgAbgiXgz1WPGmFSS8Pdhra0FfghU44R6C1BF8v42Togb4W6O8FrSnbJjjEkDngO+ZK1tdbse\ntxhjPg7UW2urDn35CJMmy2/EB5wL/NxaOw3oIAmaYI4kflzhKmA8UASk4jTnflCy/DZOiBvhXgOM\nOWS8BKhzoQ7XGGP8OMG+wFr7+/jL+4wxhfH3C4F6t+obYh8CrjTG7MRporsUZ0s+K74rDsn1G6kB\naqy1b8fHn8UJ+2T8fVwG7LDWNlhre4DfA3NI3t/GCXEj3JcDE+NHvAM4B0hecKEOV8Tbk38FbLDW\n/uiQt14Abo0/vxX4w1DX5gZr7YPW2hJr7Tic38Lr1tqbgEXANfHJkun72AvsNsZMir/0YeBdkvP3\nUQ2cb4xJif/fHPwukvK3caJcuYjJGHMFztaZF3jcWvvwkBfhEmPMBcASYC3vtzH/M067+zNAKc6P\n+lpr7QFXinSJMeZi4H5r7ceNMWU4W/LZwErgs9baqJv1DRVjzFScg8sBYDswH2dDLOl+H8aYfwGu\nxznLbCVwB04be1L+Nk6ErlAVEUlAukJVRCQBKdxFRBKQwl1EJAEp3EVEEpDCXUQkASncRUQSkMJd\nRCQBKdxFRBLQ/wW/+OilWbtJdwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20a1b518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "훈련결과[['loss', 'val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20d8b518>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1B1POI0dHrUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20d80ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "훈련결과[['acc', 'val_acc']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(50, input_shape=(784,), \n",
    "                activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=categorical_crossentropy, optimizer='sgd', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 1.1692 - acc: 0.6745 - val_loss: 0.5278 - val_acc: 0.8691\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.4682 - acc: 0.8739 - val_loss: 0.3795 - val_acc: 0.8937\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3779 - acc: 0.8942 - val_loss: 0.3291 - val_acc: 0.9059\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.3365 - acc: 0.9055 - val_loss: 0.3012 - val_acc: 0.9135\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.3095 - acc: 0.9136 - val_loss: 0.2817 - val_acc: 0.9186\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2886 - acc: 0.9185 - val_loss: 0.2654 - val_acc: 0.9240\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2720 - acc: 0.9234 - val_loss: 0.2525 - val_acc: 0.9289\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2576 - acc: 0.9277 - val_loss: 0.2425 - val_acc: 0.9308\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2453 - acc: 0.9310 - val_loss: 0.2318 - val_acc: 0.9345\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2347 - acc: 0.9345 - val_loss: 0.2226 - val_acc: 0.9366\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2247 - acc: 0.9368 - val_loss: 0.2150 - val_acc: 0.9398\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2160 - acc: 0.9389 - val_loss: 0.2078 - val_acc: 0.9415\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2081 - acc: 0.9415 - val_loss: 0.2032 - val_acc: 0.9425\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2008 - acc: 0.9435 - val_loss: 0.1972 - val_acc: 0.9453\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1938 - acc: 0.9446 - val_loss: 0.1917 - val_acc: 0.9458\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1880 - acc: 0.9466 - val_loss: 0.1860 - val_acc: 0.9485\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1822 - acc: 0.9479 - val_loss: 0.1812 - val_acc: 0.9497\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1766 - acc: 0.9495 - val_loss: 0.1769 - val_acc: 0.9510\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1715 - acc: 0.9511 - val_loss: 0.1746 - val_acc: 0.9510\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1669 - acc: 0.9519 - val_loss: 0.1697 - val_acc: 0.9516\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1625 - acc: 0.9536 - val_loss: 0.1680 - val_acc: 0.9520\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1583 - acc: 0.9547 - val_loss: 0.1645 - val_acc: 0.9531\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1544 - acc: 0.9561 - val_loss: 0.1602 - val_acc: 0.9556\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1505 - acc: 0.9572 - val_loss: 0.1583 - val_acc: 0.9550\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1468 - acc: 0.9582 - val_loss: 0.1559 - val_acc: 0.9559\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1435 - acc: 0.9596 - val_loss: 0.1525 - val_acc: 0.9570\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1403 - acc: 0.9601 - val_loss: 0.1513 - val_acc: 0.9578\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1374 - acc: 0.9608 - val_loss: 0.1483 - val_acc: 0.9573\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1341 - acc: 0.9618 - val_loss: 0.1455 - val_acc: 0.9589\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1311 - acc: 0.9627 - val_loss: 0.1443 - val_acc: 0.9590\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1287 - acc: 0.9637 - val_loss: 0.1423 - val_acc: 0.9593\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1260 - acc: 0.9643 - val_loss: 0.1408 - val_acc: 0.9607\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1237 - acc: 0.9650 - val_loss: 0.1400 - val_acc: 0.9598\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1211 - acc: 0.9662 - val_loss: 0.1380 - val_acc: 0.9605\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1189 - acc: 0.9664 - val_loss: 0.1369 - val_acc: 0.9605\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1169 - acc: 0.9670 - val_loss: 0.1337 - val_acc: 0.9612\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1146 - acc: 0.9676 - val_loss: 0.1333 - val_acc: 0.9615\n",
      "Epoch 38/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1126 - acc: 0.9685 - val_loss: 0.1323 - val_acc: 0.9618\n",
      "Epoch 39/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1107 - acc: 0.9687 - val_loss: 0.1307 - val_acc: 0.9618\n",
      "Epoch 40/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1088 - acc: 0.9691 - val_loss: 0.1291 - val_acc: 0.9618\n",
      "Epoch 41/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1069 - acc: 0.9701 - val_loss: 0.1284 - val_acc: 0.9629\n",
      "Epoch 42/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1050 - acc: 0.9704 - val_loss: 0.1272 - val_acc: 0.9625\n",
      "Epoch 43/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1034 - acc: 0.9708 - val_loss: 0.1263 - val_acc: 0.9629\n",
      "Epoch 44/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1017 - acc: 0.9715 - val_loss: 0.1248 - val_acc: 0.9643\n",
      "Epoch 45/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1001 - acc: 0.9720 - val_loss: 0.1242 - val_acc: 0.9636\n",
      "Epoch 46/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0984 - acc: 0.9724 - val_loss: 0.1228 - val_acc: 0.9650\n",
      "Epoch 47/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0972 - acc: 0.9727 - val_loss: 0.1215 - val_acc: 0.9644\n",
      "Epoch 48/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0955 - acc: 0.9728 - val_loss: 0.1209 - val_acc: 0.9644\n",
      "Epoch 49/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.0940 - acc: 0.9735 - val_loss: 0.1216 - val_acc: 0.9654\n",
      "Epoch 50/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0928 - acc: 0.9738 - val_loss: 0.1190 - val_acc: 0.9656\n",
      "Epoch 51/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.0914 - acc: 0.9742 - val_loss: 0.1180 - val_acc: 0.9663\n",
      "Epoch 52/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.0901 - acc: 0.9751 - val_loss: 0.1194 - val_acc: 0.9659\n",
      "Epoch 53/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.0887 - acc: 0.9751 - val_loss: 0.1175 - val_acc: 0.9657\n",
      "Epoch 54/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.0874 - acc: 0.9755 - val_loss: 0.1168 - val_acc: 0.9655\n",
      "Epoch 55/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.0863 - acc: 0.9759 - val_loss: 0.1151 - val_acc: 0.9662\n",
      "Epoch 56/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.0851 - acc: 0.9763 - val_loss: 0.1147 - val_acc: 0.9663\n",
      "Epoch 57/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0838 - acc: 0.9768 - val_loss: 0.1144 - val_acc: 0.9672\n",
      "Epoch 58/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.0827 - acc: 0.9772 - val_loss: 0.1152 - val_acc: 0.9670\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0813 - acc: 0.9774 - val_loss: 0.1131 - val_acc: 0.9673\n",
      "Epoch 60/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0803 - acc: 0.9781 - val_loss: 0.1122 - val_acc: 0.9667\n",
      "Epoch 61/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0793 - acc: 0.9781 - val_loss: 0.1128 - val_acc: 0.9670\n",
      "Epoch 62/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0782 - acc: 0.9785 - val_loss: 0.1113 - val_acc: 0.9673\n",
      "Epoch 63/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0771 - acc: 0.9786 - val_loss: 0.1108 - val_acc: 0.9674\n",
      "Epoch 64/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.0761 - acc: 0.9791 - val_loss: 0.1110 - val_acc: 0.9678\n",
      "Epoch 65/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0750 - acc: 0.9792 - val_loss: 0.1123 - val_acc: 0.9668\n",
      "Epoch 66/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0740 - acc: 0.9795 - val_loss: 0.1092 - val_acc: 0.9679\n",
      "Epoch 67/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.0732 - acc: 0.9798 - val_loss: 0.1086 - val_acc: 0.9678\n",
      "Epoch 68/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0723 - acc: 0.9803 - val_loss: 0.1087 - val_acc: 0.9678\n",
      "Epoch 69/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.0712 - acc: 0.9803 - val_loss: 0.1085 - val_acc: 0.9688\n",
      "Epoch 70/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0705 - acc: 0.9807 - val_loss: 0.1075 - val_acc: 0.9678\n",
      "Epoch 71/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0694 - acc: 0.9810 - val_loss: 0.1066 - val_acc: 0.9685\n",
      "Epoch 72/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0685 - acc: 0.9813 - val_loss: 0.1069 - val_acc: 0.9694\n",
      "Epoch 73/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0677 - acc: 0.9819 - val_loss: 0.1070 - val_acc: 0.9685\n",
      "Epoch 74/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0667 - acc: 0.9821 - val_loss: 0.1081 - val_acc: 0.9686\n",
      "Epoch 75/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0660 - acc: 0.9820 - val_loss: 0.1056 - val_acc: 0.9697\n",
      "Epoch 76/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0651 - acc: 0.9824 - val_loss: 0.1056 - val_acc: 0.9688\n",
      "Epoch 77/100\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.0642 - acc: 0.9825 - val_loss: 0.1060 - val_acc: 0.9687\n",
      "Epoch 78/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0635 - acc: 0.9829 - val_loss: 0.1048 - val_acc: 0.9696\n",
      "Epoch 79/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0627 - acc: 0.9830 - val_loss: 0.1065 - val_acc: 0.9699\n",
      "Epoch 80/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0619 - acc: 0.9835 - val_loss: 0.1042 - val_acc: 0.9696\n",
      "Epoch 81/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0610 - acc: 0.9839 - val_loss: 0.1050 - val_acc: 0.9692\n",
      "Epoch 82/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0604 - acc: 0.9843 - val_loss: 0.1059 - val_acc: 0.9691\n",
      "Epoch 83/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0597 - acc: 0.9845 - val_loss: 0.1035 - val_acc: 0.9703\n",
      "Epoch 84/100\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.0590 - acc: 0.9840 - val_loss: 0.1033 - val_acc: 0.9695\n",
      "Epoch 85/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0582 - acc: 0.9849 - val_loss: 0.1038 - val_acc: 0.9696\n",
      "Epoch 86/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0573 - acc: 0.9851 - val_loss: 0.1022 - val_acc: 0.9702\n",
      "Epoch 87/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0568 - acc: 0.9855 - val_loss: 0.1027 - val_acc: 0.9701\n",
      "Epoch 88/100\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.0563 - acc: 0.9854 - val_loss: 0.1014 - val_acc: 0.9705\n",
      "Epoch 89/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0552 - acc: 0.9855 - val_loss: 0.1014 - val_acc: 0.9701\n",
      "Epoch 90/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0548 - acc: 0.9858 - val_loss: 0.1016 - val_acc: 0.9702\n",
      "Epoch 91/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0542 - acc: 0.9856 - val_loss: 0.1014 - val_acc: 0.9707\n",
      "Epoch 92/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0536 - acc: 0.9861 - val_loss: 0.1026 - val_acc: 0.9697\n",
      "Epoch 93/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.0528 - acc: 0.9863 - val_loss: 0.1015 - val_acc: 0.9700\n",
      "Epoch 94/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0521 - acc: 0.9865 - val_loss: 0.1015 - val_acc: 0.9704\n",
      "Epoch 95/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0516 - acc: 0.9869 - val_loss: 0.1011 - val_acc: 0.9707\n",
      "Epoch 96/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.0510 - acc: 0.9869 - val_loss: 0.1010 - val_acc: 0.9705\n",
      "Epoch 97/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.0504 - acc: 0.9869 - val_loss: 0.1002 - val_acc: 0.9716\n",
      "Epoch 98/100\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.0499 - acc: 0.9874 - val_loss: 0.1001 - val_acc: 0.9706\n",
      "Epoch 99/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0493 - acc: 0.9871 - val_loss: 0.0993 - val_acc: 0.9708\n",
      "Epoch 100/100\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0488 - acc: 0.9874 - val_loss: 0.1006 - val_acc: 0.9708\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=100, epochs=100, \n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.674521</td>\n",
       "      <td>1.169194</td>\n",
       "      <td>0.869083</td>\n",
       "      <td>0.527787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.873938</td>\n",
       "      <td>0.468215</td>\n",
       "      <td>0.893667</td>\n",
       "      <td>0.379492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.894208</td>\n",
       "      <td>0.377929</td>\n",
       "      <td>0.905917</td>\n",
       "      <td>0.329114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.905542</td>\n",
       "      <td>0.336457</td>\n",
       "      <td>0.913500</td>\n",
       "      <td>0.301186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.913604</td>\n",
       "      <td>0.309486</td>\n",
       "      <td>0.918583</td>\n",
       "      <td>0.281708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.918500</td>\n",
       "      <td>0.288569</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.265414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.923354</td>\n",
       "      <td>0.272018</td>\n",
       "      <td>0.928917</td>\n",
       "      <td>0.252550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.927729</td>\n",
       "      <td>0.257623</td>\n",
       "      <td>0.930750</td>\n",
       "      <td>0.242524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.930958</td>\n",
       "      <td>0.245280</td>\n",
       "      <td>0.934500</td>\n",
       "      <td>0.231813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.934500</td>\n",
       "      <td>0.234711</td>\n",
       "      <td>0.936583</td>\n",
       "      <td>0.222583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.936771</td>\n",
       "      <td>0.224726</td>\n",
       "      <td>0.939833</td>\n",
       "      <td>0.214968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.938854</td>\n",
       "      <td>0.216001</td>\n",
       "      <td>0.941500</td>\n",
       "      <td>0.207793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.941542</td>\n",
       "      <td>0.208053</td>\n",
       "      <td>0.942500</td>\n",
       "      <td>0.203202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.943458</td>\n",
       "      <td>0.200775</td>\n",
       "      <td>0.945250</td>\n",
       "      <td>0.197216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.944646</td>\n",
       "      <td>0.193814</td>\n",
       "      <td>0.945750</td>\n",
       "      <td>0.191741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.946583</td>\n",
       "      <td>0.187981</td>\n",
       "      <td>0.948500</td>\n",
       "      <td>0.185989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.947917</td>\n",
       "      <td>0.182169</td>\n",
       "      <td>0.949667</td>\n",
       "      <td>0.181247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.949500</td>\n",
       "      <td>0.176587</td>\n",
       "      <td>0.951000</td>\n",
       "      <td>0.176851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.951146</td>\n",
       "      <td>0.171458</td>\n",
       "      <td>0.951000</td>\n",
       "      <td>0.174634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.951854</td>\n",
       "      <td>0.166915</td>\n",
       "      <td>0.951583</td>\n",
       "      <td>0.169702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.953625</td>\n",
       "      <td>0.162473</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>0.168036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.954729</td>\n",
       "      <td>0.158285</td>\n",
       "      <td>0.953083</td>\n",
       "      <td>0.164542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.956125</td>\n",
       "      <td>0.154422</td>\n",
       "      <td>0.955583</td>\n",
       "      <td>0.160170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.957167</td>\n",
       "      <td>0.150538</td>\n",
       "      <td>0.955000</td>\n",
       "      <td>0.158339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.958208</td>\n",
       "      <td>0.146848</td>\n",
       "      <td>0.955917</td>\n",
       "      <td>0.155948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.959563</td>\n",
       "      <td>0.143507</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.152450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.960125</td>\n",
       "      <td>0.140264</td>\n",
       "      <td>0.957750</td>\n",
       "      <td>0.151331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.960771</td>\n",
       "      <td>0.137393</td>\n",
       "      <td>0.957333</td>\n",
       "      <td>0.148312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.961792</td>\n",
       "      <td>0.134058</td>\n",
       "      <td>0.958917</td>\n",
       "      <td>0.145533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.962688</td>\n",
       "      <td>0.131144</td>\n",
       "      <td>0.959000</td>\n",
       "      <td>0.144314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.980958</td>\n",
       "      <td>0.069380</td>\n",
       "      <td>0.968500</td>\n",
       "      <td>0.106574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.981313</td>\n",
       "      <td>0.068540</td>\n",
       "      <td>0.969417</td>\n",
       "      <td>0.106852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.981875</td>\n",
       "      <td>0.067690</td>\n",
       "      <td>0.968500</td>\n",
       "      <td>0.106990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.982146</td>\n",
       "      <td>0.066679</td>\n",
       "      <td>0.968583</td>\n",
       "      <td>0.108109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.982042</td>\n",
       "      <td>0.066030</td>\n",
       "      <td>0.969667</td>\n",
       "      <td>0.105615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.982354</td>\n",
       "      <td>0.065080</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.105598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.982458</td>\n",
       "      <td>0.064194</td>\n",
       "      <td>0.968667</td>\n",
       "      <td>0.105960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.982896</td>\n",
       "      <td>0.063499</td>\n",
       "      <td>0.969583</td>\n",
       "      <td>0.104777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.982958</td>\n",
       "      <td>0.062665</td>\n",
       "      <td>0.969917</td>\n",
       "      <td>0.106523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.983458</td>\n",
       "      <td>0.061904</td>\n",
       "      <td>0.969583</td>\n",
       "      <td>0.104242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.983938</td>\n",
       "      <td>0.061034</td>\n",
       "      <td>0.969167</td>\n",
       "      <td>0.104972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.984250</td>\n",
       "      <td>0.060362</td>\n",
       "      <td>0.969083</td>\n",
       "      <td>0.105922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.984458</td>\n",
       "      <td>0.059731</td>\n",
       "      <td>0.970250</td>\n",
       "      <td>0.103545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.984042</td>\n",
       "      <td>0.059032</td>\n",
       "      <td>0.969500</td>\n",
       "      <td>0.103350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.984854</td>\n",
       "      <td>0.058228</td>\n",
       "      <td>0.969583</td>\n",
       "      <td>0.103803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.985125</td>\n",
       "      <td>0.057299</td>\n",
       "      <td>0.970167</td>\n",
       "      <td>0.102216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.985458</td>\n",
       "      <td>0.056787</td>\n",
       "      <td>0.970083</td>\n",
       "      <td>0.102654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.985396</td>\n",
       "      <td>0.056277</td>\n",
       "      <td>0.970500</td>\n",
       "      <td>0.101380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.985479</td>\n",
       "      <td>0.055150</td>\n",
       "      <td>0.970083</td>\n",
       "      <td>0.101449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.985833</td>\n",
       "      <td>0.054825</td>\n",
       "      <td>0.970167</td>\n",
       "      <td>0.101621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.985604</td>\n",
       "      <td>0.054173</td>\n",
       "      <td>0.970667</td>\n",
       "      <td>0.101352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.986063</td>\n",
       "      <td>0.053568</td>\n",
       "      <td>0.969667</td>\n",
       "      <td>0.102644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.986250</td>\n",
       "      <td>0.052825</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.101544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.986458</td>\n",
       "      <td>0.052138</td>\n",
       "      <td>0.970417</td>\n",
       "      <td>0.101469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.986896</td>\n",
       "      <td>0.051587</td>\n",
       "      <td>0.970667</td>\n",
       "      <td>0.101061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.986917</td>\n",
       "      <td>0.050985</td>\n",
       "      <td>0.970500</td>\n",
       "      <td>0.101001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.986875</td>\n",
       "      <td>0.050397</td>\n",
       "      <td>0.971583</td>\n",
       "      <td>0.100219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.987375</td>\n",
       "      <td>0.049874</td>\n",
       "      <td>0.970583</td>\n",
       "      <td>0.100086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.987125</td>\n",
       "      <td>0.049272</td>\n",
       "      <td>0.970833</td>\n",
       "      <td>0.099310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.987417</td>\n",
       "      <td>0.048754</td>\n",
       "      <td>0.970833</td>\n",
       "      <td>0.100599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         acc      loss   val_acc  val_loss\n",
       "0   0.674521  1.169194  0.869083  0.527787\n",
       "1   0.873938  0.468215  0.893667  0.379492\n",
       "2   0.894208  0.377929  0.905917  0.329114\n",
       "3   0.905542  0.336457  0.913500  0.301186\n",
       "4   0.913604  0.309486  0.918583  0.281708\n",
       "5   0.918500  0.288569  0.924000  0.265414\n",
       "6   0.923354  0.272018  0.928917  0.252550\n",
       "7   0.927729  0.257623  0.930750  0.242524\n",
       "8   0.930958  0.245280  0.934500  0.231813\n",
       "9   0.934500  0.234711  0.936583  0.222583\n",
       "10  0.936771  0.224726  0.939833  0.214968\n",
       "11  0.938854  0.216001  0.941500  0.207793\n",
       "12  0.941542  0.208053  0.942500  0.203202\n",
       "13  0.943458  0.200775  0.945250  0.197216\n",
       "14  0.944646  0.193814  0.945750  0.191741\n",
       "15  0.946583  0.187981  0.948500  0.185989\n",
       "16  0.947917  0.182169  0.949667  0.181247\n",
       "17  0.949500  0.176587  0.951000  0.176851\n",
       "18  0.951146  0.171458  0.951000  0.174634\n",
       "19  0.951854  0.166915  0.951583  0.169702\n",
       "20  0.953625  0.162473  0.952000  0.168036\n",
       "21  0.954729  0.158285  0.953083  0.164542\n",
       "22  0.956125  0.154422  0.955583  0.160170\n",
       "23  0.957167  0.150538  0.955000  0.158339\n",
       "24  0.958208  0.146848  0.955917  0.155948\n",
       "25  0.959563  0.143507  0.957000  0.152450\n",
       "26  0.960125  0.140264  0.957750  0.151331\n",
       "27  0.960771  0.137393  0.957333  0.148312\n",
       "28  0.961792  0.134058  0.958917  0.145533\n",
       "29  0.962688  0.131144  0.959000  0.144314\n",
       "..       ...       ...       ...       ...\n",
       "70  0.980958  0.069380  0.968500  0.106574\n",
       "71  0.981313  0.068540  0.969417  0.106852\n",
       "72  0.981875  0.067690  0.968500  0.106990\n",
       "73  0.982146  0.066679  0.968583  0.108109\n",
       "74  0.982042  0.066030  0.969667  0.105615\n",
       "75  0.982354  0.065080  0.968750  0.105598\n",
       "76  0.982458  0.064194  0.968667  0.105960\n",
       "77  0.982896  0.063499  0.969583  0.104777\n",
       "78  0.982958  0.062665  0.969917  0.106523\n",
       "79  0.983458  0.061904  0.969583  0.104242\n",
       "80  0.983938  0.061034  0.969167  0.104972\n",
       "81  0.984250  0.060362  0.969083  0.105922\n",
       "82  0.984458  0.059731  0.970250  0.103545\n",
       "83  0.984042  0.059032  0.969500  0.103350\n",
       "84  0.984854  0.058228  0.969583  0.103803\n",
       "85  0.985125  0.057299  0.970167  0.102216\n",
       "86  0.985458  0.056787  0.970083  0.102654\n",
       "87  0.985396  0.056277  0.970500  0.101380\n",
       "88  0.985479  0.055150  0.970083  0.101449\n",
       "89  0.985833  0.054825  0.970167  0.101621\n",
       "90  0.985604  0.054173  0.970667  0.101352\n",
       "91  0.986063  0.053568  0.969667  0.102644\n",
       "92  0.986250  0.052825  0.970000  0.101544\n",
       "93  0.986458  0.052138  0.970417  0.101469\n",
       "94  0.986896  0.051587  0.970667  0.101061\n",
       "95  0.986917  0.050985  0.970500  0.101001\n",
       "96  0.986875  0.050397  0.971583  0.100219\n",
       "97  0.987375  0.049874  0.970583  0.100086\n",
       "98  0.987125  0.049272  0.970833  0.099310\n",
       "99  0.987417  0.048754  0.970833  0.100599\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "훈련결과 = pd.DataFrame(history.history)\n",
    "훈련결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ccd0a90>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSAn6/wY5rTfXxiiwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cd3fc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "훈련결과[['loss', 'val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "열제목 = boston.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.14455</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.172</td>\n",
       "      <td>96.1</td>\n",
       "      <td>5.9505</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>19.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.21124</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.631</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.0821</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.63</td>\n",
       "      <td>29.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.17004</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.004</td>\n",
       "      <td>85.9</td>\n",
       "      <td>6.5921</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.71</td>\n",
       "      <td>17.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.22489</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.377</td>\n",
       "      <td>94.3</td>\n",
       "      <td>6.3467</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>392.52</td>\n",
       "      <td>20.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.11747</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.009</td>\n",
       "      <td>82.9</td>\n",
       "      <td>6.2267</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.09378</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.889</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5.4509</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>390.50</td>\n",
       "      <td>15.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.62976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.949</td>\n",
       "      <td>61.8</td>\n",
       "      <td>4.7075</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.63796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.096</td>\n",
       "      <td>84.5</td>\n",
       "      <td>4.4619</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>380.02</td>\n",
       "      <td>10.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.62739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.834</td>\n",
       "      <td>56.5</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>395.62</td>\n",
       "      <td>8.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.05393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.935</td>\n",
       "      <td>29.3</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.85</td>\n",
       "      <td>6.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.78420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.990</td>\n",
       "      <td>81.7</td>\n",
       "      <td>4.2579</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.75</td>\n",
       "      <td>14.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.80271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.456</td>\n",
       "      <td>36.6</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>288.99</td>\n",
       "      <td>11.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.72580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.727</td>\n",
       "      <td>69.5</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>390.95</td>\n",
       "      <td>11.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.25179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.570</td>\n",
       "      <td>98.1</td>\n",
       "      <td>3.7979</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>376.57</td>\n",
       "      <td>21.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.85204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.965</td>\n",
       "      <td>89.2</td>\n",
       "      <td>4.0123</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>392.53</td>\n",
       "      <td>13.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.23247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.142</td>\n",
       "      <td>91.7</td>\n",
       "      <td>3.9769</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>18.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.98843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.813</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0952</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>394.54</td>\n",
       "      <td>19.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.75026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.924</td>\n",
       "      <td>94.1</td>\n",
       "      <td>4.3996</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>394.33</td>\n",
       "      <td>16.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.84054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.599</td>\n",
       "      <td>85.7</td>\n",
       "      <td>4.4546</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>303.42</td>\n",
       "      <td>16.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.67191</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.813</td>\n",
       "      <td>90.3</td>\n",
       "      <td>4.6820</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>376.88</td>\n",
       "      <td>14.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.95577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.047</td>\n",
       "      <td>88.8</td>\n",
       "      <td>4.4534</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>306.38</td>\n",
       "      <td>17.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.77299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.495</td>\n",
       "      <td>94.4</td>\n",
       "      <td>4.4547</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>387.94</td>\n",
       "      <td>12.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.00245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.674</td>\n",
       "      <td>87.3</td>\n",
       "      <td>4.2390</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>380.23</td>\n",
       "      <td>11.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>4.87141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.484</td>\n",
       "      <td>93.6</td>\n",
       "      <td>2.3053</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.21</td>\n",
       "      <td>18.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>15.02340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>5.304</td>\n",
       "      <td>97.3</td>\n",
       "      <td>2.1007</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>349.48</td>\n",
       "      <td>24.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>10.23300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.185</td>\n",
       "      <td>96.7</td>\n",
       "      <td>2.1705</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>379.70</td>\n",
       "      <td>18.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>14.33370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.229</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.9512</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>383.32</td>\n",
       "      <td>13.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>5.82401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>6.242</td>\n",
       "      <td>64.7</td>\n",
       "      <td>3.4242</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>10.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>5.70818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>6.750</td>\n",
       "      <td>74.9</td>\n",
       "      <td>3.3317</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>393.07</td>\n",
       "      <td>7.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>5.73116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>7.061</td>\n",
       "      <td>77.0</td>\n",
       "      <td>3.4106</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>395.28</td>\n",
       "      <td>7.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>2.81838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>5.762</td>\n",
       "      <td>40.3</td>\n",
       "      <td>4.0983</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>392.92</td>\n",
       "      <td>10.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>2.37857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>5.871</td>\n",
       "      <td>41.9</td>\n",
       "      <td>3.7240</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>370.73</td>\n",
       "      <td>13.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>3.67367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>6.312</td>\n",
       "      <td>51.9</td>\n",
       "      <td>3.9917</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>388.62</td>\n",
       "      <td>10.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>5.69175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>6.114</td>\n",
       "      <td>79.8</td>\n",
       "      <td>3.5459</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>392.68</td>\n",
       "      <td>14.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>4.83567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>5.905</td>\n",
       "      <td>53.2</td>\n",
       "      <td>3.1523</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>388.22</td>\n",
       "      <td>11.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>0.15086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.454</td>\n",
       "      <td>92.7</td>\n",
       "      <td>1.8209</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>395.09</td>\n",
       "      <td>18.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.18337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.414</td>\n",
       "      <td>98.3</td>\n",
       "      <td>1.7554</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>344.05</td>\n",
       "      <td>23.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0.20746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.093</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1.8226</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>318.43</td>\n",
       "      <td>29.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.10574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.983</td>\n",
       "      <td>98.8</td>\n",
       "      <td>1.8681</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>390.11</td>\n",
       "      <td>18.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0.11132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.983</td>\n",
       "      <td>83.5</td>\n",
       "      <td>2.1099</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.17331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.707</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>12.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0.27957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.926</td>\n",
       "      <td>42.6</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.17899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.670</td>\n",
       "      <td>28.8</td>\n",
       "      <td>2.7986</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>393.29</td>\n",
       "      <td>17.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.28960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.390</td>\n",
       "      <td>72.9</td>\n",
       "      <td>2.7986</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>21.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.26838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.794</td>\n",
       "      <td>70.6</td>\n",
       "      <td>2.8927</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.23912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>6.019</td>\n",
       "      <td>65.3</td>\n",
       "      <td>2.4091</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>12.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.17783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.569</td>\n",
       "      <td>73.5</td>\n",
       "      <td>2.3999</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>395.77</td>\n",
       "      <td>15.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.22438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>6.027</td>\n",
       "      <td>79.7</td>\n",
       "      <td>2.4982</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS   RAD    TAX  \\\n",
       "0     0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900   1.0  296.0   \n",
       "1     0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671   2.0  242.0   \n",
       "2     0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671   2.0  242.0   \n",
       "3     0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622   3.0  222.0   \n",
       "4     0.06905   0.0   2.18   0.0  0.458  7.147   54.2  6.0622   3.0  222.0   \n",
       "5     0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622   3.0  222.0   \n",
       "6     0.08829  12.5   7.87   0.0  0.524  6.012   66.6  5.5605   5.0  311.0   \n",
       "7     0.14455  12.5   7.87   0.0  0.524  6.172   96.1  5.9505   5.0  311.0   \n",
       "8     0.21124  12.5   7.87   0.0  0.524  5.631  100.0  6.0821   5.0  311.0   \n",
       "9     0.17004  12.5   7.87   0.0  0.524  6.004   85.9  6.5921   5.0  311.0   \n",
       "10    0.22489  12.5   7.87   0.0  0.524  6.377   94.3  6.3467   5.0  311.0   \n",
       "11    0.11747  12.5   7.87   0.0  0.524  6.009   82.9  6.2267   5.0  311.0   \n",
       "12    0.09378  12.5   7.87   0.0  0.524  5.889   39.0  5.4509   5.0  311.0   \n",
       "13    0.62976   0.0   8.14   0.0  0.538  5.949   61.8  4.7075   4.0  307.0   \n",
       "14    0.63796   0.0   8.14   0.0  0.538  6.096   84.5  4.4619   4.0  307.0   \n",
       "15    0.62739   0.0   8.14   0.0  0.538  5.834   56.5  4.4986   4.0  307.0   \n",
       "16    1.05393   0.0   8.14   0.0  0.538  5.935   29.3  4.4986   4.0  307.0   \n",
       "17    0.78420   0.0   8.14   0.0  0.538  5.990   81.7  4.2579   4.0  307.0   \n",
       "18    0.80271   0.0   8.14   0.0  0.538  5.456   36.6  3.7965   4.0  307.0   \n",
       "19    0.72580   0.0   8.14   0.0  0.538  5.727   69.5  3.7965   4.0  307.0   \n",
       "20    1.25179   0.0   8.14   0.0  0.538  5.570   98.1  3.7979   4.0  307.0   \n",
       "21    0.85204   0.0   8.14   0.0  0.538  5.965   89.2  4.0123   4.0  307.0   \n",
       "22    1.23247   0.0   8.14   0.0  0.538  6.142   91.7  3.9769   4.0  307.0   \n",
       "23    0.98843   0.0   8.14   0.0  0.538  5.813  100.0  4.0952   4.0  307.0   \n",
       "24    0.75026   0.0   8.14   0.0  0.538  5.924   94.1  4.3996   4.0  307.0   \n",
       "25    0.84054   0.0   8.14   0.0  0.538  5.599   85.7  4.4546   4.0  307.0   \n",
       "26    0.67191   0.0   8.14   0.0  0.538  5.813   90.3  4.6820   4.0  307.0   \n",
       "27    0.95577   0.0   8.14   0.0  0.538  6.047   88.8  4.4534   4.0  307.0   \n",
       "28    0.77299   0.0   8.14   0.0  0.538  6.495   94.4  4.4547   4.0  307.0   \n",
       "29    1.00245   0.0   8.14   0.0  0.538  6.674   87.3  4.2390   4.0  307.0   \n",
       "..        ...   ...    ...   ...    ...    ...    ...     ...   ...    ...   \n",
       "476   4.87141   0.0  18.10   0.0  0.614  6.484   93.6  2.3053  24.0  666.0   \n",
       "477  15.02340   0.0  18.10   0.0  0.614  5.304   97.3  2.1007  24.0  666.0   \n",
       "478  10.23300   0.0  18.10   0.0  0.614  6.185   96.7  2.1705  24.0  666.0   \n",
       "479  14.33370   0.0  18.10   0.0  0.614  6.229   88.0  1.9512  24.0  666.0   \n",
       "480   5.82401   0.0  18.10   0.0  0.532  6.242   64.7  3.4242  24.0  666.0   \n",
       "481   5.70818   0.0  18.10   0.0  0.532  6.750   74.9  3.3317  24.0  666.0   \n",
       "482   5.73116   0.0  18.10   0.0  0.532  7.061   77.0  3.4106  24.0  666.0   \n",
       "483   2.81838   0.0  18.10   0.0  0.532  5.762   40.3  4.0983  24.0  666.0   \n",
       "484   2.37857   0.0  18.10   0.0  0.583  5.871   41.9  3.7240  24.0  666.0   \n",
       "485   3.67367   0.0  18.10   0.0  0.583  6.312   51.9  3.9917  24.0  666.0   \n",
       "486   5.69175   0.0  18.10   0.0  0.583  6.114   79.8  3.5459  24.0  666.0   \n",
       "487   4.83567   0.0  18.10   0.0  0.583  5.905   53.2  3.1523  24.0  666.0   \n",
       "488   0.15086   0.0  27.74   0.0  0.609  5.454   92.7  1.8209   4.0  711.0   \n",
       "489   0.18337   0.0  27.74   0.0  0.609  5.414   98.3  1.7554   4.0  711.0   \n",
       "490   0.20746   0.0  27.74   0.0  0.609  5.093   98.0  1.8226   4.0  711.0   \n",
       "491   0.10574   0.0  27.74   0.0  0.609  5.983   98.8  1.8681   4.0  711.0   \n",
       "492   0.11132   0.0  27.74   0.0  0.609  5.983   83.5  2.1099   4.0  711.0   \n",
       "493   0.17331   0.0   9.69   0.0  0.585  5.707   54.0  2.3817   6.0  391.0   \n",
       "494   0.27957   0.0   9.69   0.0  0.585  5.926   42.6  2.3817   6.0  391.0   \n",
       "495   0.17899   0.0   9.69   0.0  0.585  5.670   28.8  2.7986   6.0  391.0   \n",
       "496   0.28960   0.0   9.69   0.0  0.585  5.390   72.9  2.7986   6.0  391.0   \n",
       "497   0.26838   0.0   9.69   0.0  0.585  5.794   70.6  2.8927   6.0  391.0   \n",
       "498   0.23912   0.0   9.69   0.0  0.585  6.019   65.3  2.4091   6.0  391.0   \n",
       "499   0.17783   0.0   9.69   0.0  0.585  5.569   73.5  2.3999   6.0  391.0   \n",
       "500   0.22438   0.0   9.69   0.0  0.585  6.027   79.7  2.4982   6.0  391.0   \n",
       "501   0.06263   0.0  11.93   0.0  0.573  6.593   69.1  2.4786   1.0  273.0   \n",
       "502   0.04527   0.0  11.93   0.0  0.573  6.120   76.7  2.2875   1.0  273.0   \n",
       "503   0.06076   0.0  11.93   0.0  0.573  6.976   91.0  2.1675   1.0  273.0   \n",
       "504   0.10959   0.0  11.93   0.0  0.573  6.794   89.3  2.3889   1.0  273.0   \n",
       "505   0.04741   0.0  11.93   0.0  0.573  6.030   80.8  2.5050   1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  \n",
       "0       15.3  396.90   4.98  \n",
       "1       17.8  396.90   9.14  \n",
       "2       17.8  392.83   4.03  \n",
       "3       18.7  394.63   2.94  \n",
       "4       18.7  396.90   5.33  \n",
       "5       18.7  394.12   5.21  \n",
       "6       15.2  395.60  12.43  \n",
       "7       15.2  396.90  19.15  \n",
       "8       15.2  386.63  29.93  \n",
       "9       15.2  386.71  17.10  \n",
       "10      15.2  392.52  20.45  \n",
       "11      15.2  396.90  13.27  \n",
       "12      15.2  390.50  15.71  \n",
       "13      21.0  396.90   8.26  \n",
       "14      21.0  380.02  10.26  \n",
       "15      21.0  395.62   8.47  \n",
       "16      21.0  386.85   6.58  \n",
       "17      21.0  386.75  14.67  \n",
       "18      21.0  288.99  11.69  \n",
       "19      21.0  390.95  11.28  \n",
       "20      21.0  376.57  21.02  \n",
       "21      21.0  392.53  13.83  \n",
       "22      21.0  396.90  18.72  \n",
       "23      21.0  394.54  19.88  \n",
       "24      21.0  394.33  16.30  \n",
       "25      21.0  303.42  16.51  \n",
       "26      21.0  376.88  14.81  \n",
       "27      21.0  306.38  17.28  \n",
       "28      21.0  387.94  12.80  \n",
       "29      21.0  380.23  11.98  \n",
       "..       ...     ...    ...  \n",
       "476     20.2  396.21  18.68  \n",
       "477     20.2  349.48  24.91  \n",
       "478     20.2  379.70  18.03  \n",
       "479     20.2  383.32  13.11  \n",
       "480     20.2  396.90  10.74  \n",
       "481     20.2  393.07   7.74  \n",
       "482     20.2  395.28   7.01  \n",
       "483     20.2  392.92  10.42  \n",
       "484     20.2  370.73  13.34  \n",
       "485     20.2  388.62  10.58  \n",
       "486     20.2  392.68  14.98  \n",
       "487     20.2  388.22  11.45  \n",
       "488     20.1  395.09  18.06  \n",
       "489     20.1  344.05  23.97  \n",
       "490     20.1  318.43  29.68  \n",
       "491     20.1  390.11  18.07  \n",
       "492     20.1  396.90  13.35  \n",
       "493     19.2  396.90  12.01  \n",
       "494     19.2  396.90  13.59  \n",
       "495     19.2  393.29  17.60  \n",
       "496     19.2  396.90  21.14  \n",
       "497     19.2  396.90  14.10  \n",
       "498     19.2  396.90  12.92  \n",
       "499     19.2  395.77  15.10  \n",
       "500     19.2  396.90  14.33  \n",
       "501     21.0  391.99   9.67  \n",
       "502     21.0  396.90   9.08  \n",
       "503     21.0  396.90   5.64  \n",
       "504     21.0  393.45   6.48  \n",
       "505     21.0  396.90   7.88  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X, columns=열제목)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_std_train = scaler.transform(X_train)\n",
    "X_std_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.413890</td>\n",
       "      <td>2.836914</td>\n",
       "      <td>-1.363836</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-1.219834</td>\n",
       "      <td>-0.482145</td>\n",
       "      <td>-1.732911</td>\n",
       "      <td>3.190834</td>\n",
       "      <td>-0.643265</td>\n",
       "      <td>-0.449941</td>\n",
       "      <td>1.647059</td>\n",
       "      <td>0.219588</td>\n",
       "      <td>-0.999149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.111989</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>1.018645</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>0.512798</td>\n",
       "      <td>1.043419</td>\n",
       "      <td>-0.027752</td>\n",
       "      <td>-0.594081</td>\n",
       "      <td>1.637346</td>\n",
       "      <td>1.528227</td>\n",
       "      <td>0.803435</td>\n",
       "      <td>0.204682</td>\n",
       "      <td>-0.144954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.156250</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>1.236438</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>2.728154</td>\n",
       "      <td>-1.578096</td>\n",
       "      <td>0.901045</td>\n",
       "      <td>-1.065644</td>\n",
       "      <td>-0.529234</td>\n",
       "      <td>-0.038816</td>\n",
       "      <td>-1.774308</td>\n",
       "      <td>0.010782</td>\n",
       "      <td>2.191808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.421945</td>\n",
       "      <td>0.344441</td>\n",
       "      <td>-1.154872</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-0.962094</td>\n",
       "      <td>1.025884</td>\n",
       "      <td>-1.105441</td>\n",
       "      <td>0.680499</td>\n",
       "      <td>-0.529234</td>\n",
       "      <td>-1.153025</td>\n",
       "      <td>-1.680572</td>\n",
       "      <td>0.397046</td>\n",
       "      <td>-1.136606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.419747</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>-1.282899</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-0.573330</td>\n",
       "      <td>2.286958</td>\n",
       "      <td>-0.524056</td>\n",
       "      <td>-0.280912</td>\n",
       "      <td>-0.757295</td>\n",
       "      <td>-1.290067</td>\n",
       "      <td>-0.321399</td>\n",
       "      <td>0.401431</td>\n",
       "      <td>-1.156242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.244049</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>1.018645</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>1.598926</td>\n",
       "      <td>0.285021</td>\n",
       "      <td>0.883320</td>\n",
       "      <td>-0.843328</td>\n",
       "      <td>1.637346</td>\n",
       "      <td>1.528227</td>\n",
       "      <td>0.803435</td>\n",
       "      <td>-3.600853</td>\n",
       "      <td>0.751319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.151891</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>1.236438</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>2.728154</td>\n",
       "      <td>-1.958026</td>\n",
       "      <td>0.968401</td>\n",
       "      <td>-1.097981</td>\n",
       "      <td>-0.529234</td>\n",
       "      <td>-0.038816</td>\n",
       "      <td>-1.774308</td>\n",
       "      <td>0.391347</td>\n",
       "      <td>2.361524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.411956</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>-0.171860</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-0.064746</td>\n",
       "      <td>-0.515754</td>\n",
       "      <td>0.869140</td>\n",
       "      <td>-0.678448</td>\n",
       "      <td>-0.415204</td>\n",
       "      <td>0.133976</td>\n",
       "      <td>-0.321399</td>\n",
       "      <td>0.426860</td>\n",
       "      <td>0.493237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.410996</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>-0.171860</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-0.064746</td>\n",
       "      <td>-0.131441</td>\n",
       "      <td>0.145954</td>\n",
       "      <td>-0.501394</td>\n",
       "      <td>-0.415204</td>\n",
       "      <td>0.133976</td>\n",
       "      <td>-0.321399</td>\n",
       "      <td>0.408775</td>\n",
       "      <td>-0.091655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.400247</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>-0.218950</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>0.262816</td>\n",
       "      <td>-0.349170</td>\n",
       "      <td>0.401196</td>\n",
       "      <td>-0.610390</td>\n",
       "      <td>-0.415204</td>\n",
       "      <td>-0.110316</td>\n",
       "      <td>0.334754</td>\n",
       "      <td>0.448234</td>\n",
       "      <td>0.229545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.019654</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>1.018645</td>\n",
       "      <td>3.762978</td>\n",
       "      <td>1.857528</td>\n",
       "      <td>0.188577</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-0.607100</td>\n",
       "      <td>1.637346</td>\n",
       "      <td>1.528227</td>\n",
       "      <td>0.803435</td>\n",
       "      <td>0.387291</td>\n",
       "      <td>0.080867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.379013</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>-0.732530</td>\n",
       "      <td>3.762978</td>\n",
       "      <td>-0.409548</td>\n",
       "      <td>-0.148976</td>\n",
       "      <td>0.812419</td>\n",
       "      <td>-0.351977</td>\n",
       "      <td>-0.187142</td>\n",
       "      <td>-0.610816</td>\n",
       "      <td>-0.508871</td>\n",
       "      <td>0.430039</td>\n",
       "      <td>1.229611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.421472</td>\n",
       "      <td>0.552147</td>\n",
       "      <td>-0.929721</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-1.107773</td>\n",
       "      <td>0.673718</td>\n",
       "      <td>-1.236607</td>\n",
       "      <td>0.753820</td>\n",
       "      <td>-0.643265</td>\n",
       "      <td>-0.765733</td>\n",
       "      <td>0.241018</td>\n",
       "      <td>0.448234</td>\n",
       "      <td>-1.038422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.085973</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>1.018645</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>0.254196</td>\n",
       "      <td>-0.384240</td>\n",
       "      <td>0.925860</td>\n",
       "      <td>-0.590603</td>\n",
       "      <td>1.637346</td>\n",
       "      <td>1.528227</td>\n",
       "      <td>0.803435</td>\n",
       "      <td>-0.270914</td>\n",
       "      <td>1.209975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.059813</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>1.018645</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>0.866221</td>\n",
       "      <td>-0.740790</td>\n",
       "      <td>-0.715488</td>\n",
       "      <td>-0.343282</td>\n",
       "      <td>1.637346</td>\n",
       "      <td>1.528227</td>\n",
       "      <td>0.803435</td>\n",
       "      <td>-0.236826</td>\n",
       "      <td>0.201492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.675677</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>1.018645</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>1.366184</td>\n",
       "      <td>0.675179</td>\n",
       "      <td>0.911680</td>\n",
       "      <td>-0.611377</td>\n",
       "      <td>1.637346</td>\n",
       "      <td>1.528227</td>\n",
       "      <td>0.803435</td>\n",
       "      <td>-3.828950</td>\n",
       "      <td>0.843891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.414318</td>\n",
       "      <td>0.759853</td>\n",
       "      <td>-0.919420</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-1.090533</td>\n",
       "      <td>0.134510</td>\n",
       "      <td>-0.548871</td>\n",
       "      <td>1.522196</td>\n",
       "      <td>-0.415204</td>\n",
       "      <td>-0.652524</td>\n",
       "      <td>-0.883815</td>\n",
       "      <td>0.183527</td>\n",
       "      <td>-0.206670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.419795</td>\n",
       "      <td>2.421502</td>\n",
       "      <td>-1.315274</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-1.331895</td>\n",
       "      <td>1.132556</td>\n",
       "      <td>-2.069688</td>\n",
       "      <td>1.894587</td>\n",
       "      <td>-0.529234</td>\n",
       "      <td>-0.306941</td>\n",
       "      <td>-1.727440</td>\n",
       "      <td>0.170703</td>\n",
       "      <td>-1.115566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.419198</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>0.110683</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>0.159376</td>\n",
       "      <td>1.037574</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-0.765823</td>\n",
       "      <td>-0.985356</td>\n",
       "      <td>-0.813400</td>\n",
       "      <td>1.178379</td>\n",
       "      <td>0.448234</td>\n",
       "      <td>-0.989331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.295750</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>1.236438</td>\n",
       "      <td>3.762978</td>\n",
       "      <td>2.728154</td>\n",
       "      <td>-1.832357</td>\n",
       "      <td>0.695434</td>\n",
       "      <td>-1.027761</td>\n",
       "      <td>-0.529234</td>\n",
       "      <td>-0.038816</td>\n",
       "      <td>-1.774308</td>\n",
       "      <td>-0.139492</td>\n",
       "      <td>-0.080434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.414110</td>\n",
       "      <td>1.175266</td>\n",
       "      <td>-0.701627</td>\n",
       "      <td>3.762978</td>\n",
       "      <td>-0.926752</td>\n",
       "      <td>1.462803</td>\n",
       "      <td>-0.687127</td>\n",
       "      <td>0.465468</td>\n",
       "      <td>-0.643265</td>\n",
       "      <td>-0.926608</td>\n",
       "      <td>-0.415135</td>\n",
       "      <td>0.364383</td>\n",
       "      <td>-0.931823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.395035</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>-0.188047</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-0.090606</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.511092</td>\n",
       "      <td>-0.251019</td>\n",
       "      <td>-0.643265</td>\n",
       "      <td>-0.628691</td>\n",
       "      <td>-0.040190</td>\n",
       "      <td>0.409761</td>\n",
       "      <td>-0.672339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.419139</td>\n",
       "      <td>1.175266</td>\n",
       "      <td>-0.701627</td>\n",
       "      <td>3.762978</td>\n",
       "      <td>-0.926752</td>\n",
       "      <td>0.818383</td>\n",
       "      <td>-1.445764</td>\n",
       "      <td>0.501001</td>\n",
       "      <td>-0.643265</td>\n",
       "      <td>-0.926608</td>\n",
       "      <td>-0.415135</td>\n",
       "      <td>0.410419</td>\n",
       "      <td>-1.196918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.262477</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>1.236438</td>\n",
       "      <td>3.762978</td>\n",
       "      <td>2.728154</td>\n",
       "      <td>-0.200120</td>\n",
       "      <td>0.979036</td>\n",
       "      <td>-0.962335</td>\n",
       "      <td>-0.529234</td>\n",
       "      <td>-0.038816</td>\n",
       "      <td>-1.774308</td>\n",
       "      <td>-0.383483</td>\n",
       "      <td>0.340351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.041512</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>1.018645</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-0.194047</td>\n",
       "      <td>-0.053994</td>\n",
       "      <td>0.791149</td>\n",
       "      <td>-0.327866</td>\n",
       "      <td>1.637346</td>\n",
       "      <td>1.528227</td>\n",
       "      <td>0.803435</td>\n",
       "      <td>0.431025</td>\n",
       "      <td>0.024762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.409493</td>\n",
       "      <td>0.032882</td>\n",
       "      <td>-0.486777</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-0.263007</td>\n",
       "      <td>-0.137286</td>\n",
       "      <td>0.982581</td>\n",
       "      <td>1.012233</td>\n",
       "      <td>-0.529234</td>\n",
       "      <td>-0.586983</td>\n",
       "      <td>-1.539968</td>\n",
       "      <td>0.448234</td>\n",
       "      <td>0.905607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.389713</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>-0.732530</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-0.435409</td>\n",
       "      <td>2.922610</td>\n",
       "      <td>0.351566</td>\n",
       "      <td>-0.424171</td>\n",
       "      <td>-0.187142</td>\n",
       "      <td>-0.610816</td>\n",
       "      <td>-0.508871</td>\n",
       "      <td>0.318347</td>\n",
       "      <td>-1.199723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.237599</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>1.576372</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>0.598999</td>\n",
       "      <td>-1.822128</td>\n",
       "      <td>1.120837</td>\n",
       "      <td>-1.108039</td>\n",
       "      <td>-0.643265</td>\n",
       "      <td>0.163768</td>\n",
       "      <td>1.272115</td>\n",
       "      <td>0.448234</td>\n",
       "      <td>3.046002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.150739</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>1.018645</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>0.245576</td>\n",
       "      <td>-0.577128</td>\n",
       "      <td>-0.938824</td>\n",
       "      <td>-0.034249</td>\n",
       "      <td>1.637346</td>\n",
       "      <td>1.528227</td>\n",
       "      <td>0.803435</td>\n",
       "      <td>0.161386</td>\n",
       "      <td>0.090685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.422180</td>\n",
       "      <td>2.836914</td>\n",
       "      <td>-0.916477</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-1.237074</td>\n",
       "      <td>0.869528</td>\n",
       "      <td>-1.435128</td>\n",
       "      <td>0.620337</td>\n",
       "      <td>-0.643265</td>\n",
       "      <td>-0.980233</td>\n",
       "      <td>0.334754</td>\n",
       "      <td>0.448234</td>\n",
       "      <td>-1.313335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>-0.412995</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>-0.385238</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-0.297488</td>\n",
       "      <td>0.752626</td>\n",
       "      <td>0.103414</td>\n",
       "      <td>-0.442173</td>\n",
       "      <td>-0.529234</td>\n",
       "      <td>-0.152024</td>\n",
       "      <td>1.131511</td>\n",
       "      <td>0.433766</td>\n",
       "      <td>-0.704599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>-0.406348</td>\n",
       "      <td>0.552147</td>\n",
       "      <td>-0.889989</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-0.875032</td>\n",
       "      <td>-0.438307</td>\n",
       "      <td>0.886865</td>\n",
       "      <td>1.420204</td>\n",
       "      <td>-0.187142</td>\n",
       "      <td>-0.747858</td>\n",
       "      <td>0.569094</td>\n",
       "      <td>0.241949</td>\n",
       "      <td>0.244973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>-0.382711</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>-0.086508</td>\n",
       "      <td>3.762978</td>\n",
       "      <td>-0.564710</td>\n",
       "      <td>-1.259540</td>\n",
       "      <td>0.716704</td>\n",
       "      <td>-0.061980</td>\n",
       "      <td>-0.643265</td>\n",
       "      <td>-0.789566</td>\n",
       "      <td>0.053546</td>\n",
       "      <td>0.430039</td>\n",
       "      <td>1.583071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>-0.419696</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>0.243124</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-1.012953</td>\n",
       "      <td>-0.049610</td>\n",
       "      <td>-0.520511</td>\n",
       "      <td>0.572114</td>\n",
       "      <td>-0.529234</td>\n",
       "      <td>-0.068607</td>\n",
       "      <td>0.100414</td>\n",
       "      <td>0.333144</td>\n",
       "      <td>-0.049576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>-0.422906</td>\n",
       "      <td>0.676771</td>\n",
       "      <td>0.568343</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-0.780211</td>\n",
       "      <td>-0.080296</td>\n",
       "      <td>-1.399678</td>\n",
       "      <td>-0.061557</td>\n",
       "      <td>-0.643265</td>\n",
       "      <td>-0.831275</td>\n",
       "      <td>-0.133926</td>\n",
       "      <td>0.441986</td>\n",
       "      <td>-0.909381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>0.828632</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>1.018645</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>1.073102</td>\n",
       "      <td>0.754088</td>\n",
       "      <td>0.794694</td>\n",
       "      <td>-0.929387</td>\n",
       "      <td>1.637346</td>\n",
       "      <td>1.528227</td>\n",
       "      <td>0.803435</td>\n",
       "      <td>-3.665742</td>\n",
       "      <td>1.836945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>-0.411112</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>-0.171860</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-0.064746</td>\n",
       "      <td>-0.575666</td>\n",
       "      <td>0.167224</td>\n",
       "      <td>-0.620119</td>\n",
       "      <td>-0.415204</td>\n",
       "      <td>0.133976</td>\n",
       "      <td>-0.321399</td>\n",
       "      <td>-0.190461</td>\n",
       "      <td>0.375417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>1.121159</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>1.018645</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>1.193783</td>\n",
       "      <td>-0.553747</td>\n",
       "      <td>0.932950</td>\n",
       "      <td>-0.946966</td>\n",
       "      <td>1.637346</td>\n",
       "      <td>1.528227</td>\n",
       "      <td>0.803435</td>\n",
       "      <td>0.448234</td>\n",
       "      <td>0.512873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>-0.422704</td>\n",
       "      <td>1.798384</td>\n",
       "      <td>-1.088651</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-0.607810</td>\n",
       "      <td>0.888524</td>\n",
       "      <td>-1.428038</td>\n",
       "      <td>1.254242</td>\n",
       "      <td>-0.529234</td>\n",
       "      <td>-0.235441</td>\n",
       "      <td>-0.415135</td>\n",
       "      <td>0.350353</td>\n",
       "      <td>-1.133800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>-0.404075</td>\n",
       "      <td>0.427524</td>\n",
       "      <td>-0.782564</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-1.064673</td>\n",
       "      <td>-0.965825</td>\n",
       "      <td>0.064418</td>\n",
       "      <td>1.954326</td>\n",
       "      <td>-0.301173</td>\n",
       "      <td>-0.473774</td>\n",
       "      <td>0.287886</td>\n",
       "      <td>0.363067</td>\n",
       "      <td>0.808826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>-0.395151</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>-0.218950</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>0.262816</td>\n",
       "      <td>-0.689645</td>\n",
       "      <td>0.078599</td>\n",
       "      <td>-0.424970</td>\n",
       "      <td>-0.415204</td>\n",
       "      <td>-0.110316</td>\n",
       "      <td>0.334754</td>\n",
       "      <td>0.448234</td>\n",
       "      <td>0.197284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>-0.269326</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>-0.447044</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-0.142327</td>\n",
       "      <td>-0.283413</td>\n",
       "      <td>1.120837</td>\n",
       "      <td>0.177726</td>\n",
       "      <td>-0.643265</td>\n",
       "      <td>-0.610816</td>\n",
       "      <td>1.178379</td>\n",
       "      <td>0.227151</td>\n",
       "      <td>0.048607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>-0.424455</td>\n",
       "      <td>3.252326</td>\n",
       "      <td>-1.093066</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-1.383615</td>\n",
       "      <td>1.736060</td>\n",
       "      <td>-1.211792</td>\n",
       "      <td>1.193470</td>\n",
       "      <td>-0.757295</td>\n",
       "      <td>-0.986191</td>\n",
       "      <td>-1.211891</td>\n",
       "      <td>0.332486</td>\n",
       "      <td>-1.344193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>5.496553</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>1.018645</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>0.366257</td>\n",
       "      <td>-0.743712</td>\n",
       "      <td>1.120837</td>\n",
       "      <td>-1.120447</td>\n",
       "      <td>1.637346</td>\n",
       "      <td>1.528227</td>\n",
       "      <td>0.803435</td>\n",
       "      <td>-3.873671</td>\n",
       "      <td>-0.362360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.018165</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>1.018645</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>1.857528</td>\n",
       "      <td>-0.021846</td>\n",
       "      <td>0.805329</td>\n",
       "      <td>-0.705662</td>\n",
       "      <td>1.637346</td>\n",
       "      <td>1.528227</td>\n",
       "      <td>0.803435</td>\n",
       "      <td>-0.058710</td>\n",
       "      <td>0.209908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>-0.420014</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>0.403526</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-1.012953</td>\n",
       "      <td>0.413612</td>\n",
       "      <td>-0.616227</td>\n",
       "      <td>1.016886</td>\n",
       "      <td>-0.643265</td>\n",
       "      <td>-0.718066</td>\n",
       "      <td>-1.165023</td>\n",
       "      <td>0.403842</td>\n",
       "      <td>-0.743872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>-0.406381</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>-0.628048</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-0.918132</td>\n",
       "      <td>-0.853307</td>\n",
       "      <td>-1.225972</td>\n",
       "      <td>0.612676</td>\n",
       "      <td>-0.757295</td>\n",
       "      <td>-1.051733</td>\n",
       "      <td>-0.274531</td>\n",
       "      <td>0.448234</td>\n",
       "      <td>-0.348334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>1.246076</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>1.018645</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>0.366257</td>\n",
       "      <td>0.856376</td>\n",
       "      <td>1.120837</td>\n",
       "      <td>-1.095772</td>\n",
       "      <td>1.637346</td>\n",
       "      <td>1.528227</td>\n",
       "      <td>0.803435</td>\n",
       "      <td>-1.936212</td>\n",
       "      <td>0.993972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>0.860356</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>1.018645</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>0.978281</td>\n",
       "      <td>-1.987251</td>\n",
       "      <td>1.120837</td>\n",
       "      <td>-1.232686</td>\n",
       "      <td>1.637346</td>\n",
       "      <td>1.528227</td>\n",
       "      <td>0.803435</td>\n",
       "      <td>0.448234</td>\n",
       "      <td>3.096496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>-0.422049</td>\n",
       "      <td>2.836914</td>\n",
       "      <td>-0.916477</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-1.237074</td>\n",
       "      <td>0.531975</td>\n",
       "      <td>-1.594655</td>\n",
       "      <td>0.620337</td>\n",
       "      <td>-0.643265</td>\n",
       "      <td>-0.980233</td>\n",
       "      <td>0.334754</td>\n",
       "      <td>0.448234</td>\n",
       "      <td>-1.121177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>0.655438</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>1.018645</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>1.073102</td>\n",
       "      <td>0.166658</td>\n",
       "      <td>0.964856</td>\n",
       "      <td>-0.859497</td>\n",
       "      <td>1.637346</td>\n",
       "      <td>1.528227</td>\n",
       "      <td>0.803435</td>\n",
       "      <td>-3.236620</td>\n",
       "      <td>1.597097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>0.654303</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>1.018645</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>1.366184</td>\n",
       "      <td>-0.118289</td>\n",
       "      <td>1.074752</td>\n",
       "      <td>-0.721595</td>\n",
       "      <td>1.637346</td>\n",
       "      <td>1.528227</td>\n",
       "      <td>0.803435</td>\n",
       "      <td>0.448234</td>\n",
       "      <td>0.762540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>-0.412911</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>-1.219622</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-0.943992</td>\n",
       "      <td>-0.150437</td>\n",
       "      <td>0.043148</td>\n",
       "      <td>-0.141788</td>\n",
       "      <td>-0.871326</td>\n",
       "      <td>-0.795525</td>\n",
       "      <td>-0.227662</td>\n",
       "      <td>0.392662</td>\n",
       "      <td>-0.189838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>-0.349385</td>\n",
       "      <td>0.344441</td>\n",
       "      <td>-1.060691</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>0.797260</td>\n",
       "      <td>1.559246</td>\n",
       "      <td>1.120837</td>\n",
       "      <td>-0.894090</td>\n",
       "      <td>-0.529234</td>\n",
       "      <td>-0.867025</td>\n",
       "      <td>-2.571065</td>\n",
       "      <td>0.299055</td>\n",
       "      <td>-0.687768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>0.427057</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>1.018645</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>1.073102</td>\n",
       "      <td>-0.106599</td>\n",
       "      <td>0.344476</td>\n",
       "      <td>-0.874819</td>\n",
       "      <td>1.637346</td>\n",
       "      <td>1.528227</td>\n",
       "      <td>0.803435</td>\n",
       "      <td>-2.841916</td>\n",
       "      <td>1.238027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>-0.424497</td>\n",
       "      <td>2.836914</td>\n",
       "      <td>-1.349120</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-1.030193</td>\n",
       "      <td>0.539281</td>\n",
       "      <td>-1.371318</td>\n",
       "      <td>2.137208</td>\n",
       "      <td>-0.643265</td>\n",
       "      <td>-0.771691</td>\n",
       "      <td>-0.696343</td>\n",
       "      <td>0.382907</td>\n",
       "      <td>-0.940239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>-0.414644</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>-1.282899</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>-0.573330</td>\n",
       "      <td>0.434070</td>\n",
       "      <td>0.964856</td>\n",
       "      <td>-0.446450</td>\n",
       "      <td>-0.757295</td>\n",
       "      <td>-1.290067</td>\n",
       "      <td>-0.321399</td>\n",
       "      <td>0.448234</td>\n",
       "      <td>-0.983720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>-0.365940</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>-0.732530</td>\n",
       "      <td>3.762978</td>\n",
       "      <td>-0.409548</td>\n",
       "      <td>0.533436</td>\n",
       "      <td>0.287755</td>\n",
       "      <td>0.165036</td>\n",
       "      <td>-0.187142</td>\n",
       "      <td>-0.610816</td>\n",
       "      <td>-0.508871</td>\n",
       "      <td>0.355614</td>\n",
       "      <td>-0.442310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>0.247997</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>1.018645</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>1.366184</td>\n",
       "      <td>0.361006</td>\n",
       "      <td>0.762789</td>\n",
       "      <td>-0.467788</td>\n",
       "      <td>1.637346</td>\n",
       "      <td>1.528227</td>\n",
       "      <td>0.803435</td>\n",
       "      <td>0.414474</td>\n",
       "      <td>-0.337113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>-0.388866</td>\n",
       "      <td>-0.486383</td>\n",
       "      <td>1.576372</td>\n",
       "      <td>-0.265747</td>\n",
       "      <td>0.598999</td>\n",
       "      <td>-0.473378</td>\n",
       "      <td>0.890410</td>\n",
       "      <td>-0.860108</td>\n",
       "      <td>-0.643265</td>\n",
       "      <td>0.163768</td>\n",
       "      <td>1.272115</td>\n",
       "      <td>0.243812</td>\n",
       "      <td>0.590017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>379 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
       "0   -0.413890  2.836914 -1.363836 -0.265747 -1.219834 -0.482145 -1.732911   \n",
       "1    0.111989 -0.486383  1.018645 -0.265747  0.512798  1.043419 -0.027752   \n",
       "2   -0.156250 -0.486383  1.236438 -0.265747  2.728154 -1.578096  0.901045   \n",
       "3   -0.421945  0.344441 -1.154872 -0.265747 -0.962094  1.025884 -1.105441   \n",
       "4   -0.419747 -0.486383 -1.282899 -0.265747 -0.573330  2.286958 -0.524056   \n",
       "5    1.244049 -0.486383  1.018645 -0.265747  1.598926  0.285021  0.883320   \n",
       "6   -0.151891 -0.486383  1.236438 -0.265747  2.728154 -1.958026  0.968401   \n",
       "7   -0.411956 -0.486383 -0.171860 -0.265747 -0.064746 -0.515754  0.869140   \n",
       "8   -0.410996 -0.486383 -0.171860 -0.265747 -0.064746 -0.131441  0.145954   \n",
       "9   -0.400247 -0.486383 -0.218950 -0.265747  0.262816 -0.349170  0.401196   \n",
       "10   0.019654 -0.486383  1.018645  3.762978  1.857528  0.188577  0.801784   \n",
       "11  -0.379013 -0.486383 -0.732530  3.762978 -0.409548 -0.148976  0.812419   \n",
       "12  -0.421472  0.552147 -0.929721 -0.265747 -1.107773  0.673718 -1.236607   \n",
       "13   0.085973 -0.486383  1.018645 -0.265747  0.254196 -0.384240  0.925860   \n",
       "14  -0.059813 -0.486383  1.018645 -0.265747  0.866221 -0.740790 -0.715488   \n",
       "15   0.675677 -0.486383  1.018645 -0.265747  1.366184  0.675179  0.911680   \n",
       "16  -0.414318  0.759853 -0.919420 -0.265747 -1.090533  0.134510 -0.548871   \n",
       "17  -0.419795  2.421502 -1.315274 -0.265747 -1.331895  1.132556 -2.069688   \n",
       "18  -0.419198 -0.486383  0.110683 -0.265747  0.159376  1.037574  0.801784   \n",
       "19  -0.295750 -0.486383  1.236438  3.762978  2.728154 -1.832357  0.695434   \n",
       "20  -0.414110  1.175266 -0.701627  3.762978 -0.926752  1.462803 -0.687127   \n",
       "21  -0.395035 -0.486383 -0.188047 -0.265747 -0.090606  0.000073  0.511092   \n",
       "22  -0.419139  1.175266 -0.701627  3.762978 -0.926752  0.818383 -1.445764   \n",
       "23  -0.262477 -0.486383  1.236438  3.762978  2.728154 -0.200120  0.979036   \n",
       "24   0.041512 -0.486383  1.018645 -0.265747 -0.194047 -0.053994  0.791149   \n",
       "25  -0.409493  0.032882 -0.486777 -0.265747 -0.263007 -0.137286  0.982581   \n",
       "26  -0.389713 -0.486383 -0.732530 -0.265747 -0.435409  2.922610  0.351566   \n",
       "27  -0.237599 -0.486383  1.576372 -0.265747  0.598999 -1.822128  1.120837   \n",
       "28  -0.150739 -0.486383  1.018645 -0.265747  0.245576 -0.577128 -0.938824   \n",
       "29  -0.422180  2.836914 -0.916477 -0.265747 -1.237074  0.869528 -1.435128   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "349 -0.412995 -0.486383 -0.385238 -0.265747 -0.297488  0.752626  0.103414   \n",
       "350 -0.406348  0.552147 -0.889989 -0.265747 -0.875032 -0.438307  0.886865   \n",
       "351 -0.382711 -0.486383 -0.086508  3.762978 -0.564710 -1.259540  0.716704   \n",
       "352 -0.419696 -0.486383  0.243124 -0.265747 -1.012953 -0.049610 -0.520511   \n",
       "353 -0.422906  0.676771  0.568343 -0.265747 -0.780211 -0.080296 -1.399678   \n",
       "354  0.828632 -0.486383  1.018645 -0.265747  1.073102  0.754088  0.794694   \n",
       "355 -0.411112 -0.486383 -0.171860 -0.265747 -0.064746 -0.575666  0.167224   \n",
       "356  1.121159 -0.486383  1.018645 -0.265747  1.193783 -0.553747  0.932950   \n",
       "357 -0.422704  1.798384 -1.088651 -0.265747 -0.607810  0.888524 -1.428038   \n",
       "358 -0.404075  0.427524 -0.782564 -0.265747 -1.064673 -0.965825  0.064418   \n",
       "359 -0.395151 -0.486383 -0.218950 -0.265747  0.262816 -0.689645  0.078599   \n",
       "360 -0.269326 -0.486383 -0.447044 -0.265747 -0.142327 -0.283413  1.120837   \n",
       "361 -0.424455  3.252326 -1.093066 -0.265747 -1.383615  1.736060 -1.211792   \n",
       "362  5.496553 -0.486383  1.018645 -0.265747  0.366257 -0.743712  1.120837   \n",
       "363  0.018165 -0.486383  1.018645 -0.265747  1.857528 -0.021846  0.805329   \n",
       "364 -0.420014 -0.486383  0.403526 -0.265747 -1.012953  0.413612 -0.616227   \n",
       "365 -0.406381 -0.486383 -0.628048 -0.265747 -0.918132 -0.853307 -1.225972   \n",
       "366  1.246076 -0.486383  1.018645 -0.265747  0.366257  0.856376  1.120837   \n",
       "367  0.860356 -0.486383  1.018645 -0.265747  0.978281 -1.987251  1.120837   \n",
       "368 -0.422049  2.836914 -0.916477 -0.265747 -1.237074  0.531975 -1.594655   \n",
       "369  0.655438 -0.486383  1.018645 -0.265747  1.073102  0.166658  0.964856   \n",
       "370  0.654303 -0.486383  1.018645 -0.265747  1.366184 -0.118289  1.074752   \n",
       "371 -0.412911 -0.486383 -1.219622 -0.265747 -0.943992 -0.150437  0.043148   \n",
       "372 -0.349385  0.344441 -1.060691 -0.265747  0.797260  1.559246  1.120837   \n",
       "373  0.427057 -0.486383  1.018645 -0.265747  1.073102 -0.106599  0.344476   \n",
       "374 -0.424497  2.836914 -1.349120 -0.265747 -1.030193  0.539281 -1.371318   \n",
       "375 -0.414644 -0.486383 -1.282899 -0.265747 -0.573330  0.434070  0.964856   \n",
       "376 -0.365940 -0.486383 -0.732530  3.762978 -0.409548  0.533436  0.287755   \n",
       "377  0.247997 -0.486383  1.018645 -0.265747  1.366184  0.361006  0.762789   \n",
       "378 -0.388866 -0.486383  1.576372 -0.265747  0.598999 -0.473378  0.890410   \n",
       "\n",
       "          DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
       "0    3.190834 -0.643265 -0.449941  1.647059  0.219588 -0.999149  \n",
       "1   -0.594081  1.637346  1.528227  0.803435  0.204682 -0.144954  \n",
       "2   -1.065644 -0.529234 -0.038816 -1.774308  0.010782  2.191808  \n",
       "3    0.680499 -0.529234 -1.153025 -1.680572  0.397046 -1.136606  \n",
       "4   -0.280912 -0.757295 -1.290067 -0.321399  0.401431 -1.156242  \n",
       "5   -0.843328  1.637346  1.528227  0.803435 -3.600853  0.751319  \n",
       "6   -1.097981 -0.529234 -0.038816 -1.774308  0.391347  2.361524  \n",
       "7   -0.678448 -0.415204  0.133976 -0.321399  0.426860  0.493237  \n",
       "8   -0.501394 -0.415204  0.133976 -0.321399  0.408775 -0.091655  \n",
       "9   -0.610390 -0.415204 -0.110316  0.334754  0.448234  0.229545  \n",
       "10  -0.607100  1.637346  1.528227  0.803435  0.387291  0.080867  \n",
       "11  -0.351977 -0.187142 -0.610816 -0.508871  0.430039  1.229611  \n",
       "12   0.753820 -0.643265 -0.765733  0.241018  0.448234 -1.038422  \n",
       "13  -0.590603  1.637346  1.528227  0.803435 -0.270914  1.209975  \n",
       "14  -0.343282  1.637346  1.528227  0.803435 -0.236826  0.201492  \n",
       "15  -0.611377  1.637346  1.528227  0.803435 -3.828950  0.843891  \n",
       "16   1.522196 -0.415204 -0.652524 -0.883815  0.183527 -0.206670  \n",
       "17   1.894587 -0.529234 -0.306941 -1.727440  0.170703 -1.115566  \n",
       "18  -0.765823 -0.985356 -0.813400  1.178379  0.448234 -0.989331  \n",
       "19  -1.027761 -0.529234 -0.038816 -1.774308 -0.139492 -0.080434  \n",
       "20   0.465468 -0.643265 -0.926608 -0.415135  0.364383 -0.931823  \n",
       "21  -0.251019 -0.643265 -0.628691 -0.040190  0.409761 -0.672339  \n",
       "22   0.501001 -0.643265 -0.926608 -0.415135  0.410419 -1.196918  \n",
       "23  -0.962335 -0.529234 -0.038816 -1.774308 -0.383483  0.340351  \n",
       "24  -0.327866  1.637346  1.528227  0.803435  0.431025  0.024762  \n",
       "25   1.012233 -0.529234 -0.586983 -1.539968  0.448234  0.905607  \n",
       "26  -0.424171 -0.187142 -0.610816 -0.508871  0.318347 -1.199723  \n",
       "27  -1.108039 -0.643265  0.163768  1.272115  0.448234  3.046002  \n",
       "28  -0.034249  1.637346  1.528227  0.803435  0.161386  0.090685  \n",
       "29   0.620337 -0.643265 -0.980233  0.334754  0.448234 -1.313335  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "349 -0.442173 -0.529234 -0.152024  1.131511  0.433766 -0.704599  \n",
       "350  1.420204 -0.187142 -0.747858  0.569094  0.241949  0.244973  \n",
       "351 -0.061980 -0.643265 -0.789566  0.053546  0.430039  1.583071  \n",
       "352  0.572114 -0.529234 -0.068607  0.100414  0.333144 -0.049576  \n",
       "353 -0.061557 -0.643265 -0.831275 -0.133926  0.441986 -0.909381  \n",
       "354 -0.929387  1.637346  1.528227  0.803435 -3.665742  1.836945  \n",
       "355 -0.620119 -0.415204  0.133976 -0.321399 -0.190461  0.375417  \n",
       "356 -0.946966  1.637346  1.528227  0.803435  0.448234  0.512873  \n",
       "357  1.254242 -0.529234 -0.235441 -0.415135  0.350353 -1.133800  \n",
       "358  1.954326 -0.301173 -0.473774  0.287886  0.363067  0.808826  \n",
       "359 -0.424970 -0.415204 -0.110316  0.334754  0.448234  0.197284  \n",
       "360  0.177726 -0.643265 -0.610816  1.178379  0.227151  0.048607  \n",
       "361  1.193470 -0.757295 -0.986191 -1.211891  0.332486 -1.344193  \n",
       "362 -1.120447  1.637346  1.528227  0.803435 -3.873671 -0.362360  \n",
       "363 -0.705662  1.637346  1.528227  0.803435 -0.058710  0.209908  \n",
       "364  1.016886 -0.643265 -0.718066 -1.165023  0.403842 -0.743872  \n",
       "365  0.612676 -0.757295 -1.051733 -0.274531  0.448234 -0.348334  \n",
       "366 -1.095772  1.637346  1.528227  0.803435 -1.936212  0.993972  \n",
       "367 -1.232686  1.637346  1.528227  0.803435  0.448234  3.096496  \n",
       "368  0.620337 -0.643265 -0.980233  0.334754  0.448234 -1.121177  \n",
       "369 -0.859497  1.637346  1.528227  0.803435 -3.236620  1.597097  \n",
       "370 -0.721595  1.637346  1.528227  0.803435  0.448234  0.762540  \n",
       "371 -0.141788 -0.871326 -0.795525 -0.227662  0.392662 -0.189838  \n",
       "372 -0.894090 -0.529234 -0.867025 -2.571065  0.299055 -0.687768  \n",
       "373 -0.874819  1.637346  1.528227  0.803435 -2.841916  1.238027  \n",
       "374  2.137208 -0.643265 -0.771691 -0.696343  0.382907 -0.940239  \n",
       "375 -0.446450 -0.757295 -1.290067 -0.321399  0.448234 -0.983720  \n",
       "376  0.165036 -0.187142 -0.610816 -0.508871  0.355614 -0.442310  \n",
       "377 -0.467788  1.637346  1.528227  0.803435  0.414474 -0.337113  \n",
       "378 -0.860108 -0.643265  0.163768  1.272115  0.243812  0.590017  \n",
       "\n",
       "[379 rows x 13 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_std_train, columns=열제목)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(379, 13)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_std_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(20, input_shape=(13,), activation='relu'))\n",
    "model.add(Dense(40))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 341 samples, validate on 38 samples\n",
      "Epoch 1/10\n",
      "341/341 [==============================] - 0s 1ms/step - loss: 539.2680 - val_loss: 454.7256\n",
      "Epoch 2/10\n",
      "341/341 [==============================] - 0s 63us/step - loss: 515.2937 - val_loss: 431.9739\n",
      "Epoch 3/10\n",
      "341/341 [==============================] - 0s 21us/step - loss: 489.9463 - val_loss: 408.1008\n",
      "Epoch 4/10\n",
      "341/341 [==============================] - 0s 21us/step - loss: 462.2027 - val_loss: 382.7370\n",
      "Epoch 5/10\n",
      "341/341 [==============================] - 0s 18us/step - loss: 433.8106 - val_loss: 355.0994\n",
      "Epoch 6/10\n",
      "341/341 [==============================] - 0s 18us/step - loss: 402.0980 - val_loss: 325.5348\n",
      "Epoch 7/10\n",
      "341/341 [==============================] - 0s 18us/step - loss: 367.3405 - val_loss: 294.0869\n",
      "Epoch 8/10\n",
      "341/341 [==============================] - 0s 18us/step - loss: 330.4930 - val_loss: 260.9415\n",
      "Epoch 9/10\n",
      "341/341 [==============================] - 0s 18us/step - loss: 291.7813 - val_loss: 226.4142\n",
      "Epoch 10/10\n",
      "341/341 [==============================] - 0s 18us/step - loss: 252.5039 - val_loss: 191.3550\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_std_train, y_train, \n",
    "                    batch_size=50, epochs=10, \n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91513766144449937"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = RandomForestRegressor().fit(X_train, y_train)\n",
    "forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deepy.dataset import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = \\\n",
    "    cifar10.load('data/cifar-10-batches-py/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xbea24e0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xbec9240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = pd.get_dummies(y_train).values\n",
    "Y_test = pd.get_dummies(y_test).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 32*32*3)\n",
    "X_test = X_test.reshape(-1, 32*32*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3072)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(400, input_shape=(3072,), activation='relu'))\n",
    "model.add(Dense(600, activation='relu'))\n",
    "model.add(Dense(400, activation='relu'))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss=categorical_crossentropy, \n",
    "              optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "40000/40000 [==============================] - 13s 322us/step - loss: 1.8970 - acc: 0.3067 - val_loss: 1.7790 - val_acc: 0.3569\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 12s 307us/step - loss: 1.7120 - acc: 0.3812 - val_loss: 1.6938 - val_acc: 0.4027\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 12s 308us/step - loss: 1.6295 - acc: 0.4147 - val_loss: 1.6080 - val_acc: 0.4174\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 12s 307us/step - loss: 1.5562 - acc: 0.4432 - val_loss: 1.5621 - val_acc: 0.4360\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 12s 309us/step - loss: 1.5037 - acc: 0.4575 - val_loss: 1.5700 - val_acc: 0.4420\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 12s 308us/step - loss: 1.4603 - acc: 0.4760 - val_loss: 1.5307 - val_acc: 0.4526\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 12s 310us/step - loss: 1.4176 - acc: 0.4925 - val_loss: 1.4834 - val_acc: 0.4747\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 13s 315us/step - loss: 1.3861 - acc: 0.4996 - val_loss: 1.5283 - val_acc: 0.4598\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 12s 310us/step - loss: 1.3584 - acc: 0.5108 - val_loss: 1.4445 - val_acc: 0.4831\n",
      "Epoch 10/10\n",
      "40000/40000 [==============================] - 12s 310us/step - loss: 1.3122 - acc: 0.5265 - val_loss: 1.4617 - val_acc: 0.4934\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=100, epochs=10, \n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "훈련결과 = pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d3c04e0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1018d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "훈련결과[['loss', 'val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1d3cc5c0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nikjoCyFEKyKhL4QQrcj/B2+hEBP8SI1yAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bfc06d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "훈련결과[['acc', 'val_acc']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1997 LeNet \n",
    "\n",
    "... 을 시작으로 합성곱 신경망이 탄생합니다.\n",
    "\n",
    "CNN; Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = \\\n",
    "    mnist.load_mnist(flatten=False, normalize=True, \n",
    "                     one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "축순서 = [0, 2, 3, 1]\n",
    "X_train = X_train.transpose(축순서)\n",
    "X_test = X_test.transpose(축순서)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (10000, 28, 28, 1))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# 1층\n",
    "model.add(Conv2D(20, input_shape=(28, 28, 1), \n",
    "                 kernel_size=(5,5), padding='same', \n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "# 2층\n",
    "model.add(Conv2D(50, kernel_size=5, padding='same', \n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "# 출력 준비층\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "# 출력\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss=categorical_crossentropy, \n",
    "              optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 20)        1520      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 50)        25050     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 50)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 500)               1600500   \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 1,632,080\n",
      "Trainable params: 1,632,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 51s 1ms/step - loss: 0.1773 - acc: 0.9453 - val_loss: 0.0509 - val_acc: 0.9841\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 50s 1ms/step - loss: 0.0497 - acc: 0.9842 - val_loss: 0.0468 - val_acc: 0.9862\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 50s 1ms/step - loss: 0.0322 - acc: 0.9904 - val_loss: 0.0464 - val_acc: 0.9859\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 51s 1ms/step - loss: 0.0244 - acc: 0.9923 - val_loss: 0.0323 - val_acc: 0.9903\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 51s 1ms/step - loss: 0.0176 - acc: 0.9939 - val_loss: 0.0303 - val_acc: 0.9910\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 51s 1ms/step - loss: 0.0134 - acc: 0.9956 - val_loss: 0.0316 - val_acc: 0.9907\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 52s 1ms/step - loss: 0.0105 - acc: 0.9970 - val_loss: 0.0399 - val_acc: 0.9889\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 51s 1ms/step - loss: 0.0074 - acc: 0.9976 - val_loss: 0.0432 - val_acc: 0.9878\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 51s 1ms/step - loss: 0.0090 - acc: 0.9969 - val_loss: 0.0377 - val_acc: 0.9892\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 50s 1ms/step - loss: 0.0063 - acc: 0.9979 - val_loss: 0.0425 - val_acc: 0.9890\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 51s 1ms/step - loss: 0.0059 - acc: 0.9978 - val_loss: 0.0388 - val_acc: 0.9902\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 51s 1ms/step - loss: 0.0074 - acc: 0.9974 - val_loss: 0.0395 - val_acc: 0.9894\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 51s 1ms/step - loss: 0.0059 - acc: 0.9979 - val_loss: 0.0418 - val_acc: 0.9887\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 50s 1ms/step - loss: 0.0051 - acc: 0.9983 - val_loss: 0.0343 - val_acc: 0.9907\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 54s 1ms/step - loss: 0.0050 - acc: 0.9983 - val_loss: 0.0484 - val_acc: 0.9897\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 52s 1ms/step - loss: 0.0054 - acc: 0.9981 - val_loss: 0.0346 - val_acc: 0.9918\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 52s 1ms/step - loss: 0.0034 - acc: 0.9988 - val_loss: 0.0338 - val_acc: 0.9922\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 53s 1ms/step - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0410 - val_acc: 0.9919\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 51s 1ms/step - loss: 0.0035 - acc: 0.9988 - val_loss: 0.0303 - val_acc: 0.9930\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 51s 1ms/step - loss: 0.0027 - acc: 0.9993 - val_loss: 0.0335 - val_acc: 0.9927\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=128, epochs=20, \n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
